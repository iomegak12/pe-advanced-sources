{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b494cb",
   "metadata": {},
   "source": [
    "# LAB 1.5: CROSS-LLM PROMPT PORTABILITY\n",
    "\n",
    "**Course:** Advanced Prompt Engineering Training  \n",
    "**Session:** Session 1 - Prompt Engineering Fundamentals Review  \n",
    "**Duration:** 50 minutes  \n",
    "**Type:** Hands-on Multi-Provider Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c758cba",
   "metadata": {},
   "source": [
    "## LAB OVERVIEW\n",
    "\n",
    "This lab explores **cross-LLM prompt portability** - designing prompts that work reliably across different model providers (OpenAI, Anthropic, Azure, etc.). You'll learn to:\n",
    "\n",
    "- Understand model-specific behaviors and quirks\n",
    "- Adapt prompts for different providers\n",
    "- Build provider-agnostic abstraction layers\n",
    "- Test prompts across multiple models\n",
    "- Design fallback and failover strategies\n",
    "\n",
    "**Scenario:** Your bank is evaluating multiple LLM providers for different use cases. Some departments use OpenAI (GPT-4), others use Anthropic (Claude), and compliance requires Azure OpenAI for data residency. You need to ensure prompts work consistently across all providers while minimizing vendor lock-in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e5cbc",
   "metadata": {},
   "source": [
    "## LEARNING OBJECTIVES\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "✓ Identify model-specific behaviors and limitations  \n",
    "✓ Adapt prompts for different LLM providers  \n",
    "✓ Build provider abstraction layers  \n",
    "✓ Create unified testing frameworks  \n",
    "✓ Design multi-provider fallback strategies  \n",
    "✓ Maintain prompt quality across models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9bea1",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daabfc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 1.5: Cross-LLM Prompt Portability\n",
    "# Advanced Prompt Engineering Training - Session 1\n",
    "\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from typing import Dict, List, Any, Optional, Callable\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)  # Load environment variables from .env file\n",
    "\n",
    "# Try to import Anthropic (optional)\n",
    "try:\n",
    "    from anthropic import Anthropic\n",
    "    ANTHROPIC_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ANTHROPIC_AVAILABLE = False\n",
    "    print(\"⚠ Anthropic SDK not available - some examples will be skipped\")\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1bbc55",
   "metadata": {},
   "source": [
    "### Step 2: Configure API Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b273f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Configuration\n",
    "MODEL = os.getenv(\"MODEL_NAME\")\n",
    "TEMPERATURE = 0  # Deterministic for consistent debugging\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found. Please set it in .env file\")\n",
    "\n",
    "if not MODEL:\n",
    "    raise ValueError(\"MODEL_NAME not found. Please set it in .env file\")\n",
    "\n",
    "openai_client = OpenAI(api_key=api_key)\n",
    "\n",
    "print(f\"✓ Model: {MODEL}\")\n",
    "print(f\"✓ Temperature: {TEMPERATURE}\")\n",
    "\n",
    "# Initialize Anthropic client (optional)\n",
    "anthropic_client = None\n",
    "if ANTHROPIC_AVAILABLE and os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
    "    anthropic_client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "    print(\"✓ Anthropic client configured\")\n",
    "else:\n",
    "    print(\"ℹ Anthropic client not available (optional)\")\n",
    "\n",
    "print(\"✓ OpenAI client configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f94e83",
   "metadata": {},
   "source": [
    "### Step 3: Create Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9744ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai(\n",
    "    prompt: str, \n",
    "    system_prompt: str = \"You are a helpful AI assistant.\",\n",
    "    model: str = \"gpt-4\",\n",
    "    temperature: float = 0\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Call OpenAI API\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): User prompt\n",
    "        system_prompt (str): System prompt\n",
    "        model (str): Model name\n",
    "        temperature (float): Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Response with metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"provider\": \"openai\",\n",
    "            \"model\": model,\n",
    "            \"content\": response.choices[0].message.content,\n",
    "            \"total_tokens\": response.usage.total_tokens,\n",
    "            \"finish_reason\": response.choices[0].finish_reason,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"provider\": \"openai\",\n",
    "            \"model\": model,\n",
    "            \"content\": \"\",\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "def call_anthropic(\n",
    "    prompt: str,\n",
    "    system_prompt: str = \"You are a helpful AI assistant.\",\n",
    "    model: str = \"claude-3-5-sonnet-20241022\",\n",
    "    temperature: float = 0\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Call Anthropic API\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): User prompt\n",
    "        system_prompt (str): System prompt\n",
    "        model (str): Model name\n",
    "        temperature (float): Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Response with metadata\n",
    "    \"\"\"\n",
    "    if not anthropic_client:\n",
    "        return {\n",
    "            \"provider\": \"anthropic\",\n",
    "            \"model\": model,\n",
    "            \"content\": \"\",\n",
    "            \"error\": \"Anthropic client not available\",\n",
    "            \"success\": False\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=4096,\n",
    "            system=system_prompt,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"provider\": \"anthropic\",\n",
    "            \"model\": model,\n",
    "            \"content\": response.content[0].text,\n",
    "            \"total_tokens\": response.usage.input_tokens + response.usage.output_tokens,\n",
    "            \"finish_reason\": response.stop_reason,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"provider\": \"anthropic\",\n",
    "            \"model\": model,\n",
    "            \"content\": \"\",\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "print(\"✓ Helper functions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fafc8cd",
   "metadata": {},
   "source": [
    "### Step 4: Test Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OpenAI\n",
    "print(\"Testing OpenAI connection...\")\n",
    "test_openai = call_openai(\"Say 'OpenAI ready' if you receive this.\")\n",
    "if test_openai['success']:\n",
    "    print(f\"✓ OpenAI: {test_openai['content']}\")\n",
    "else:\n",
    "    print(f\"✗ OpenAI error: {test_openai.get('error', 'Unknown')}\")\n",
    "\n",
    "# Test Anthropic (if available)\n",
    "if anthropic_client:\n",
    "    print(\"\\nTesting Anthropic connection...\")\n",
    "    test_anthropic = call_anthropic(\"Say 'Anthropic ready' if you receive this.\")\n",
    "    if test_anthropic['success']:\n",
    "        print(f\"✓ Anthropic: {test_anthropic['content']}\")\n",
    "    else:\n",
    "        print(f\"✗ Anthropic error: {test_anthropic.get('error', 'Unknown')}\")\n",
    "\n",
    "print(\"\\n✓ Connection tests complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca6ef7",
   "metadata": {},
   "source": [
    "## CROSS-LLM CHALLENGES\n",
    "\n",
    "### Why Multi-Provider Matters\n",
    "\n",
    "**Business Reasons:**\n",
    "- **Vendor diversification** - Avoid single-provider lock-in\n",
    "- **Cost optimization** - Use cheaper models for simpler tasks\n",
    "- **Compliance** - Data residency requirements (Azure OpenAI in EU)\n",
    "- **Capabilities** - Different models excel at different tasks\n",
    "- **Redundancy** - Failover if one provider has downtime\n",
    "\n",
    "**Technical Challenges:**\n",
    "- Different API formats\n",
    "- Different prompt behaviors\n",
    "- Different token limits\n",
    "- Different pricing structures\n",
    "- Different rate limits\n",
    "- Different output formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a512ddc",
   "metadata": {},
   "source": [
    "## CHALLENGE 1: MODEL BEHAVIOR DIFFERENCES\n",
    "\n",
    "**Time:** 10 minutes  \n",
    "**Objective:** Identify and document model-specific behaviors\n",
    "\n",
    "### Background\n",
    "\n",
    "The same prompt can produce different outputs across models. Some differences are subtle, others are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d573328",
   "metadata": {},
   "source": [
    "### Test Case: JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test JSON output consistency across models\n",
    "\n",
    "json_prompt = \"\"\"\n",
    "Extract the following information from this transaction and return as JSON:\n",
    "\n",
    "Transaction: Card 4523, $1,250 purchase at Electronics Store, Lagos Nigeria, 3:47 AM\n",
    "\n",
    "Required fields:\n",
    "- card_last_4\n",
    "- amount\n",
    "- merchant\n",
    "- location\n",
    "- time\n",
    "- risk_level (HIGH, MEDIUM, or LOW)\n",
    "\n",
    "Return ONLY valid JSON, no additional text.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"You are a data extraction assistant.\"\n",
    "\n",
    "print(\"JSON OUTPUT TEST:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test with OpenAI\n",
    "openai_result = call_openai(json_prompt, system_prompt)\n",
    "print(f\"\\nOpenAI GPT-4o:\")\n",
    "print(f\"Output:\\n{openai_result['content']}\")\n",
    "print(f\"Success: {openai_result['success']}\")\n",
    "\n",
    "# Test with Anthropic (if available)\n",
    "if anthropic_client:\n",
    "    anthropic_result = call_anthropic(json_prompt, system_prompt)\n",
    "    print(f\"\\nAnthropic Claude:\")\n",
    "    print(f\"Output:\\n{anthropic_result['content']}\")\n",
    "    print(f\"Success: {anthropic_result['success']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07190fb7",
   "metadata": {},
   "source": [
    "### Test Case: Instruction Following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96066ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test strict instruction following\n",
    "\n",
    "strict_prompt = \"\"\"\n",
    "You must answer with EXACTLY one word: yes or no.\n",
    "\n",
    "Question: Is 2+2 equal to 4?\n",
    "\n",
    "Remember: EXACTLY one word only.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nSTRICT INSTRUCTION TEST:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test with OpenAI\n",
    "openai_strict = call_openai(strict_prompt, \"Follow instructions precisely.\")\n",
    "print(f\"\\nOpenAI Response: '{openai_strict['content']}'\")\n",
    "print(f\"Word count: {len(openai_strict['content'].split())}\")\n",
    "\n",
    "# Test with Anthropic\n",
    "if anthropic_client:\n",
    "    anthropic_strict = call_anthropic(strict_prompt, \"Follow instructions precisely.\")\n",
    "    print(f\"\\nAnthropic Response: '{anthropic_strict['content']}'\")\n",
    "    print(f\"Word count: {len(anthropic_strict['content'].split())}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84072382",
   "metadata": {},
   "source": [
    "### Documented Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82502831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive behavior matrix\n",
    "\n",
    "behavior_differences = pd.DataFrame([\n",
    "    {\n",
    "        \"Aspect\": \"JSON Formatting\",\n",
    "        \"OpenAI\": \"May add ```json markers\",\n",
    "        \"Anthropic\": \"May add preambles\",\n",
    "        \"Solution\": \"Explicit 'no markdown' instruction\"\n",
    "    },\n",
    "    {\n",
    "        \"Aspect\": \"Output Length\",\n",
    "        \"OpenAI\": \"More concise by default\",\n",
    "        \"Anthropic\": \"More verbose by default\",\n",
    "        \"Solution\": \"Specify exact word/character limits\"\n",
    "    },\n",
    "    {\n",
    "        \"Aspect\": \"Instruction Adherence\",\n",
    "        \"OpenAI\": \"Good with clear instructions\",\n",
    "        \"Anthropic\": \"Excellent with clear instructions\",\n",
    "        \"Solution\": \"Use explicit constraints\"\n",
    "    },\n",
    "    {\n",
    "        \"Aspect\": \"Temperature Range\",\n",
    "        \"OpenAI\": \"0-2\",\n",
    "        \"Anthropic\": \"0-1\",\n",
    "        \"Solution\": \"Normalize temperature values\"\n",
    "    },\n",
    "    {\n",
    "        \"Aspect\": \"System Prompt Weight\",\n",
    "        \"OpenAI\": \"Moderate influence\",\n",
    "        \"Anthropic\": \"Strong influence\",\n",
    "        \"Solution\": \"Test system prompt effectiveness\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\nMODEL BEHAVIOR DIFFERENCES:\")\n",
    "print(\"=\" * 80)\n",
    "print(behavior_differences.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d3c3ce",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "✓ **Test empirically** - Don't assume same prompt = same behavior  \n",
    "✓ **Document differences** - Build knowledge base  \n",
    "✓ **Explicit constraints** - More specific = more consistent  \n",
    "✓ **Normalize parameters** - Handle temperature range differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c92a7",
   "metadata": {},
   "source": [
    "## CHALLENGE 2: PROMPT ADAPTATION STRATEGIES\n",
    "\n",
    "**Time:** 10 minutes  \n",
    "**Objective:** Create prompt adapters for different providers\n",
    "\n",
    "### Background\n",
    "\n",
    "Instead of maintaining separate prompts for each provider, create **adapters** that modify prompts based on known model behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14592bb",
   "metadata": {},
   "source": [
    "### Student Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create PromptAdapter class\n",
    "# Requirements:\n",
    "# - Takes base prompt\n",
    "# - Applies provider-specific modifications\n",
    "# - Returns adapted prompt\n",
    "\n",
    "class PromptAdapter:\n",
    "    \"\"\"\n",
    "    Adapts prompts for different LLM providers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_prompt: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_prompt (str): Base prompt template\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def adapt_for_provider(self, provider: str) -> str:\n",
    "        \"\"\"\n",
    "        Adapt prompt for specific provider\n",
    "        \n",
    "        Args:\n",
    "            provider (str): 'openai', 'anthropic', etc.\n",
    "        \n",
    "        Returns:\n",
    "            str: Adapted prompt\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "\n",
    "# TODO: Test adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2912446a",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a497ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: PromptAdapter with provider-specific rules\n",
    "\n",
    "class PromptAdapter:\n",
    "    \"\"\"\n",
    "    Adapts prompts for different LLM providers based on known behaviors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Provider-specific adaptation rules\n",
    "    ADAPTATION_RULES = {\n",
    "        \"openai\": {\n",
    "            \"json_enforcement\": \"Return ONLY valid JSON with no markdown formatting.\",\n",
    "            \"conciseness\": \"\",  # OpenAI is concise by default\n",
    "            \"instruction_prefix\": \"\",\n",
    "            \"temperature_max\": 2.0\n",
    "        },\n",
    "        \"anthropic\": {\n",
    "            \"json_enforcement\": \"You must return ONLY valid JSON. Do not include any preamble, explanation, or markdown. Begin your response with {{ and end with }}.\",\n",
    "            \"conciseness\": \"Be concise and direct in your response.\",\n",
    "            \"instruction_prefix\": \"You must follow these instructions exactly:\",\n",
    "            \"temperature_max\": 1.0\n",
    "        },\n",
    "        \"azure_openai\": {\n",
    "            \"json_enforcement\": \"Return ONLY valid JSON with no markdown formatting.\",\n",
    "            \"conciseness\": \"\",\n",
    "            \"instruction_prefix\": \"\",\n",
    "            \"temperature_max\": 2.0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, base_prompt: str, prompt_type: str = \"general\"):\n",
    "        \"\"\"\n",
    "        Initialize adapter\n",
    "        \n",
    "        Args:\n",
    "            base_prompt (str): Base prompt (provider-agnostic)\n",
    "            prompt_type (str): Type of prompt (general, json, classification, etc.)\n",
    "        \"\"\"\n",
    "        self.base_prompt = base_prompt\n",
    "        self.prompt_type = prompt_type\n",
    "    \n",
    "    def adapt_for_provider(self, provider: str, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Adapt prompt for specific provider\n",
    "        \n",
    "        Args:\n",
    "            provider (str): Provider name\n",
    "            **kwargs: Additional parameters (e.g., enforce_json=True)\n",
    "        \n",
    "        Returns:\n",
    "            str: Adapted prompt\n",
    "        \"\"\"\n",
    "        if provider not in self.ADAPTATION_RULES:\n",
    "            # Default to base prompt for unknown providers\n",
    "            return self.base_prompt\n",
    "        \n",
    "        rules = self.ADAPTATION_RULES[provider]\n",
    "        adapted = self.base_prompt\n",
    "        \n",
    "        # Apply JSON enforcement if needed\n",
    "        if kwargs.get(\"enforce_json\", False) and self.prompt_type == \"json\":\n",
    "            adapted = rules[\"json_enforcement\"] + \"\\n\\n\" + adapted\n",
    "        \n",
    "        # Apply conciseness instruction if needed\n",
    "        if kwargs.get(\"enforce_conciseness\", False) and rules[\"conciseness\"]:\n",
    "            adapted = rules[\"conciseness\"] + \"\\n\\n\" + adapted\n",
    "        \n",
    "        # Add instruction prefix if present\n",
    "        if rules[\"instruction_prefix\"]:\n",
    "            adapted = rules[\"instruction_prefix\"] + \"\\n\\n\" + adapted\n",
    "        \n",
    "        return adapted\n",
    "    \n",
    "    def normalize_temperature(self, temperature: float, provider: str) -> float:\n",
    "        \"\"\"\n",
    "        Normalize temperature for provider's range\n",
    "        \n",
    "        Args:\n",
    "            temperature (float): Desired temperature (0-2 scale)\n",
    "            provider (str): Provider name\n",
    "        \n",
    "        Returns:\n",
    "            float: Normalized temperature\n",
    "        \"\"\"\n",
    "        if provider not in self.ADAPTATION_RULES:\n",
    "            return temperature\n",
    "        \n",
    "        max_temp = self.ADAPTATION_RULES[provider][\"temperature_max\"]\n",
    "        \n",
    "        # If temperature exceeds provider max, scale it down\n",
    "        if temperature > max_temp:\n",
    "            return max_temp\n",
    "        \n",
    "        return temperature\n",
    "\n",
    "# Test the adapter\n",
    "print(\"PROMPT ADAPTER TEST:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Base prompt (provider-agnostic)\n",
    "base_fraud_prompt = \"\"\"\n",
    "Analyze this transaction for fraud indicators:\n",
    "\n",
    "Transaction: Card {card}, ${amount} at {merchant}, {location}, {time}\n",
    "\n",
    "Return a JSON object with:\n",
    "- risk_level: HIGH, MEDIUM, or LOW\n",
    "- reasoning: Brief explanation\n",
    "- recommendation: Action to take\n",
    "\"\"\"\n",
    "\n",
    "# Create adapter\n",
    "adapter = PromptAdapter(base_fraud_prompt, prompt_type=\"json\")\n",
    "\n",
    "# Test data\n",
    "test_data = {\n",
    "    \"card\": \"4523\",\n",
    "    \"amount\": \"1250\",\n",
    "    \"merchant\": \"Electronics Store\",\n",
    "    \"location\": \"Lagos, Nigeria\",\n",
    "    \"time\": \"3:47 AM\"\n",
    "}\n",
    "\n",
    "# Adapt for OpenAI\n",
    "openai_adapted = adapter.adapt_for_provider(\"openai\", enforce_json=True)\n",
    "print(\"\\nOPENAI ADAPTED PROMPT:\")\n",
    "print(\"-\" * 80)\n",
    "print(openai_adapted.format(**test_data))\n",
    "\n",
    "# Adapt for Anthropic\n",
    "anthropic_adapted = adapter.adapt_for_provider(\"anthropic\", enforce_json=True)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nANTHROPIC ADAPTED PROMPT:\")\n",
    "print(\"-\" * 80)\n",
    "print(anthropic_adapted.format(**test_data))\n",
    "\n",
    "# Test temperature normalization\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nTEMPERATURE NORMALIZATION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for temp in [0, 0.7, 1.5]:\n",
    "    openai_temp = adapter.normalize_temperature(temp, \"openai\")\n",
    "    anthropic_temp = adapter.normalize_temperature(temp, \"anthropic\")\n",
    "    print(f\"Input: {temp} → OpenAI: {openai_temp}, Anthropic: {anthropic_temp}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954155b",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "✓ **Single base prompt** - Maintain one source of truth  \n",
    "✓ **Provider-specific adaptations** - Automatic modifications  \n",
    "✓ **Testable** - Verify adaptations work  \n",
    "✓ **Maintainable** - Update rules in one place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d3b7d",
   "metadata": {},
   "source": [
    "## CHALLENGE 3: PROVIDER ABSTRACTION LAYER\n",
    "\n",
    "**Time:** 10 minutes  \n",
    "**Objective:** Build unified interface for multiple providers\n",
    "\n",
    "### Background\n",
    "\n",
    "Your application code shouldn't know which provider it's using. Create an abstraction layer that provides a consistent interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c38930d",
   "metadata": {},
   "source": [
    "### Student Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae1e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create UnifiedLLMClient\n",
    "# Requirements:\n",
    "# - Single interface for all providers\n",
    "# - Automatic prompt adaptation\n",
    "# - Fallback support\n",
    "# - Consistent response format\n",
    "\n",
    "class UnifiedLLMClient:\n",
    "    \"\"\"\n",
    "    Unified client for multiple LLM providers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, primary_provider: str, fallback_providers: List[str] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            primary_provider (str): Primary provider to use\n",
    "            fallback_providers (List[str]): Fallback providers if primary fails\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def complete(self, prompt: str, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Get completion from LLM\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): Prompt text\n",
    "            **kwargs: Additional parameters\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Standardized response\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "\n",
    "# TODO: Test unified client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16146601",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Unified LLM Client with abstraction\n",
    "\n",
    "class UnifiedLLMClient:\n",
    "    \"\"\"\n",
    "    Provider-agnostic LLM client with fallback support\n",
    "    \"\"\"\n",
    "    \n",
    "    PROVIDER_CLIENTS = {\n",
    "        \"openai\": call_openai,\n",
    "        \"anthropic\": call_anthropic\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        primary_provider: str,\n",
    "        fallback_providers: List[str] = None,\n",
    "        auto_adapt: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize unified client\n",
    "        \n",
    "        Args:\n",
    "            primary_provider (str): Primary provider\n",
    "            fallback_providers (List[str]): Fallback providers\n",
    "            auto_adapt (bool): Automatically adapt prompts for providers\n",
    "        \"\"\"\n",
    "        self.primary_provider = primary_provider\n",
    "        self.fallback_providers = fallback_providers or []\n",
    "        self.auto_adapt = auto_adapt\n",
    "        \n",
    "        # Validate providers\n",
    "        all_providers = [primary_provider] + self.fallback_providers\n",
    "        for provider in all_providers:\n",
    "            if provider not in self.PROVIDER_CLIENTS:\n",
    "                raise ValueError(f\"Unknown provider: {provider}\")\n",
    "    \n",
    "    def complete(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system_prompt: str = \"You are a helpful AI assistant.\",\n",
    "        temperature: float = 0,\n",
    "        prompt_type: str = \"general\",\n",
    "        **kwargs\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Get completion from LLM with fallback support\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): User prompt\n",
    "            system_prompt (str): System prompt\n",
    "            temperature (float): Sampling temperature\n",
    "            prompt_type (str): Type of prompt for adaptation\n",
    "            **kwargs: Provider-specific parameters\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Standardized response with metadata\n",
    "        \"\"\"\n",
    "        # Create adapter if auto_adapt is enabled\n",
    "        adapter = PromptAdapter(prompt, prompt_type) if self.auto_adapt else None\n",
    "        \n",
    "        # Try primary provider\n",
    "        result = self._try_provider(\n",
    "            self.primary_provider,\n",
    "            prompt,\n",
    "            system_prompt,\n",
    "            temperature,\n",
    "            adapter,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        if result['success']:\n",
    "            result['provider_used'] = self.primary_provider\n",
    "            result['fallback_attempted'] = False\n",
    "            return result\n",
    "        \n",
    "        # Try fallback providers\n",
    "        for fallback_provider in self.fallback_providers:\n",
    "            result = self._try_provider(\n",
    "                fallback_provider,\n",
    "                prompt,\n",
    "                system_prompt,\n",
    "                temperature,\n",
    "                adapter,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            if result['success']:\n",
    "                result['provider_used'] = fallback_provider\n",
    "                result['fallback_attempted'] = True\n",
    "                result['primary_provider_failed'] = self.primary_provider\n",
    "                return result\n",
    "        \n",
    "        # All providers failed\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"All providers failed\",\n",
    "            \"provider_used\": None,\n",
    "            \"content\": \"\"\n",
    "        }\n",
    "    \n",
    "    def _try_provider(\n",
    "        self,\n",
    "        provider: str,\n",
    "        prompt: str,\n",
    "        system_prompt: str,\n",
    "        temperature: float,\n",
    "        adapter: Optional[PromptAdapter],\n",
    "        **kwargs\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Try a single provider\n",
    "        \n",
    "        Args:\n",
    "            provider (str): Provider name\n",
    "            prompt (str): User prompt\n",
    "            system_prompt (str): System prompt\n",
    "            temperature (float): Temperature\n",
    "            adapter (PromptAdapter): Optional adapter\n",
    "            **kwargs: Additional parameters\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Response\n",
    "        \"\"\"\n",
    "        # Adapt prompt if adapter provided\n",
    "        if adapter:\n",
    "            adapted_prompt = adapter.adapt_for_provider(provider, **kwargs)\n",
    "            adapted_temp = adapter.normalize_temperature(temperature, provider)\n",
    "        else:\n",
    "            adapted_prompt = prompt\n",
    "            adapted_temp = temperature\n",
    "        \n",
    "        # Call provider\n",
    "        provider_function = self.PROVIDER_CLIENTS[provider]\n",
    "        return provider_function(adapted_prompt, system_prompt, temperature=adapted_temp)\n",
    "\n",
    "# Test unified client\n",
    "print(\"UNIFIED LLM CLIENT TEST:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize client with fallback\n",
    "if anthropic_client:\n",
    "    client = UnifiedLLMClient(\n",
    "        primary_provider=\"openai\",\n",
    "        fallback_providers=[\"anthropic\"],\n",
    "        auto_adapt=True\n",
    "    )\n",
    "else:\n",
    "    client = UnifiedLLMClient(\n",
    "        primary_provider=\"openai\",\n",
    "        auto_adapt=True\n",
    "    )\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"\n",
    "Classify this customer inquiry:\n",
    "\n",
    "\"I want to increase my credit card limit\"\n",
    "\n",
    "Categories: CARD_SERVICES, LENDING, FRAUD, ACCOUNT_INQUIRY, GENERAL\n",
    "\n",
    "Output ONLY the category name.\n",
    "\"\"\"\n",
    "\n",
    "# Make request (will use primary provider)\n",
    "print(\"\\nTest 1: Normal request\")\n",
    "response1 = client.complete(\n",
    "    test_prompt,\n",
    "    system_prompt=\"You are a classification system.\",\n",
    "    prompt_type=\"classification\"\n",
    ")\n",
    "\n",
    "print(f\"Success: {response1['success']}\")\n",
    "print(f\"Provider used: {response1.get('provider_used', 'N/A')}\")\n",
    "print(f\"Response: {response1['content']}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f4a1c",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "✓ **Abstraction** - Hide provider details  \n",
    "✓ **Consistency** - Same interface for all providers  \n",
    "✓ **Reliability** - Automatic failover  \n",
    "✓ **Maintainability** - Single place to update logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42d4f47",
   "metadata": {},
   "source": [
    "## LAB SUMMARY\n",
    "\n",
    "### Multi-Provider Patterns Mastered\n",
    "\n",
    "| Challenge | Pattern | Key Benefit | Production Impact |\n",
    "|-----------|---------|-------------|-------------------|\n",
    "| 1 | Behavior Analysis | Document provider differences | Informed design decisions |\n",
    "| 2 | Prompt Adaptation | Auto-adapt for each provider | Single prompt, multiple providers |\n",
    "| 3 | Abstraction Layer | Unified interface | Provider independence |\n",
    "\n",
    "### Multi-Provider Architecture\n",
    "\n",
    "```\n",
    "Application Layer\n",
    "    ↓\n",
    "Template Library\n",
    "    ↓\n",
    "Prompt Adapter (provider-specific rules)\n",
    "    ↓\n",
    "Unified LLM Client (abstraction)\n",
    "    ↓\n",
    "Provider Clients (OpenAI, Anthropic, Azure, etc.)\n",
    "    ↓\n",
    "LLM APIs\n",
    "```\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "Before deploying multi-provider system:\n",
    "\n",
    "- [ ] Documented provider behavior differences\n",
    "- [ ] Created prompt adapters for each provider\n",
    "- [ ] Built unified client interface\n",
    "- [ ] Tested prompts across all providers\n",
    "- [ ] Implemented fallback logic\n",
    "- [ ] Set up monitoring and metrics\n",
    "- [ ] Configured cost tracking per provider\n",
    "- [ ] Established SLAs and error handling\n",
    "- [ ] Created runbooks for provider failures\n",
    "- [ ] Documented provider switch procedures\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
