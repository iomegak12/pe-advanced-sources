{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d681737",
   "metadata": {},
   "source": [
    "# LAB 2.1: CONTEXT WINDOW OPTIMIZATION\n",
    "\n",
    "**Course:** Advanced Prompt Engineering Training  \n",
    "**Session:** Session 2 - Advanced Context Engineering  \n",
    "**Duration:** 50 minutes  \n",
    "**Type:** Hands-on Context Compression & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be475bf",
   "metadata": {},
   "source": [
    "## LAB OVERVIEW\n",
    "\n",
    "This lab focuses on **optimizing context usage** to fit large amounts of information into limited context windows. You'll learn to:\n",
    "\n",
    "- Count and budget tokens accurately\n",
    "- Maximize information density\n",
    "- Implement progressive detail loading\n",
    "- Apply extractive summarization\n",
    "- Build hierarchical context compression systems\n",
    "\n",
    "**Scenario:** You're building a commercial loan underwriting system. Each loan application consists of:\n",
    "- Application form (5 pages)\n",
    "- 3 years of tax returns (60 pages)\n",
    "- 6 months of bank statements (30 pages)\n",
    "- Business plan (20 pages)\n",
    "- Property appraisal (15 pages)\n",
    "- Credit report (10 pages)\n",
    "\n",
    "**Total: ~140 pages = ~70,000 tokens**\n",
    "\n",
    "Your context window: **8,000 tokens** (realistic production constraint for cost/speed optimization)\n",
    "\n",
    "**Challenge:** Provide accurate loan analysis while using only 11% of the available information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a9138",
   "metadata": {},
   "source": [
    "## LEARNING OBJECTIVES\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "✓ Count tokens accurately using tiktoken  \n",
    "✓ Identify and eliminate low-value tokens  \n",
    "✓ Transform verbose text into high-density formats  \n",
    "✓ Implement progressive detail strategies  \n",
    "✓ Extract key information without losing critical context  \n",
    "✓ Build multi-tier context hierarchies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c2b67d",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f2911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 2.1: Context Window Optimization\n",
    "# Advanced Prompt Engineering Training - Session 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c926c7",
   "metadata": {},
   "source": [
    "### Step 2: Configure OpenAI Client & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddc65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if API key exists\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found. Please set it in .env file\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Configuration\n",
    "MODEL = os.getenv(\"MODEL_NAME\")\n",
    "TEMPERATURE = 0  # Deterministic for BFSI applications\n",
    "\n",
    "if not MODEL:\n",
    "    raise ValueError(\"MODEL_NAME not found. Please set it in .env file\")\n",
    "\n",
    "# Initialize tokenizer (for GPT-4)\n",
    "encoding = tiktoken.encoding_for_model(MODEL)\n",
    "\n",
    "print(f\"✓ Model: {MODEL}\")\n",
    "print(f\"✓ Tokenizer: {encoding.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50737e3",
   "metadata": {},
   "source": [
    "### Step 3: Create Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7184e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Count tokens in text using tiktoken\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to count\n",
    "    \n",
    "    Returns:\n",
    "        int: Token count\n",
    "    \"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def call_gpt4(prompt: str, system_prompt: str = \"You are a helpful AI assistant.\") -> Dict:\n",
    "    \"\"\"\n",
    "    Call GPT-4 API with token tracking\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): User prompt\n",
    "        system_prompt (str): System prompt\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Response with metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=TEMPERATURE\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"content\": response.choices[0].message.content,\n",
    "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"completion_tokens\": response.usage.completion_tokens,\n",
    "            \"total_tokens\": response.usage.total_tokens,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"content\": \"\",\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "def analyze_token_distribution(text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze how tokens are distributed in text\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Token statistics\n",
    "    \"\"\"\n",
    "    tokens = encoding.encode(text)\n",
    "    words = text.split()\n",
    "    \n",
    "    return {\n",
    "        \"total_tokens\": len(tokens),\n",
    "        \"total_characters\": len(text),\n",
    "        \"total_words\": len(words),\n",
    "        \"tokens_per_word\": len(tokens) / len(words) if words else 0,\n",
    "        \"characters_per_token\": len(text) / len(tokens) if tokens else 0\n",
    "    }\n",
    "\n",
    "print(\"✓ Helper functions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2138337e",
   "metadata": {},
   "source": [
    "### Step 4: Test Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a72bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test token counting\n",
    "test_texts = [\n",
    "    \"Hello world\",\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Context window optimization is critical for production systems.\",\n",
    "    \"Credit score: 720, DTI: 35%, LTV: 80%\"\n",
    "]\n",
    "\n",
    "print(\"TOKEN COUNTING TEST:\")\n",
    "print(\"=\" * 80)\n",
    "for text in test_texts:\n",
    "    token_count = count_tokens(text)\n",
    "    stats = analyze_token_distribution(text)\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"  Tokens: {token_count}\")\n",
    "    print(f\"  Words: {stats['total_words']}\")\n",
    "    print(f\"  Ratio: {stats['tokens_per_word']:.2f} tokens/word\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4fb4d3",
   "metadata": {},
   "source": [
    "## CONTEXT WINDOW FUNDAMENTALS\n",
    "\n",
    "### Understanding Token Economics\n",
    "\n",
    "**Token Basics:**\n",
    "- 1 token ≈ 4 characters in English\n",
    "- 1 token ≈ 0.75 words in English\n",
    "- Numbers and special characters vary\n",
    "- Different languages have different ratios\n",
    "\n",
    "**Rule of Thumb:** Keep context under 10,000 tokens for 95% of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbfced3",
   "metadata": {},
   "source": [
    "## CHALLENGE 1: TOKEN COUNTING & ANALYSIS\n",
    "\n",
    "**Time:** 10 minutes  \n",
    "**Objective:** Learn to count tokens accurately and identify waste\n",
    "\n",
    "### Background\n",
    "\n",
    "You can't optimize what you don't measure. This challenge teaches precise token accounting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a9a87c",
   "metadata": {},
   "source": [
    "### Loan Application Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188fb776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realistic commercial loan application data\n",
    "\n",
    "loan_application = {\n",
    "    \"applicant_info\": \"\"\"\n",
    "    APPLICANT INFORMATION FORM\n",
    "    \n",
    "    Full Legal Name: Sarah Elizabeth Chen\n",
    "    Date of Birth: March 15, 1978 (Age: 46 years old)\n",
    "    Social Security Number: XXX-XX-5678\n",
    "    Current Address: 456 Oak Street, Apartment 3B, Seattle, Washington 98101, United States of America\n",
    "    Telephone Number: (206) 555-1234 (Mobile) / (206) 555-5678 (Home)\n",
    "    Email Address: sarah.chen@example.com\n",
    "    Marital Status: Married\n",
    "    Spouse Name: Michael Robert Chen\n",
    "    Number of Dependents: Two children (ages 12 and 9)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"business_info\": \"\"\"\n",
    "    BUSINESS INFORMATION\n",
    "    \n",
    "    Business Legal Name: Chen Technology Consulting Services, LLC\n",
    "    Doing Business As (DBA): Chen Tech Consulting\n",
    "    Business Structure: Limited Liability Company (LLC)\n",
    "    State of Incorporation: Washington\n",
    "    Federal Tax ID (EIN): 12-3456789\n",
    "    Date of Establishment: January 1, 2019 (5 years in operation)\n",
    "    Business Address: 789 Technology Drive, Suite 200, Seattle, WA 98102\n",
    "    Business Phone: (206) 555-9999\n",
    "    Business Email: contact@chentechconsulting.com\n",
    "    Industry Classification: Information Technology Consulting Services\n",
    "    NAICS Code: 541512 - Computer Systems Design Services\n",
    "    Number of Employees: 12 full-time employees, 3 part-time contractors\n",
    "    Annual Gross Revenue (2023): $850,000\n",
    "    Annual Gross Revenue (2022): $720,000\n",
    "    Annual Gross Revenue (2021): $580,000\n",
    "    \"\"\",\n",
    "    \n",
    "    \"loan_request\": \"\"\"\n",
    "    LOAN REQUEST DETAILS\n",
    "    \n",
    "    Type of Loan Requested: Commercial Real Estate Acquisition Loan\n",
    "    Requested Loan Amount: Five Hundred Thousand Dollars ($500,000.00)\n",
    "    Intended Use of Funds: Purchase of commercial office property for business expansion\n",
    "    Desired Loan Term: Seven (7) years\n",
    "    Preferred Repayment Schedule: Monthly payments with principal and interest\n",
    "    Requested Interest Rate Type: Fixed rate for the entire term of the loan\n",
    "    Collateral Offered: The commercial property being purchased, located at 321 Business Park Avenue, Seattle, WA 98103\n",
    "    Property Purchase Price: Eight Hundred Thousand Dollars ($800,000.00)\n",
    "    Down Payment Amount: Three Hundred Thousand Dollars ($300,000.00) from business savings\n",
    "    Loan-to-Value Ratio: 62.5% (which is $500,000 divided by $800,000)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"financial_summary\": \"\"\"\n",
    "    FINANCIAL SUMMARY (From Tax Returns and Financial Statements)\n",
    "    \n",
    "    Business Financial Overview:\n",
    "    - Total Assets as of December 31, 2023: $1,200,000\n",
    "    - Total Liabilities as of December 31, 2023: $280,000\n",
    "    - Net Worth (Assets minus Liabilities): $920,000\n",
    "    - Current Ratio (Current Assets divided by Current Liabilities): 3.2\n",
    "    - Debt-to-Income Ratio: 35% (total monthly debt payments divided by gross monthly income)\n",
    "    \n",
    "    Cash Flow Analysis:\n",
    "    - Average Monthly Revenue: $70,833 ($850,000 annual divided by 12 months)\n",
    "    - Average Monthly Operating Expenses: $52,000\n",
    "    - Average Monthly Net Profit: $18,833\n",
    "    - Free Cash Flow After Debt Service: $12,000 per month\n",
    "    \n",
    "    Personal Financial Information:\n",
    "    - Personal Credit Score (FICO): 720 (Good credit rating)\n",
    "    - Personal Annual Income (Including Spouse): $95,000 from salary plus business owner's draw\n",
    "    - Personal Assets: $400,000 (retirement accounts, savings, investments)\n",
    "    - Personal Liabilities: $250,000 (primary residence mortgage)\n",
    "    - Personal Net Worth: $150,000\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print(\"LOAN APPLICATION DATA LOADED\")\n",
    "print(\"=\" * 80)\n",
    "for section, content in loan_application.items():\n",
    "    tokens = count_tokens(content)\n",
    "    print(f\"{section}: {tokens} tokens\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_tokens = sum(count_tokens(content) for content in loan_application.values())\n",
    "print(f\"TOTAL APPLICATION: {total_tokens} tokens\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb32f34",
   "metadata": {},
   "source": [
    "### Student Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c0d607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze token waste\n",
    "# Requirements:\n",
    "# - Identify redundant information\n",
    "# - Count unnecessary words/phrases\n",
    "# - Calculate potential token savings\n",
    "\n",
    "def analyze_token_waste(text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Identify token waste in text\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Waste analysis\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    # Hints:\n",
    "    # - Count instances of \"the\", \"a\", \"an\"\n",
    "    # - Identify redundant phrases (\"dollars and cents\", \"USD\")\n",
    "    # - Find verbose patterns (\"in the amount of\" vs just the amount)\n",
    "    pass\n",
    "\n",
    "# TODO: Test your analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3530fe8",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Token Waste Analysis\n",
    "\n",
    "def analyze_token_waste(text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive token waste analysis\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Detailed waste breakdown\n",
    "    \"\"\"\n",
    "    # Common filler words\n",
    "    filler_words = ['the', 'a', 'an', 'of', 'in', 'on', 'at', 'to', 'for']\n",
    "    \n",
    "    # Count fillers\n",
    "    filler_count = 0\n",
    "    for filler in filler_words:\n",
    "        # Word boundary matching to avoid false positives\n",
    "        pattern = r'\\b' + filler + r'\\b'\n",
    "        filler_count += len(re.findall(pattern, text, re.IGNORECASE))\n",
    "    \n",
    "    # Identify redundant patterns\n",
    "    redundant_patterns = [\n",
    "        (r'United States of America', 'USA', 'location'),\n",
    "        (r'\\(\\$[\\d,]+\\.?\\d*\\)', '', 'numeric_redundancy'),\n",
    "        (r'divided by', '/', 'math_operator'),\n",
    "        (r'multiplied by', '×', 'math_operator'),\n",
    "        (r'Total (\\w+) as of [^:]+:', r'\\1:', 'date_redundancy'),\n",
    "        (r'\\(Age: \\d+ years old\\)', '', 'calculated_field'),\n",
    "        (r'\\([\\d\\.]+ percent\\)', '', 'percentage_redundancy')\n",
    "    ]\n",
    "    \n",
    "    redundancy_count = 0\n",
    "    potential_replacements = []\n",
    "    \n",
    "    for pattern, replacement, category in redundant_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        if matches:\n",
    "            redundancy_count += len(matches)\n",
    "            potential_replacements.append({\n",
    "                'category': category,\n",
    "                'instances': len(matches),\n",
    "                'example': matches[0] if isinstance(matches[0], str) else matches[0]\n",
    "            })\n",
    "    \n",
    "    # Verbose phrases\n",
    "    verbose_phrases = [\n",
    "        ('in the amount of', ''),\n",
    "        ('as of the date of', 'on'),\n",
    "        ('for the purpose of', 'to'),\n",
    "        ('in order to', 'to'),\n",
    "        ('with respect to', 'regarding'),\n",
    "        ('in the event that', 'if'),\n",
    "    ]\n",
    "    \n",
    "    verbose_count = 0\n",
    "    for verbose, compact in verbose_phrases:\n",
    "        matches = len(re.findall(re.escape(verbose), text, re.IGNORECASE))\n",
    "        verbose_count += matches\n",
    "    \n",
    "    # Estimate token savings\n",
    "    filler_tokens = filler_count  # Each filler word is ~1 token\n",
    "    redundancy_tokens = redundancy_count * 5  # Average savings per redundancy\n",
    "    verbose_tokens = verbose_count * 2  # Average savings per verbose phrase\n",
    "    \n",
    "    total_tokens = count_tokens(text)\n",
    "    estimated_savings = filler_tokens + redundancy_tokens + verbose_tokens\n",
    "    savings_percentage = (estimated_savings / total_tokens * 100) if total_tokens > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_tokens': total_tokens,\n",
    "        'filler_words': filler_count,\n",
    "        'redundant_patterns': redundancy_count,\n",
    "        'verbose_phrases': verbose_count,\n",
    "        'estimated_token_savings': estimated_savings,\n",
    "        'savings_percentage': savings_percentage,\n",
    "        'potential_replacements': potential_replacements\n",
    "    }\n",
    "\n",
    "# Analyze each section\n",
    "print(\"TOKEN WASTE ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_savings = 0\n",
    "for section_name, content in loan_application.items():\n",
    "    analysis = analyze_token_waste(content)\n",
    "    total_savings += analysis['estimated_token_savings']\n",
    "    \n",
    "    print(f\"\\n{section_name.upper().replace('_', ' ')}:\")\n",
    "    print(f\"  Current tokens: {analysis['total_tokens']}\")\n",
    "    print(f\"  Filler words: {analysis['filler_words']}\")\n",
    "    print(f\"  Redundant patterns: {analysis['redundant_patterns']}\")\n",
    "    print(f\"  Verbose phrases: {analysis['verbose_phrases']}\")\n",
    "    print(f\"  Potential savings: {analysis['estimated_token_savings']} tokens ({analysis['savings_percentage']:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"TOTAL POTENTIAL SAVINGS: {total_savings} tokens\")\n",
    "print(f\"Original: {total_tokens} tokens\")\n",
    "print(f\"Optimized: ~{total_tokens - total_savings} tokens\")\n",
    "print(f\"Reduction: {total_savings / total_tokens * 100:.1f}%\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450dae3f",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "✓ **Measure first** - Can't optimize without metrics  \n",
    "✓ **Filler words add up** - 20-30% of typical prose  \n",
    "✓ **Redundancy is expensive** - Parenthetical amounts, repeated info  \n",
    "✓ **Verbose phrases matter** - \"in order to\" → \"to\" saves 2 tokens each time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b776c4",
   "metadata": {},
   "source": [
    "## CHALLENGE 2: INFORMATION DENSITY OPTIMIZATION\n",
    "\n",
    "**Time:** 10 minutes  \n",
    "**Objective:** Transform verbose text into high-density formats\n",
    "\n",
    "### Background\n",
    "\n",
    "Same information, fewer tokens. This is information density optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9cdf0",
   "metadata": {},
   "source": [
    "### Problem: Low-Density Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa9067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current format (from loan application)\n",
    "verbose_financial = \"\"\"\n",
    "FINANCIAL SUMMARY (From Tax Returns and Financial Statements)\n",
    "\n",
    "Business Financial Overview:\n",
    "- Total Assets as of December 31, 2023: $1,200,000\n",
    "- Total Liabilities as of December 31, 2023: $280,000\n",
    "- Net Worth (Assets minus Liabilities): $920,000\n",
    "- Current Ratio (Current Assets divided by Current Liabilities): 3.2\n",
    "- Debt-to-Income Ratio: 35% (total monthly debt payments divided by gross monthly income)\n",
    "\n",
    "Cash Flow Analysis:\n",
    "- Average Monthly Revenue: $70,833 ($850,000 annual divided by 12 months)\n",
    "- Average Monthly Operating Expenses: $52,000\n",
    "- Average Monthly Net Profit: $18,833\n",
    "- Free Cash Flow After Debt Service: $12,000 per month\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Verbose format: {count_tokens(verbose_financial)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e412d",
   "metadata": {},
   "source": [
    "### Student Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93165d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create high-density version\n",
    "# Requirements:\n",
    "# - Preserve all numerical values\n",
    "# - Remove explanatory text\n",
    "# - Use compact structure (JSON, key-value, table)\n",
    "# - Target: 50%+ reduction\n",
    "\n",
    "def optimize_information_density(verbose_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert verbose text to high-density format\n",
    "    \n",
    "    Args:\n",
    "        verbose_text (str): Original verbose text\n",
    "    \n",
    "    Returns:\n",
    "        str: Optimized high-density text\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "# TODO: Test and compare token counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccbb988",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: High-Density Formats\n",
    "\n",
    "# Approach 1: Structured Key-Value\n",
    "dense_kv_format = \"\"\"\n",
    "FINANCIALS (2023):\n",
    "Assets: $1.2M | Liabilities: $280K | Net Worth: $920K\n",
    "Current Ratio: 3.2 | DTI: 35%\n",
    "\n",
    "MONTHLY CASH FLOW:\n",
    "Revenue: $70.8K | Expenses: $52K | Net: $18.8K | Free CF: $12K\n",
    "\"\"\"\n",
    "\n",
    "# Approach 2: JSON\n",
    "dense_json_format = \"\"\"{\n",
    "  \"financials_2023\": {\n",
    "    \"assets\": 1200000,\n",
    "    \"liabilities\": 280000,\n",
    "    \"net_worth\": 920000,\n",
    "    \"current_ratio\": 3.2,\n",
    "    \"dti\": 0.35\n",
    "  },\n",
    "  \"monthly_cash_flow\": {\n",
    "    \"revenue\": 70833,\n",
    "    \"expenses\": 52000,\n",
    "    \"net_profit\": 18833,\n",
    "    \"free_cash_flow\": 12000\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "# Approach 3: Table Format\n",
    "dense_table_format = \"\"\"\n",
    "FINANCIAL METRICS:\n",
    "Metric          | Value\n",
    "----------------|--------\n",
    "Assets          | $1.2M\n",
    "Liabilities     | $280K\n",
    "Net Worth       | $920K\n",
    "Current Ratio   | 3.2\n",
    "DTI             | 35%\n",
    "Monthly Revenue | $70.8K\n",
    "Monthly Expenses| $52K\n",
    "Monthly Net     | $18.8K\n",
    "Free Cash Flow  | $12K\n",
    "\"\"\"\n",
    "\n",
    "# Compare token counts\n",
    "formats = {\n",
    "    \"Original (Verbose)\": verbose_financial,\n",
    "    \"Key-Value\": dense_kv_format,\n",
    "    \"JSON\": dense_json_format,\n",
    "    \"Table\": dense_table_format\n",
    "}\n",
    "\n",
    "print(\"INFORMATION DENSITY COMPARISON:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for format_name, content in formats.items():\n",
    "    tokens = count_tokens(content)\n",
    "    print(f\"\\n{format_name}:\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    if format_name != \"Original (Verbose)\":\n",
    "        original_tokens = count_tokens(verbose_financial)\n",
    "        savings = original_tokens - tokens\n",
    "        savings_pct = (savings / original_tokens) * 100\n",
    "        print(f\"  Savings: {savings} tokens ({savings_pct:.1f}%)\")\n",
    "    print(f\"  Preview:\\n{content[:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a34b2a",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "✓ **Structured formats save tokens** - JSON, tables, key-value pairs  \n",
    "✓ **Abbreviations are safe** - $1.2M vs $1,200,000  \n",
    "✓ **LLMs understand compact formats** - No accuracy loss  \n",
    "✓ **70%+ reduction possible** - Without losing information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e343b",
   "metadata": {},
   "source": [
    "## LAB SUMMARY\n",
    "\n",
    "### Optimization Techniques Mastered\n",
    "\n",
    "| Technique | Token Savings | When to Use |\n",
    "|-----------|---------------|-------------|\n",
    "| Token Counting | Baseline | Always - measure first |\n",
    "| Density Optimization | 60-75% | Structured data, metrics |\n",
    "| Progressive Loading | 50-90% | Variable query complexity |\n",
    "| Extractive Summarization | 60-80% | Targeted queries |\n",
    "| Hierarchical Compression | 80-95% | Production systems |\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "Before deploying context optimization:\n",
    "\n",
    "- [ ] Implement accurate token counting (tiktoken)\n",
    "- [ ] Create optimized data formats (JSON, tables, key-value)\n",
    "- [ ] Build progressive detail levels (minimal, medium, maximum)\n",
    "- [ ] Implement extractive summarization for targeted queries\n",
    "- [ ] Set and enforce token budgets\n",
    "- [ ] Test with actual LLM calls to verify accuracy\n",
    "- [ ] Monitor token usage in production\n",
    "- [ ] Iterate based on query patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
