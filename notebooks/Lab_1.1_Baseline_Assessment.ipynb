{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad2eb39",
   "metadata": {},
   "source": [
    "# LAB 1.1: BASELINE ASSESSMENT - CREDIT APPLICATION ANALYSIS\n",
    "\n",
    "**Course:** Advanced Prompt Engineering Training  \n",
    "**Session:** Session 1 - Prompt Engineering Fundamentals Review  \n",
    "**Duration:** 45 minutes  \n",
    "**Model:** GPT-4o (temperature=0)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lab presents credit application analysis challenges to demonstrate:\n",
    "- Basic prompt construction with output format control\n",
    "- Multi-step reasoning with chain-of-thought\n",
    "- Strict format enforcement for compliance\n",
    "- Edge case handling without hallucination\n",
    "- Comparative analysis and ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a4d93f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d7b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if API key exists\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found. Please set it in .env file\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b280e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Configuration\n",
    "MODEL = os.getenv(\"MODEL_NAME\")\n",
    "TEMPERATURE = 0  # Deterministic for BFSI applications\n",
    "\n",
    "if not MODEL:\n",
    "    raise ValueError(\"MODEL_NAME not found. Please set it in .env file\")\n",
    "\n",
    "print(f\"✓ Model: {MODEL}\")\n",
    "print(f\"✓ Temperature: {TEMPERATURE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for GPT-4 API calls\n",
    "def call_gpt4(prompt, system_prompt=\"You are a helpful AI assistant.\", temperature=0):\n",
    "    \"\"\"\n",
    "    Wrapper function for GPT-4 API calls\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): User prompt\n",
    "        system_prompt (str): System prompt to set behavior\n",
    "        temperature (float): Controls randomness (0-2)\n",
    "    \n",
    "    Returns:\n",
    "        str: Response text from GPT-4\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"✓ Helper function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OpenAI connection\n",
    "print(\"Testing connection...\")\n",
    "test_response = call_gpt4(\"Say 'Connection successful' if you receive this.\")\n",
    "print(f\"Response: {test_response}\")\n",
    "\n",
    "if \"successful\" in test_response.lower():\n",
    "    print(\"\\n✓ Connection verified\")\n",
    "else:\n",
    "    print(\"\\n✗ Check your API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe4c74",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777fba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credit applications from data folder\n",
    "with open('../data/credit_applications.json', 'r') as f:\n",
    "    credit_applications = json.load(f)\n",
    "\n",
    "# Display dataset\n",
    "df = pd.DataFrame(credit_applications)\n",
    "print(\"Credit Application Dataset:\")\n",
    "print(\"=\" * 100)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b1cc8",
   "metadata": {},
   "source": [
    "## Challenge 1: Basic Information Extraction\n",
    "\n",
    "**Objective:** Extract specific fields and return clean JSON with no additional text.\n",
    "\n",
    "**Requirements:**\n",
    "- Extract: `application_id`, `applicant_name`, `requested_amount`, `credit_score`\n",
    "- Output must be valid JSON (parseable)\n",
    "- No preamble or markdown code blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55344ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1 Solution\n",
    "\n",
    "app = credit_applications[0]  # TechStart Solutions\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Given this credit application data:\n",
    "\n",
    "{json.dumps(app, indent=2)}\n",
    "\n",
    "Extract exactly these four fields and return as JSON:\n",
    "\n",
    "OUTPUT REQUIREMENTS:\n",
    "1. Return ONLY a JSON object\n",
    "2. No markdown, no code blocks, no explanations\n",
    "3. Use exact keys: \"application_id\", \"applicant_name\", \"requested_amount\", \"credit_score\"\n",
    "4. Preserve data types: strings for text, numbers for numeric values\n",
    "5. Start response with {{ and end with }}\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"You are a JSON extraction tool. Output valid JSON only.\"\n",
    "\n",
    "response = call_gpt4(prompt, system_prompt)\n",
    "print(\"Response:\")\n",
    "print(response)\n",
    "\n",
    "# Validate\n",
    "try:\n",
    "    parsed = json.loads(response)\n",
    "    print(\"\\n✓ JSON is valid\")\n",
    "    print(f\"✓ Contains {len(parsed)} fields\")\n",
    "    \n",
    "    required_keys = [\"application_id\", \"applicant_name\", \"requested_amount\", \"credit_score\"]\n",
    "    missing = [k for k in required_keys if k not in parsed]\n",
    "    \n",
    "    if not missing:\n",
    "        print(\"✓ All required fields present\")\n",
    "    else:\n",
    "        print(f\"✗ Missing fields: {missing}\")\n",
    "        \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\n✗ Invalid JSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95f015f",
   "metadata": {},
   "source": [
    "## Challenge 2: Multi-Step Reasoning\n",
    "\n",
    "**Objective:** Analyze credit application step-by-step using chain-of-thought reasoning.\n",
    "\n",
    "**Lending Criteria:**\n",
    "1. Credit score ≥ 680\n",
    "2. Debt Service Coverage Ratio (DSCR) ≥ 1.25\n",
    "3. Years in business ≥ 2\n",
    "4. Collateral value ≥ requested amount\n",
    "5. Industry risk rating not \"High\" for loans > $300,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d97e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2 Solution\n",
    "\n",
    "app = credit_applications[3]  # FastTrack Logistics - marginal case\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a senior credit analyst evaluating a commercial loan application. \n",
    "Analyze the following application step-by-step against our lending criteria.\n",
    "\n",
    "APPLICATION DATA:\n",
    "{json.dumps(app, indent=2)}\n",
    "\n",
    "LENDING CRITERIA:\n",
    "1. Credit score must be ≥ 680\n",
    "2. Debt Service Coverage Ratio (DSCR) must be ≥ 1.25\n",
    "3. Years in business must be ≥ 2\n",
    "4. Collateral value must be ≥ requested loan amount\n",
    "5. Industry risk rating must not be \"High\" for loans exceeding $300,000\n",
    "\n",
    "ANALYSIS INSTRUCTIONS:\n",
    "For each criterion, use this format:\n",
    "\n",
    "Criterion [number]: [description]\n",
    "- Applicant value: [actual value]\n",
    "- Required value: [threshold]\n",
    "- Assessment: [PASS or FAIL]\n",
    "- Reasoning: [brief explanation]\n",
    "\n",
    "After analyzing all criteria, provide:\n",
    "\n",
    "OVERALL DECISION: [APPROVE or DENY]\n",
    "SUMMARY: [2-3 sentence explanation]\n",
    "\n",
    "Begin your analysis now.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"You are a senior credit analyst with 15 years of experience. \n",
    "You always provide thorough, step-by-step analysis with clear reasoning. \n",
    "You never make recommendations without examining all criteria.\"\"\"\n",
    "\n",
    "print(\"CHAIN-OF-THOUGHT ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "response = call_gpt4(prompt, system_prompt)\n",
    "print(response)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17771a88",
   "metadata": {},
   "source": [
    "## Challenge 3: Output Format Control\n",
    "\n",
    "**Objective:** Generate a standardized credit decision letter with exact formatting.\n",
    "\n",
    "**Format Requirements:**\n",
    "1. First line: exactly \"CREDIT DECISION NOTICE\"\n",
    "2. Second line: \"Application ID: [id] | Date: [YYYY-MM-DD]\"\n",
    "3. Decision: APPROVED, DENIED, or CONDITIONAL APPROVAL\n",
    "4. If DENIED: list exactly 3 specific reasons (numbered)\n",
    "5. Final line: exactly \"This decision is final and binding.\"\n",
    "6. Total length: 150-200 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe8af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 3 Solution\n",
    "\n",
    "app = credit_applications[4]  # Sunrise Restaurant - likely denial\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Generate a credit decision letter for this loan application following EXACT format requirements.\n",
    "\n",
    "APPLICATION DATA:\n",
    "{json.dumps(app, indent=2)}\n",
    "\n",
    "MANDATORY FORMAT REQUIREMENTS:\n",
    "1. First line: exactly \"CREDIT DECISION NOTICE\"\n",
    "2. Second line: \"Application ID: [id] | Date: [YYYY-MM-DD format]\"\n",
    "3. Blank line\n",
    "4. \"Decision: \" followed by one of: APPROVED, DENIED, CONDITIONAL APPROVAL\n",
    "5. If DENIED, list exactly 3 specific reasons (numbered 1, 2, 3)\n",
    "6. Each reason must reference specific application data\n",
    "7. Final line: exactly \"This decision is final and binding.\"\n",
    "8. Total length: 150-200 words (excluding header/footer)\n",
    "\n",
    "LENDING CRITERIA:\n",
    "- Credit score minimum: 680\n",
    "- DSCR minimum: 1.25\n",
    "- Years in business minimum: 2\n",
    "- Collateral must cover loan amount\n",
    "- High-risk industries require DSCR ≥ 1.5 for loans > $100,000\n",
    "\n",
    "Generate the letter now. Follow the format precisely.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"You are an automated credit decision system. \n",
    "You generate standardized decision letters following exact formatting requirements.\n",
    "You never deviate from specified formats.\"\"\"\n",
    "\n",
    "print(\"FORMATTED DECISION LETTER:\")\n",
    "print(\"=\" * 80)\n",
    "response = call_gpt4(prompt, system_prompt)\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40daa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate format compliance\n",
    "\n",
    "lines = response.strip().split('\\n')\n",
    "\n",
    "print(\"\\nFORMAT VALIDATION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check 1: Header\n",
    "if lines[0].strip() == \"CREDIT DECISION NOTICE\":\n",
    "    print(\"✓ Header correct\")\n",
    "else:\n",
    "    print(f\"✗ Header incorrect: '{lines[0]}'\")\n",
    "\n",
    "# Check 2: Application ID line\n",
    "if \"Application ID:\" in lines[1] and \"Date:\" in lines[1]:\n",
    "    print(\"✓ Application ID and date present\")\n",
    "else:\n",
    "    print(\"✗ Second line format incorrect\")\n",
    "\n",
    "# Check 3: Decision keyword\n",
    "decision_lines = [l for l in lines if \"Decision:\" in l]\n",
    "if decision_lines:\n",
    "    decision = decision_lines[0].split(\"Decision:\")[1].strip()\n",
    "    if decision in [\"APPROVED\", \"DENIED\", \"CONDITIONAL APPROVAL\"]:\n",
    "        print(f\"✓ Valid decision: {decision}\")\n",
    "    else:\n",
    "        print(f\"✗ Invalid decision: {decision}\")\n",
    "\n",
    "# Check 4: Footer\n",
    "if lines[-1].strip() == \"This decision is final and binding.\":\n",
    "    print(\"✓ Footer correct\")\n",
    "else:\n",
    "    print(f\"✗ Footer incorrect: '{lines[-1]}'\")\n",
    "\n",
    "# Check 5: Word count\n",
    "word_count = len(response.split())\n",
    "if 150 <= word_count <= 220:\n",
    "    print(f\"✓ Word count OK: {word_count}\")\n",
    "else:\n",
    "    print(f\"⚠ Word count: {word_count} (target: 150-200)\")\n",
    "\n",
    "# Check 6: Numbered reasons for denial\n",
    "if \"DENIED\" in response:\n",
    "    if all(f\"{i}.\" in response for i in [1, 2, 3]):\n",
    "        print(\"✓ Three numbered reasons provided\")\n",
    "    else:\n",
    "        print(\"✗ Must include 3 numbered reasons\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ad859",
   "metadata": {},
   "source": [
    "## Challenge 4: Edge Case Handling\n",
    "\n",
    "**Objective:** Handle incomplete/invalid data without hallucinating.\n",
    "\n",
    "**Requirements:**\n",
    "- Identify ALL data quality issues\n",
    "- Do NOT crash or hallucinate missing data\n",
    "- Provide clear report of problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5daee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge case applications with data quality issues\n",
    "\n",
    "edge_case_apps = [\n",
    "    {\n",
    "        \"application_id\": \"CA-2024-101\",\n",
    "        \"applicant_name\": \"Mystery Corp\",\n",
    "        \"business_type\": \"Unknown\",\n",
    "        \"years_in_business\": None,  # Missing\n",
    "        \"annual_revenue\": 0,  # Suspicious\n",
    "        \"requested_amount\": 1000000,  # Large request\n",
    "        \"credit_score\": None,  # Missing\n",
    "        \"existing_debt\": \"Not disclosed\",  # Wrong type\n",
    "        \"collateral_value\": -50000,  # Invalid negative\n",
    "    },\n",
    "    {\n",
    "        \"application_id\": \"CA-2024-102\",\n",
    "        \"applicant_name\": \"\",  # Empty\n",
    "        \"business_type\": \"Retail\",\n",
    "        \"requested_amount\": \"five hundred thousand\",  # Text not number\n",
    "        \"credit_score\": 1200,  # Impossible (max is 850)\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Edge Case Applications:\")\n",
    "for i, app in enumerate(edge_case_apps, 1):\n",
    "    print(f\"\\nEdge Case {i}:\")\n",
    "    print(json.dumps(app, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 4 Solution\n",
    "\n",
    "def analyze_data_quality(app_data):\n",
    "    \"\"\"Analyzes credit application for data quality issues\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a data quality analyst for a credit processing system. \n",
    "Identify data quality issues in this loan application BEFORE it reaches underwriting.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. Do NOT make assumptions about missing data\n",
    "2. Do NOT fill in missing values with estimates\n",
    "3. Do NOT proceed with credit analysis if data is incomplete\n",
    "4. Flag ALL data quality issues\n",
    "\n",
    "APPLICATION DATA:\n",
    "{json.dumps(app_data, indent=2)}\n",
    "\n",
    "EXPECTED DATA STANDARDS:\n",
    "- application_id: Non-empty string starting with \"CA-\"\n",
    "- applicant_name: Non-empty string\n",
    "- years_in_business: Positive integer or zero\n",
    "- annual_revenue: Non-negative number\n",
    "- requested_amount: Positive number\n",
    "- credit_score: Integer between 300-850, or null\n",
    "- existing_debt: Non-negative number\n",
    "- collateral_value: Non-negative number\n",
    "\n",
    "ANALYSIS FORMAT:\n",
    "For each field:\n",
    "- Field name\n",
    "- Current value\n",
    "- Issue (if any)\n",
    "- Severity: CRITICAL (blocks processing) or WARNING (review needed)\n",
    "\n",
    "Then provide:\n",
    "\n",
    "DATA QUALITY SUMMARY:\n",
    "- Total issues: [count]\n",
    "- Critical issues: [count]\n",
    "- Warnings: [count]\n",
    "\n",
    "RECOMMENDATION:\n",
    "Can this application proceed? Yes/No and why.\n",
    "\n",
    "Begin analysis.\n",
    "\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a data quality validation system. \n",
    "You never make assumptions about missing data. \n",
    "You never fill in values not provided.\n",
    "You flag all data quality issues objectively.\"\"\"\n",
    "    \n",
    "    return call_gpt4(prompt, system_prompt, temperature=0)\n",
    "\n",
    "# Test Edge Case 1\n",
    "print(\"EDGE CASE 1: Missing and Invalid Data\")\n",
    "print(\"=\" * 80)\n",
    "response1 = analyze_data_quality(edge_case_apps[0])\n",
    "print(response1)\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Test Edge Case 2\n",
    "print(\"EDGE CASE 2: Invalid Formats and Impossible Values\")\n",
    "print(\"=\" * 80)\n",
    "response2 = analyze_data_quality(edge_case_apps[1])\n",
    "print(response2)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f781440f",
   "metadata": {},
   "source": [
    "## Challenge 5: Comparative Analysis\n",
    "\n",
    "**Objective:** Compare all five applications and rank by approval likelihood.\n",
    "\n",
    "**Requirements:**\n",
    "- Rank all 5 applications from most to least likely to approve\n",
    "- Provide data-driven justification for each\n",
    "- Identify strongest/weakest factor for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab3559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 5 Solution\n",
    "\n",
    "all_apps = credit_applications\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a senior credit committee member reviewing multiple loan applications. \n",
    "Rank these by approval likelihood with detailed justification.\n",
    "\n",
    "APPLICATIONS:\n",
    "{json.dumps(all_apps, indent=2)}\n",
    "\n",
    "LENDING CRITERIA (for reference):\n",
    "1. Credit score ≥ 680 (weight: 25%)\n",
    "2. DSCR ≥ 1.25 (weight: 30%)\n",
    "3. Years in business ≥ 2 (weight: 15%)\n",
    "4. Collateral coverage ≥ 100% (weight: 20%)\n",
    "5. Industry risk rating (weight: 10%)\n",
    "\n",
    "ANALYSIS INSTRUCTIONS:\n",
    "\n",
    "Step 1: For each application, note:\n",
    "- Strongest factor\n",
    "- Weakest factor\n",
    "\n",
    "Step 2: Rank 1 (most likely) to 5 (least likely)\n",
    "\n",
    "Step 3: For each rank, provide:\n",
    "- Rank number\n",
    "- Application ID and name\n",
    "- Assessment: STRONG / MODERATE / WEAK / HIGH RISK\n",
    "- Justification (2-3 sentences with specific metrics)\n",
    "\n",
    "Step 4: Summary:\n",
    "- Clear winner and why\n",
    "- Clear reject and why\n",
    "- Borderline cases needing review\n",
    "\n",
    "Use data-driven reasoning. Reference specific metrics.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"You are a senior credit analyst with expertise in comparative analysis.\n",
    "You provide objective, data-driven rankings based on established criteria.\n",
    "You always support conclusions with specific metrics.\"\"\"\n",
    "\n",
    "print(\"COMPARATIVE CREDIT ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "response = call_gpt4(prompt, system_prompt, temperature=0)\n",
    "print(response)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440b99a",
   "metadata": {},
   "source": [
    "## Lab Summary\n",
    "\n",
    "### Key Principles Learned\n",
    "\n",
    "1. **Specificity beats vagueness** - The more explicit your prompt, the better the output\n",
    "2. **Structure enables consistency** - Templates and formats improve reliability\n",
    "3. **Validation is mandatory** - Always programmatically verify outputs\n",
    "4. **Chain-of-thought for BFSI** - Explainability is regulatory requirement\n",
    "5. **Edge cases matter** - Production systems need robust error handling\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
