{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a618d823",
   "metadata": {},
   "source": [
    "# LAB 2.2: BUILDING A STATEFUL CONVERSATION SYSTEM\n",
    "\n",
    "**Course:** Advanced Prompt Engineering Training  \n",
    "**Session:** Session 2 - Advanced Context Engineering  \n",
    "**Duration:** 50 minutes  \n",
    "**Difficulty:** ⭐⭐⭐⭐☆  \n",
    "**Type:** Hands-on Conversation State Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3334b769",
   "metadata": {},
   "source": [
    "## LAB OVERVIEW\n",
    "\n",
    "This lab focuses on **maintaining conversation state across multiple turns** without exceeding token limits. You'll learn to:\n",
    "\n",
    "- Implement sliding window buffer memory\n",
    "- Generate and use conversation summaries\n",
    "- Track entities across conversation history\n",
    "- Combine memory strategies for optimal performance\n",
    "- Build production-ready conversation state managers\n",
    "\n",
    "**Scenario:** You're building an AI loan advisor chatbot for a bank. Customers have extended conversations (20-30 turns) discussing loan options, asking questions, providing information, and making decisions. The chatbot must remember:\n",
    "- What the customer is looking for\n",
    "- Information they've already provided\n",
    "- Decisions they've made\n",
    "- Questions they've asked\n",
    "- Context from earlier in the conversation\n",
    "\n",
    "**Challenge:** Maintain perfect context across 30+ message exchanges while staying under 8,000 token budget."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bfaf48",
   "metadata": {},
   "source": [
    "## LEARNING OBJECTIVES\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "✓ Implement sliding window buffer memory  \n",
    "✓ Generate conversation summaries programmatically  \n",
    "✓ Build entity extraction and tracking systems  \n",
    "✓ Combine multiple memory strategies  \n",
    "✓ Handle conversation state in production applications  \n",
    "✓ Optimize token usage across long conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41baa4f",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bbb5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 2.2: Building a Stateful Conversation System\n",
    "# Advanced Prompt Engineering Training - Session 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c800df",
   "metadata": {},
   "source": [
    "### Step 2: Configure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c355f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if API key exists\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found. Please set it in .env file\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Configuration\n",
    "MODEL = os.getenv(\"MODEL_NAME\")\n",
    "TEMPERATURE = 0  # Deterministic for BFSI applications\n",
    "\n",
    "if not MODEL:\n",
    "    raise ValueError(\"MODEL_NAME not found. Please set it in .env file\")\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(MODEL)\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in text\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "print(f\"✓ Model: {MODEL}\")\n",
    "print(f\"✓ Tokenizer: {encoding.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009cdc6b",
   "metadata": {},
   "source": [
    "### Step 3: Create Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt4(\n",
    "    messages: List[Dict],\n",
    "    system_prompt: str = \"You are a helpful AI assistant.\",\n",
    "    temperature: float = 0\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Call GPT-4 with conversation history\n",
    "    \n",
    "    Args:\n",
    "        messages (List[Dict]): Conversation messages\n",
    "        system_prompt (str): System prompt\n",
    "        temperature (float): Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Response with metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare messages\n",
    "        full_messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        full_messages.extend(messages)\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=full_messages,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"content\": response.choices[0].message.content,\n",
    "            \"role\": \"assistant\",\n",
    "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"completion_tokens\": response.usage.completion_tokens,\n",
    "            \"total_tokens\": response.usage.total_tokens,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"content\": \"\",\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "def calculate_conversation_tokens(messages: List[Dict]) -> int:\n",
    "    \"\"\"\n",
    "    Calculate total tokens in conversation history\n",
    "    \n",
    "    Args:\n",
    "        messages (List[Dict]): Conversation messages\n",
    "    \n",
    "    Returns:\n",
    "        int: Total token count\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for msg in messages:\n",
    "        total += count_tokens(msg.get(\"content\", \"\"))\n",
    "    return total\n",
    "\n",
    "print(\"✓ Helper functions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e18856",
   "metadata": {},
   "source": [
    "### Step 4: Create Sample Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc97783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated multi-turn loan advisory conversation\n",
    "\n",
    "sample_conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hi, I'm interested in getting a mortgage for my first home.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Hello! I'd be happy to help you explore mortgage options for your first home. To get started, can you tell me a bit about the type of property you're looking at and your approximate budget?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm looking at a condo in Seattle, probably around $450,000. I've been saving for a while and have about $90,000 for a down payment.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Great! A $90,000 down payment on a $450,000 property is 20%, which means you'd avoid PMI (private mortgage insurance). That's excellent. What's your annual household income, and do you have a sense of your credit score?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"My income is $85,000 per year, and my credit score is around 720 last time I checked.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Perfect! With a 720 credit score, you'll qualify for competitive interest rates. Based on your $85,000 income and assuming minimal other debts, you should be well within the debt-to-income requirements. Have you been pre-approved, or would you like to start that process?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Not yet pre-approved. What documents would I need?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"For pre-approval, you'll typically need: 2 years of tax returns, 2 recent pay stubs, 2-3 months of bank statements, and documentation of your down payment source. Also, we'll run a credit check. The process usually takes 3-5 business days. Should I set that up for you?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Yes, please. Also, what interest rates are you seeing right now for someone with my profile?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"For a borrower with your profile - 720 credit score, 20% down, $85K income - we're currently seeing rates between 6.5% and 7.0% for a 30-year fixed mortgage. The exact rate will depend on the final underwriting, but you'd likely qualify for the lower end of that range. Would you prefer a 30-year fixed, or would you like to explore 15-year or ARM options?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I think 30-year fixed makes sense for me. How much would my monthly payment be approximately?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"With a $360,000 loan ($450,000 - $90,000 down) at 6.75% for 30 years, your principal and interest payment would be approximately $2,335 per month. Add in property taxes (varies by county but ~$400/month in Seattle), HOA fees for the condo (varies, maybe $200-400/month), and homeowner's insurance (~$100/month), you're looking at a total monthly payment around $3,000-3,200. Does that fit your budget?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Sample conversation loaded: {len(sample_conversation)} messages\")\n",
    "print(f\"Total tokens: {calculate_conversation_tokens(sample_conversation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ff6af7",
   "metadata": {},
   "source": [
    "## CONVERSATION STATE FUNDAMENTALS\n",
    "\n",
    "### The Stateful Conversation Problem\n",
    "\n",
    "**Challenge:** Each API call to an LLM is stateless. The model doesn't remember previous messages unless you send them again.\n",
    "\n",
    "**Naive Approach:**\n",
    "```python\n",
    "# Send entire conversation history with each request\n",
    "messages = conversation_history + [new_user_message]\n",
    "response = call_gpt4(messages)\n",
    "```\n",
    "\n",
    "**Problem:** After 20 turns (40 messages), you might have 8,000+ tokens just in conversation history, leaving no room for context or instructions.\n",
    "\n",
    "### Memory Patterns Comparison\n",
    "\n",
    "| Pattern | Token Growth | Accuracy | Complexity | Best For |\n",
    "|---------|--------------|----------|------------|----------|\n",
    "| **Buffer** | Linear (capped) | High (recent) | Low | Short conversations |\n",
    "| **Summary** | Logarithmic | Medium | Medium | Long conversations |\n",
    "| **Entity** | Sub-linear | High (facts) | High | Complex interactions |\n",
    "| **Hybrid** | Optimized | Highest | High | Production systems |\n",
    "\n",
    "### Token Budget Example\n",
    "\n",
    "```\n",
    "Token Budget: 8,000 total\n",
    "├─ System Prompt: 500 tokens\n",
    "├─ Conversation State: 3,000 tokens (what we manage)\n",
    "├─ New User Message: 100 tokens\n",
    "├─ Retrieved Context: 2,000 tokens (optional)\n",
    "├─ Model Response: 1,500 tokens\n",
    "└─ Safety Buffer: 900 tokens\n",
    "\n",
    "Goal: Keep conversation state under 3,000 tokens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bad7cb",
   "metadata": {},
   "source": [
    "## CHALLENGE 1: BUFFER MEMORY IMPLEMENTATION\n",
    "\n",
    "**Time:** 10 minutes  \n",
    "**Objective:** Implement sliding window buffer memory\n",
    "\n",
    "### Background\n",
    "\n",
    "Buffer memory keeps the N most recent messages. Simple, effective for short conversations, but loses history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba82c62",
   "metadata": {},
   "source": [
    "### Student Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8044bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement BufferMemory class\n",
    "# Requirements:\n",
    "# - Store last N message pairs (user + assistant)\n",
    "# - Automatically discard oldest when window exceeded\n",
    "# - Provide token count\n",
    "\n",
    "class BufferMemory:\n",
    "    \"\"\"\n",
    "    Sliding window buffer memory\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_pairs: int = 5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_pairs (int): Maximum conversation pairs to keep\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def add_message(self, role: str, content: str) -> None:\n",
    "        \"\"\"\n",
    "        Add message to buffer\n",
    "        \n",
    "        Args:\n",
    "            role (str): 'user' or 'assistant'\n",
    "            content (str): Message content\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def get_messages(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get current message buffer\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: Messages in buffer\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def get_token_count(self) -> int:\n",
    "        \"\"\"\n",
    "        Get total tokens in buffer\n",
    "        \n",
    "        Returns:\n",
    "            int: Token count\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "\n",
    "# TODO: Test with sample conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb7f94",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc072f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Buffer Memory System\n",
    "\n",
    "class BufferMemory:\n",
    "    \"\"\"\n",
    "    Sliding window buffer memory for conversations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_pairs: int = 5):\n",
    "        \"\"\"\n",
    "        Initialize buffer\n",
    "        \n",
    "        Args:\n",
    "            max_pairs (int): Maximum conversation pairs (user + assistant) to keep\n",
    "        \"\"\"\n",
    "        self.max_pairs = max_pairs\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_message(self, role: str, content: str) -> None:\n",
    "        \"\"\"\n",
    "        Add message to buffer, maintaining size limit\n",
    "        \n",
    "        Args:\n",
    "            role (str): 'user' or 'assistant'\n",
    "            content (str): Message content\n",
    "        \"\"\"\n",
    "        self.messages.append({\n",
    "            \"role\": role,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Enforce max_pairs limit (each pair is 2 messages)\n",
    "        max_messages = self.max_pairs * 2\n",
    "        if len(self.messages) > max_messages:\n",
    "            # Remove oldest messages to maintain window\n",
    "            self.messages = self.messages[-max_messages:]\n",
    "    \n",
    "    def get_messages(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get current message buffer\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: Messages in buffer\n",
    "        \"\"\"\n",
    "        return self.messages\n",
    "    \n",
    "    def get_token_count(self) -> int:\n",
    "        \"\"\"\n",
    "        Calculate total tokens in buffer\n",
    "        \n",
    "        Returns:\n",
    "            int: Token count\n",
    "        \"\"\"\n",
    "        return calculate_conversation_tokens(self.messages)\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear all messages from buffer\"\"\"\n",
    "        self.messages = []\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get buffer statistics\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Statistics\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"message_count\": len(self.messages),\n",
    "            \"max_pairs\": self.max_pairs,\n",
    "            \"token_count\": self.get_token_count(),\n",
    "            \"oldest_message\": self.messages[0][\"timestamp\"] if self.messages else None,\n",
    "            \"newest_message\": self.messages[-1][\"timestamp\"] if self.messages else None\n",
    "        }\n",
    "\n",
    "# Test buffer memory\n",
    "print(\"BUFFER MEMORY TEST:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create buffer that keeps last 3 conversation pairs (6 messages)\n",
    "buffer = BufferMemory(max_pairs=3)\n",
    "\n",
    "# Simulate conversation\n",
    "test_messages = [\n",
    "    (\"user\", \"Hi, I want a mortgage.\"),\n",
    "    (\"assistant\", \"I can help! What's your budget?\"),\n",
    "    (\"user\", \"Around $400,000.\"),\n",
    "    (\"assistant\", \"Great! What's your down payment?\"),\n",
    "    (\"user\", \"$80,000.\"),\n",
    "    (\"assistant\", \"That's 20% - excellent! Credit score?\"),\n",
    "    (\"user\", \"720.\"),\n",
    "    (\"assistant\", \"Perfect! You'll qualify for good rates.\"),\n",
    "    (\"user\", \"What rate can I get?\"),  # This should push out first 2 messages\n",
    "    (\"assistant\", \"Around 6.5% - 7.0% for your profile.\"),\n",
    "]\n",
    "\n",
    "for role, content in test_messages:\n",
    "    buffer.add_message(role, content)\n",
    "    stats = buffer.get_stats()\n",
    "    print(f\"\\nAdded: {role} - '{content[:40]}...'\")\n",
    "    print(f\"  Buffer size: {stats['message_count']} messages, {stats['token_count']} tokens\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\nFINAL BUFFER CONTENTS:\")\n",
    "for msg in buffer.get_messages():\n",
    "    print(f\"  {msg['role']}: {msg['content']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a5132c",
   "metadata": {},
   "source": [
    "### Test Context Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71982d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the limitation: context loss\n",
    "\n",
    "print(\"\\nCONTEXT LOSS DEMONSTRATION:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# The buffer has lost early context\n",
    "# User's budget ($400,000) is no longer in buffer\n",
    "# Let's see if the model can still answer\n",
    "\n",
    "buffer_messages = buffer.get_messages()\n",
    "test_query = \"What was my budget again?\"\n",
    "\n",
    "response = call_gpt4(\n",
    "    buffer_messages + [{\"role\": \"user\", \"content\": test_query}],\n",
    "    \"You are a loan advisor. Answer based only on the conversation history.\"\n",
    ")\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Buffer has budget info: {'$400,000' in str(buffer_messages)}\")\n",
    "print(f\"Response: {response['content']}\")\n",
    "\n",
    "print(\"\\n⚠ LIMITATION: Buffer memory loses early conversation context!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1bd634",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "✓ **Simple and fast** - Easy to implement  \n",
    "✓ **Predictable token usage** - Constant (max_pairs × avg_tokens_per_pair)  \n",
    "✓ **Good for recent context** - Maintains flow of conversation  \n",
    "✗ **Loses history** - Early messages are forgotten  \n",
    "✗ **Not suitable for long conversations** - Critical info may be lost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad2a5d",
   "metadata": {},
   "source": [
    "## CHALLENGE 2: CONVERSATION SUMMARY MEMORY\n",
    "\n",
    "**Time:** 10 minutes  \n",
    "**Objective:** Generate and use conversation summaries to compress history\n",
    "\n",
    "### Background\n",
    "\n",
    "Instead of keeping all messages, periodically summarize the conversation and keep only the summary plus recent messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c198e",
   "metadata": {},
   "source": [
    "### Student Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d6c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement SummaryMemory class\n",
    "# Requirements:\n",
    "# - Summarize conversation every N messages\n",
    "# - Keep summary + recent buffer\n",
    "# - Reduce token usage while preserving key information\n",
    "\n",
    "class SummaryMemory:\n",
    "    \"\"\"\n",
    "    Conversation summary memory\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, summarize_every: int = 10, buffer_size: int = 4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            summarize_every (int): Summarize after this many messages\n",
    "            buffer_size (int): Keep this many recent messages\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def add_message(self, role: str, content: str) -> None:\n",
    "        \"\"\"Add message and trigger summarization if needed\"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def _generate_summary(self) -> str:\n",
    "        \"\"\"Generate conversation summary\"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "\n",
    "# TODO: Test with long conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7fe278",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575bccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Conversation Summary Memory\n",
    "\n",
    "class SummaryMemory:\n",
    "    \"\"\"\n",
    "    Conversation memory with automatic summarization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        summarize_every: int = 10,\n",
    "        buffer_size: int = 4,\n",
    "        max_summary_tokens: int = 500\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize summary memory\n",
    "        \n",
    "        Args:\n",
    "            summarize_every (int): Summarize after this many messages\n",
    "            buffer_size (int): Number of recent messages to keep\n",
    "            max_summary_tokens (int): Target tokens for summary\n",
    "        \"\"\"\n",
    "        self.summarize_every = summarize_every\n",
    "        self.buffer_size = buffer_size\n",
    "        self.max_summary_tokens = max_summary_tokens\n",
    "        \n",
    "        self.summary = \"\"\n",
    "        self.messages = []\n",
    "        self.all_messages = []  # For summarization\n",
    "        self.summary_count = 0\n",
    "    \n",
    "    def add_message(self, role: str, content: str) -> None:\n",
    "        \"\"\"\n",
    "        Add message and trigger summarization if needed\n",
    "        \n",
    "        Args:\n",
    "            role (str): 'user' or 'assistant'\n",
    "            content (str): Message content\n",
    "        \"\"\"\n",
    "        message = {\n",
    "            \"role\": role,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.messages.append(message)\n",
    "        self.all_messages.append(message)\n",
    "        \n",
    "        # Check if we need to summarize\n",
    "        if len(self.messages) >= self.summarize_every:\n",
    "            self._generate_summary()\n",
    "    \n",
    "    def _generate_summary(self) -> None:\n",
    "        \"\"\"Generate summary of conversation so far\"\"\"\n",
    "        # Create summarization prompt\n",
    "        conversation_text = \"\\n\".join([\n",
    "            f\"{msg['role'].capitalize()}: {msg['content']}\"\n",
    "            for msg in self.all_messages\n",
    "        ])\n",
    "        \n",
    "        summary_prompt = f\"\"\"\n",
    "Summarize this conversation in {self.max_summary_tokens} tokens or less.\n",
    "Preserve all key facts, decisions, and important details.\n",
    "Focus on what matters for continuing the conversation.\n",
    "\n",
    "CONVERSATION:\n",
    "{conversation_text}\n",
    "\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "        \n",
    "        response = call_gpt4(\n",
    "            [{\"role\": \"user\", \"content\": summary_prompt}],\n",
    "            \"You are a conversation summarizer. Be concise and factual.\"\n",
    "        )\n",
    "        \n",
    "        if response['success']:\n",
    "            # Update summary (or append if we already have one)\n",
    "            if self.summary:\n",
    "                # Combine old summary with new\n",
    "                self.summary = f\"{self.summary}\\n\\nRecent activity: {response['content']}\"\n",
    "            else:\n",
    "                self.summary = response['content']\n",
    "            \n",
    "            self.summary_count += 1\n",
    "            \n",
    "            # Keep only most recent messages in buffer\n",
    "            self.messages = self.messages[-self.buffer_size:]\n",
    "    \n",
    "    def get_context(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get context to send to LLM\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: Summary + recent messages\n",
    "        \"\"\"\n",
    "        context = []\n",
    "        \n",
    "        # Add summary if exists\n",
    "        if self.summary:\n",
    "            context.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"CONVERSATION SUMMARY:\\n{self.summary}\"\n",
    "            })\n",
    "        \n",
    "        # Add recent messages\n",
    "        context.extend(self.messages)\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def get_token_count(self) -> int:\n",
    "        \"\"\"Get total tokens in context\"\"\"\n",
    "        return calculate_conversation_tokens(self.get_context())\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get memory statistics\"\"\"\n",
    "        return {\n",
    "            \"summary_count\": self.summary_count,\n",
    "            \"summary_tokens\": count_tokens(self.summary) if self.summary else 0,\n",
    "            \"buffer_message_count\": len(self.messages),\n",
    "            \"buffer_tokens\": calculate_conversation_tokens(self.messages),\n",
    "            \"total_tokens\": self.get_token_count(),\n",
    "            \"total_messages_processed\": len(self.all_messages)\n",
    "        }\n",
    "\n",
    "# Test summary memory\n",
    "print(\"CONVERSATION SUMMARY MEMORY TEST:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create summary memory (summarize every 6 messages, keep last 2)\n",
    "summary_memory = SummaryMemory(\n",
    "    summarize_every=6,\n",
    "    buffer_size=2,\n",
    "    max_summary_tokens=300\n",
    ")\n",
    "\n",
    "# Simulate longer conversation\n",
    "long_conversation = [\n",
    "    (\"user\", \"I'm interested in a home mortgage.\"),\n",
    "    (\"assistant\", \"Great! What's your budget and location?\"),\n",
    "    (\"user\", \"Budget is $500,000, looking in Portland.\"),\n",
    "    (\"assistant\", \"How much down payment do you have?\"),\n",
    "    (\"user\", \"I have $100,000 saved.\"),\n",
    "    (\"assistant\", \"Excellent! That's 20%. What's your credit score?\"),  # Triggers summarization\n",
    "    (\"user\", \"My credit score is 740.\"),\n",
    "    (\"assistant\", \"Perfect! With 740 credit, you qualify for premium rates.\"),\n",
    "    (\"user\", \"What documentation do I need?\"),\n",
    "    (\"assistant\", \"Tax returns, pay stubs, bank statements. Standard process.\"),\n",
    "    (\"user\", \"How long does pre-approval take?\"),\n",
    "    (\"assistant\", \"Usually 3-5 business days.\"),  # Another summarization\n",
    "    (\"user\", \"Great! And what interest rate would I get?\"),\n",
    "    (\"assistant\", \"For your profile, probably 6.25% - 6.75% on a 30-year fixed.\"),\n",
    "]\n",
    "\n",
    "for i, (role, content) in enumerate(long_conversation):\n",
    "    summary_memory.add_message(role, content)\n",
    "    stats = summary_memory.get_stats()\n",
    "    \n",
    "    print(f\"\\nMessage {i+1}: {role} - '{content[:50]}...'\")\n",
    "    print(f\"  Total messages processed: {stats['total_messages_processed']}\")\n",
    "    print(f\"  Summaries generated: {stats['summary_count']}\")\n",
    "    print(f\"  Current tokens: {stats['total_tokens']} (summary: {stats['summary_tokens']}, buffer: {stats['buffer_tokens']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nFINAL STATE:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nSUMMARY ({summary_memory.get_stats()['summary_tokens']} tokens):\")\n",
    "print(summary_memory.summary)\n",
    "\n",
    "print(f\"\\nRECENT BUFFER ({summary_memory.get_stats()['buffer_message_count']} messages):\")\n",
    "for msg in summary_memory.messages:\n",
    "    print(f\"  {msg['role']}: {msg['content']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Compare with naive approach\n",
    "naive_tokens = calculate_conversation_tokens([\n",
    "    {\"role\": role, \"content\": content}\n",
    "    for role, content in long_conversation\n",
    "])\n",
    "\n",
    "print(f\"\\nTOKEN COMPARISON:\")\n",
    "print(f\"  Naive (all messages): {naive_tokens} tokens\")\n",
    "print(f\"  Summary memory: {summary_memory.get_token_count()} tokens\")\n",
    "print(f\"  Savings: {naive_tokens - summary_memory.get_token_count()} tokens ({(1 - summary_memory.get_token_count()/naive_tokens)*100:.1f}%)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d05ca53",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "✓ **Scalable** - Handles long conversations  \n",
    "✓ **Token efficient** - Compresses history significantly  \n",
    "✓ **Preserves key facts** - Summarization retains important information  \n",
    "✗ **Lossy** - Some details are lost in summarization  \n",
    "✗ **Latency** - Summarization adds processing time  \n",
    "✗ **Cost** - Extra API calls for summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f64a787",
   "metadata": {},
   "source": [
    "## CHALLENGE 3: ENTITY MEMORY SYSTEM\n",
    "\n",
    "**Time:** 10 minutes  \n",
    "**Objective:** Track entities (facts) across conversation\n",
    "\n",
    "### Background\n",
    "\n",
    "Instead of summarizing conversations, extract and track specific entities: names, numbers, decisions, facts. This preserves precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a52b929",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecef14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Entity Memory System\n",
    "\n",
    "class EntityMemory:\n",
    "    \"\"\"\n",
    "    Entity-based conversation memory\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, buffer_size: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize entity memory\n",
    "        \n",
    "        Args:\n",
    "            buffer_size (int): Number of recent messages to keep\n",
    "        \"\"\"\n",
    "        self.entities = {}  # entity_type -> {key: value}\n",
    "        self.messages = []\n",
    "        self.all_messages = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add_message(self, role: str, content: str) -> None:\n",
    "        \"\"\"\n",
    "        Add message and extract entities\n",
    "        \n",
    "        Args:\n",
    "            role (str): 'user' or 'assistant'\n",
    "            content (str): Message content\n",
    "        \"\"\"\n",
    "        message = {\n",
    "            \"role\": role,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.messages.append(message)\n",
    "        self.all_messages.append(message)\n",
    "        \n",
    "        # Extract entities after each user message\n",
    "        if role == \"user\":\n",
    "            self._extract_entities(content)\n",
    "        \n",
    "        # Maintain buffer size\n",
    "        if len(self.messages) > self.buffer_size * 2:\n",
    "            self.messages = self.messages[-(self.buffer_size * 2):]\n",
    "    \n",
    "    def _extract_entities(self, user_message: str) -> None:\n",
    "        \"\"\"\n",
    "        Extract entities from user message\n",
    "        \n",
    "        Args:\n",
    "            user_message (str): User's message\n",
    "        \"\"\"\n",
    "        # Use LLM to extract entities\n",
    "        extraction_prompt = f\"\"\"\n",
    "Extract key entities from this user message in a loan advisory conversation.\n",
    "\n",
    "USER MESSAGE:\n",
    "{user_message}\n",
    "\n",
    "Extract in this JSON format:\n",
    "{{\n",
    "  \"applicant_info\": {{\"name\": \"...\", \"age\": ..., ...}},\n",
    "  \"property_info\": {{\"location\": \"...\", \"price\": ..., ...}},\n",
    "  \"loan_info\": {{\"amount\": ..., \"down_payment\": ..., ...}},\n",
    "  \"financial_info\": {{\"income\": ..., \"credit_score\": ..., ...}}\n",
    "}}\n",
    "\n",
    "Only include entities that are explicitly mentioned. Return empty {{}} for categories with no information.\n",
    "\n",
    "ENTITIES:\n",
    "\"\"\"\n",
    "        \n",
    "        response = call_gpt4(\n",
    "            [{\"role\": \"user\", \"content\": extraction_prompt}],\n",
    "            \"You are an entity extraction system. Output only valid JSON.\"\n",
    "        )\n",
    "        \n",
    "        if response['success']:\n",
    "            try:\n",
    "                # Parse JSON response\n",
    "                # Extract JSON from response (might have markdown backticks)\n",
    "                json_match = re.search(r'\\{.*\\}', response['content'], re.DOTALL)\n",
    "                if json_match:\n",
    "                    extracted = json.loads(json_match.group())\n",
    "                    \n",
    "                    # Merge with existing entities\n",
    "                    for entity_type, entities in extracted.items():\n",
    "                        if entities:  # Only if not empty\n",
    "                            if entity_type not in self.entities:\n",
    "                                self.entities[entity_type] = {}\n",
    "                            self.entities[entity_type].update(entities)\n",
    "            except json.JSONDecodeError:\n",
    "                pass  # Silently fail if JSON parsing fails\n",
    "    \n",
    "    def get_context(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get context with entities + recent buffer\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: Context messages\n",
    "        \"\"\"\n",
    "        context = []\n",
    "        \n",
    "        # Add entities as structured context\n",
    "        if self.entities:\n",
    "            entity_text = \"KNOWN INFORMATION:\\n\"\n",
    "            for entity_type, entities in self.entities.items():\n",
    "                entity_text += f\"\\n{entity_type.replace('_', ' ').title()}:\\n\"\n",
    "                for key, value in entities.items():\n",
    "                    entity_text += f\"  - {key}: {value}\\n\"\n",
    "            \n",
    "            context.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": entity_text.strip()\n",
    "            })\n",
    "        \n",
    "        # Add recent messages\n",
    "        context.extend(self.messages)\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def get_entity_summary(self) -> str:\n",
    "        \"\"\"Get readable summary of tracked entities\"\"\"\n",
    "        if not self.entities:\n",
    "            return \"No entities tracked yet.\"\n",
    "        \n",
    "        summary = []\n",
    "        for entity_type, entities in self.entities.items():\n",
    "            summary.append(f\"{entity_type.replace('_', ' ').title()}: {entities}\")\n",
    "        return \"\\n\".join(summary)\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get memory statistics\"\"\"\n",
    "        entity_count = sum(len(entities) for entities in self.entities.values())\n",
    "        entity_tokens = count_tokens(self.get_entity_summary())\n",
    "        \n",
    "        return {\n",
    "            \"entity_types\": len(self.entities),\n",
    "            \"total_entities\": entity_count,\n",
    "            \"entity_tokens\": entity_tokens,\n",
    "            \"buffer_messages\": len(self.messages),\n",
    "            \"buffer_tokens\": calculate_conversation_tokens(self.messages),\n",
    "            \"total_tokens\": calculate_conversation_tokens(self.get_context())\n",
    "        }\n",
    "\n",
    "# Test entity memory\n",
    "print(\"ENTITY MEMORY SYSTEM TEST:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "entity_memory = EntityMemory(buffer_size=2)\n",
    "\n",
    "# Simulate conversation with entity extraction\n",
    "entity_conversation = [\n",
    "    (\"user\", \"Hi, I'm Sarah Chen and I'm looking for a mortgage.\"),\n",
    "    (\"assistant\", \"Hello Sarah! I'd be happy to help you with a mortgage.\"),\n",
    "    (\"user\", \"I'm 35 years old and I found a house in Seattle for $600,000.\"),\n",
    "    (\"assistant\", \"Great location! How much down payment do you have?\"),\n",
    "    (\"user\", \"I can put down $120,000. My annual income is $95,000 and credit score is 750.\"),\n",
    "    (\"assistant\", \"Excellent! With 750 credit and 20% down, you'll get great rates.\"),\n",
    "    (\"user\", \"What rate can I expect?\"),\n",
    "    (\"assistant\", \"For your profile, we're seeing 6.25% - 6.75% on 30-year fixed.\"),\n",
    "]\n",
    "\n",
    "for i, (role, content) in enumerate(entity_conversation):\n",
    "    entity_memory.add_message(role, content)\n",
    "    stats = entity_memory.get_stats()\n",
    "    \n",
    "    print(f\"\\nMessage {i+1}: {role}\")\n",
    "    if role == \"user\":\n",
    "        print(f\"  Content: '{content}'\")\n",
    "        print(f\"  Entities tracked: {stats['total_entities']} across {stats['entity_types']} types\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nTRACKED ENTITIES:\")\n",
    "print(\"-\" * 80)\n",
    "print(entity_memory.get_entity_summary())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nRECENT BUFFER:\")\n",
    "print(\"-\" * 80)\n",
    "for msg in entity_memory.messages:\n",
    "    print(f\"  {msg['role']}: {msg['content'][:60]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Test entity recall\n",
    "test_query = \"What was my name, property price, and credit score again?\"\n",
    "\n",
    "context = entity_memory.get_context()\n",
    "response = call_gpt4(\n",
    "    context + [{\"role\": \"user\", \"content\": test_query}],\n",
    "    \"You are a loan advisor. Answer using known information.\"\n",
    ")\n",
    "\n",
    "print(f\"\\nENTITY RECALL TEST:\")\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Response: {response['content']}\")\n",
    "print(f\"Context tokens: {calculate_conversation_tokens(context)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b353ef15",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "✓ **Precise** - Preserves exact values  \n",
    "✓ **Efficient** - Very low token usage  \n",
    "✓ **Scalable** - Entities don't grow linearly  \n",
    "✓ **Queryable** - Easy to look up specific facts  \n",
    "✗ **Complex** - Requires entity extraction  \n",
    "✗ **Lossy** - Loses conversational flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5148f9",
   "metadata": {},
   "source": [
    "## CHALLENGE 4: HYBRID MEMORY APPROACH\n",
    "\n",
    "**Time:** 10 minutes  \n",
    "**Objective:** Combine multiple memory strategies\n",
    "\n",
    "### Background\n",
    "\n",
    "Production systems use hybrid approaches: entities for facts, buffer for flow, summaries for long-term history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a7a530",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Hybrid Memory System\n",
    "\n",
    "class HybridMemory:\n",
    "    \"\"\"\n",
    "    Production-grade hybrid memory combining all strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        buffer_size: int = 3,\n",
    "        summarize_every: int = 10,\n",
    "        track_entities: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize hybrid memory\n",
    "        \n",
    "        Args:\n",
    "            buffer_size (int): Recent messages to keep\n",
    "            summarize_every (int): Summarize after this many messages\n",
    "            track_entities (bool): Whether to extract entities\n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.summarize_every = summarize_every\n",
    "        self.track_entities = track_entities\n",
    "        \n",
    "        # Storage\n",
    "        self.entities = {}\n",
    "        self.summary = \"\"\n",
    "        self.messages = []\n",
    "        self.all_messages = []\n",
    "        self.summary_count = 0\n",
    "    \n",
    "    def add_message(self, role: str, content: str) -> None:\n",
    "        \"\"\"\n",
    "        Add message and manage memory\n",
    "        \n",
    "        Args:\n",
    "            role (str): 'user' or 'assistant'\n",
    "            content (str): Message content\n",
    "        \"\"\"\n",
    "        message = {\n",
    "            \"role\": role,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.messages.append(message)\n",
    "        self.all_messages.append(message)\n",
    "        \n",
    "        # Extract entities from user messages\n",
    "        if self.track_entities and role == \"user\":\n",
    "            self._extract_entities_simple(content)\n",
    "        \n",
    "        # Check if we need to summarize\n",
    "        if len(self.messages) >= self.summarize_every:\n",
    "            self._generate_summary()\n",
    "    \n",
    "    def _extract_entities_simple(self, text: str) -> None:\n",
    "        \"\"\"\n",
    "        Simplified entity extraction (pattern-based)\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to extract from\n",
    "        \"\"\"\n",
    "        # Extract numbers (amounts, credit scores, etc.)\n",
    "        numbers = re.findall(r'\\$?[\\d,]+', text)\n",
    "        \n",
    "        # Extract locations (basic pattern)\n",
    "        locations = re.findall(r'\\b[A-Z][a-z]+(?: [A-Z][a-z]+)*\\b', text)\n",
    "        \n",
    "        # Store in entities\n",
    "        if 'numbers_mentioned' not in self.entities:\n",
    "            self.entities['numbers_mentioned'] = set()\n",
    "        if 'locations_mentioned' not in self.entities:\n",
    "            self.entities['locations_mentioned'] = set()\n",
    "        \n",
    "        self.entities['numbers_mentioned'].update(numbers)\n",
    "        self.entities['locations_mentioned'].update([loc for loc in locations if len(loc) > 3])\n",
    "    \n",
    "    def _generate_summary(self) -> None:\n",
    "        \"\"\"Generate summary and compress buffer\"\"\"\n",
    "        # Create conversation text\n",
    "        conv_text = \"\\n\".join([\n",
    "            f\"{msg['role'].capitalize()}: {msg['content']}\"\n",
    "            for msg in self.messages[:-self.buffer_size]  # Don't summarize most recent\n",
    "        ])\n",
    "        \n",
    "        if not conv_text:\n",
    "            return\n",
    "        \n",
    "        summary_prompt = f\"\"\"\n",
    "Concisely summarize this conversation excerpt in 200 tokens or less.\n",
    "Focus on key facts, decisions, and important details.\n",
    "\n",
    "{conv_text}\n",
    "\"\"\"\n",
    "        \n",
    "        response = call_gpt4(\n",
    "            [{\"role\": \"user\", \"content\": summary_prompt}],\n",
    "            \"You are a summarizer. Be concise and factual.\"\n",
    "        )\n",
    "        \n",
    "        if response['success']:\n",
    "            if self.summary:\n",
    "                self.summary += f\"\\n{response['content']}\"\n",
    "            else:\n",
    "                self.summary = response['content']\n",
    "            \n",
    "            self.summary_count += 1\n",
    "            \n",
    "            # Keep only recent messages\n",
    "            self.messages = self.messages[-self.buffer_size * 2:]\n",
    "    \n",
    "    def get_context(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Build context from all memory components\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: Context messages\n",
    "        \"\"\"\n",
    "        context = []\n",
    "        \n",
    "        # 1. Add summary if exists\n",
    "        if self.summary:\n",
    "            context.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"CONVERSATION HISTORY:\\n{self.summary}\"\n",
    "            })\n",
    "        \n",
    "        # 2. Add entities if tracked\n",
    "        if self.entities:\n",
    "            entity_text = \"KEY INFORMATION MENTIONED:\\n\"\n",
    "            for entity_type, values in self.entities.items():\n",
    "                if values:\n",
    "                    entity_text += f\"- {entity_type.replace('_', ' ').title()}: {', '.join(str(v) for v in list(values)[:5])}\\n\"\n",
    "            \n",
    "            context.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": entity_text.strip()\n",
    "            })\n",
    "        \n",
    "        # 3. Add recent message buffer\n",
    "        context.extend(self.messages)\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get comprehensive memory statistics\"\"\"\n",
    "        return {\n",
    "            \"summary_exists\": bool(self.summary),\n",
    "            \"summary_tokens\": count_tokens(self.summary) if self.summary else 0,\n",
    "            \"summary_count\": self.summary_count,\n",
    "            \"entity_types\": len(self.entities),\n",
    "            \"buffer_messages\": len(self.messages),\n",
    "            \"buffer_tokens\": calculate_conversation_tokens(self.messages),\n",
    "            \"total_context_tokens\": calculate_conversation_tokens(self.get_context()),\n",
    "            \"messages_processed\": len(self.all_messages)\n",
    "        }\n",
    "\n",
    "# Test hybrid memory\n",
    "print(\"HYBRID MEMORY SYSTEM TEST:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "hybrid = HybridMemory(\n",
    "    buffer_size=2,\n",
    "    summarize_every=6,\n",
    "    track_entities=True\n",
    ")\n",
    "\n",
    "# Simulate extended conversation\n",
    "extended_conv = [\n",
    "    (\"user\", \"Hi, I'm looking for a $550,000 mortgage in Denver.\"),\n",
    "    (\"assistant\", \"Great! Tell me about your financial situation.\"),\n",
    "    (\"user\", \"I earn $105,000 annually and have $110,000 for down payment.\"),\n",
    "    (\"assistant\", \"Excellent 20% down! What's your credit score?\"),\n",
    "    (\"user\", \"It's 735.\"),\n",
    "    (\"assistant\", \"Perfect! You qualify for premium rates.\"),  # Triggers summarization\n",
    "    (\"user\", \"What interest rate can I expect?\"),\n",
    "    (\"assistant\", \"For your profile, 6.5% - 7.0% on 30-year fixed.\"),\n",
    "    (\"user\", \"How much would my monthly payment be?\"),\n",
    "    (\"assistant\", \"Principal & interest around $2,780/month.\"),\n",
    "    (\"user\", \"What about property taxes?\"),\n",
    "    (\"assistant\", \"Denver taxes are ~0.5%, so about $2,290/year or $190/month.\"),  # Another summarization\n",
    "    (\"user\", \"Total monthly payment?\"),\n",
    "    (\"assistant\", \"Around $3,100/month including taxes and insurance.\"),\n",
    "]\n",
    "\n",
    "for i, (role, content) in enumerate(extended_conv):\n",
    "    hybrid.add_message(role, content)\n",
    "    \n",
    "    if (i + 1) % 4 == 0:  # Print stats every 4 messages\n",
    "        stats = hybrid.get_stats()\n",
    "        print(f\"\\nAfter message {i+1}:\")\n",
    "        print(f\"  Summary: {stats['summary_exists']} ({stats['summary_tokens']} tokens)\")\n",
    "        print(f\"  Entities: {stats['entity_types']} types tracked\")\n",
    "        print(f\"  Buffer: {stats['buffer_messages']} messages ({stats['buffer_tokens']} tokens)\")\n",
    "        print(f\"  Total context: {stats['total_context_tokens']} tokens\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nFINAL HYBRID MEMORY STATE:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "stats = hybrid.get_stats()\n",
    "\n",
    "print(f\"\\n1. SUMMARY ({stats['summary_tokens']} tokens):\")\n",
    "if hybrid.summary:\n",
    "    print(f\"   {hybrid.summary[:200]}...\")\n",
    "else:\n",
    "    print(\"   (No summary generated yet)\")\n",
    "\n",
    "print(f\"\\n2. ENTITIES ({stats['entity_types']} types):\")\n",
    "for entity_type, values in hybrid.entities.items():\n",
    "    print(f\"   - {entity_type}: {list(values)[:3]}\")\n",
    "\n",
    "print(f\"\\n3. RECENT BUFFER ({stats['buffer_messages']} messages):\")\n",
    "for msg in hybrid.messages[-4:]:\n",
    "    print(f\"   {msg['role']}: {msg['content'][:50]}...\")\n",
    "\n",
    "print(f\"\\n4. TOTAL CONTEXT: {stats['total_context_tokens']} tokens\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Compare with naive approach\n",
    "naive_tokens = calculate_conversation_tokens([\n",
    "    {\"role\": role, \"content\": content}\n",
    "    for role, content in extended_conv\n",
    "])\n",
    "\n",
    "print(f\"\\nEFFICIENCY COMPARISON:\")\n",
    "print(f\"  Naive (all {len(extended_conv)} messages): {naive_tokens} tokens\")\n",
    "print(f\"  Hybrid memory: {stats['total_context_tokens']} tokens\")\n",
    "print(f\"  Savings: {naive_tokens - stats['total_context_tokens']} tokens ({(1 - stats['total_context_tokens']/naive_tokens)*100:.1f}%)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f0c60",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "✓ **Best of all worlds** - Combines strengths of each approach  \n",
    "✓ **Production-ready** - Handles long conversations efficiently  \n",
    "✓ **Flexible** - Can tune each component independently  \n",
    "✓ **Accurate** - Preserves facts (entities) and flow (buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954dd4ff",
   "metadata": {},
   "source": [
    "## CHALLENGE 5: PRODUCTION STATE MANAGER\n",
    "\n",
    "**Time:** 10 minutes  \n",
    "**Objective:** Build complete production-ready conversation manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea253055",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52374f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Production Conversation State Manager\n",
    "\n",
    "class ConversationStateManager:\n",
    "    \"\"\"\n",
    "    Production-grade conversation state management\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        conversation_id: str,\n",
    "        token_budget: int = 3000,\n",
    "        strategy: str = \"hybrid\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize state manager\n",
    "        \n",
    "        Args:\n",
    "            conversation_id (str): Unique conversation ID\n",
    "            token_budget (int): Maximum tokens for conversation state\n",
    "            strategy (str): Memory strategy ('buffer', 'summary', 'entity', 'hybrid')\n",
    "        \"\"\"\n",
    "        self.conversation_id = conversation_id\n",
    "        self.token_budget = token_budget\n",
    "        self.strategy = strategy\n",
    "        \n",
    "        # Initialize appropriate memory system\n",
    "        if strategy == \"buffer\":\n",
    "            self.memory = BufferMemory(max_pairs=5)\n",
    "        elif strategy == \"summary\":\n",
    "            self.memory = SummaryMemory(summarize_every=8, buffer_size=3)\n",
    "        elif strategy == \"entity\":\n",
    "            self.memory = EntityMemory(buffer_size=3)\n",
    "        elif strategy == \"hybrid\":\n",
    "            self.memory = HybridMemory(buffer_size=3, summarize_every=8)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "        \n",
    "        self.turn_count = 0\n",
    "        self.created_at = datetime.now().isoformat()\n",
    "    \n",
    "    def add_turn(self, user_message: str, system_prompt: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a conversation turn\n",
    "        \n",
    "        Args:\n",
    "            user_message (str): User's message\n",
    "            system_prompt (str): Optional system prompt\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Assistant response with metadata\n",
    "        \"\"\"\n",
    "        self.turn_count += 1\n",
    "        \n",
    "        # Add user message to memory\n",
    "        self.memory.add_message(\"user\", user_message)\n",
    "        \n",
    "        # Get context from memory\n",
    "        context = self.memory.get_context() if hasattr(self.memory, 'get_context') else self.memory.get_messages()\n",
    "        \n",
    "        # Check token budget\n",
    "        context_tokens = calculate_conversation_tokens(context)\n",
    "        if context_tokens > self.token_budget:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Context exceeds budget: {context_tokens} > {self.token_budget}\",\n",
    "                \"content\": \"\"\n",
    "            }\n",
    "        \n",
    "        # Call LLM\n",
    "        response = call_gpt4(\n",
    "            context,\n",
    "            system_prompt or \"You are a helpful loan advisor assistant.\"\n",
    "        )\n",
    "        \n",
    "        if response['success']:\n",
    "            # Add assistant response to memory\n",
    "            self.memory.add_message(\"assistant\", response['content'])\n",
    "        \n",
    "        # Add metadata\n",
    "        response['turn_number'] = self.turn_count\n",
    "        response['context_tokens'] = context_tokens\n",
    "        response['strategy'] = self.strategy\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"Get conversation metrics\"\"\"\n",
    "        memory_stats = self.memory.get_stats()\n",
    "        \n",
    "        return {\n",
    "            \"conversation_id\": self.conversation_id,\n",
    "            \"turn_count\": self.turn_count,\n",
    "            \"strategy\": self.strategy,\n",
    "            \"created_at\": self.created_at,\n",
    "            **memory_stats\n",
    "        }\n",
    "    \n",
    "    def export_state(self) -> Dict:\n",
    "        \"\"\"Export conversation state for persistence\"\"\"\n",
    "        return {\n",
    "            \"conversation_id\": self.conversation_id,\n",
    "            \"turn_count\": self.turn_count,\n",
    "            \"strategy\": self.strategy,\n",
    "            \"created_at\": self.created_at,\n",
    "            \"memory_state\": {\n",
    "                \"entities\": getattr(self.memory, 'entities', {}),\n",
    "                \"summary\": getattr(self.memory, 'summary', ''),\n",
    "                \"messages\": getattr(self.memory, 'messages', [])\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Test production state manager\n",
    "print(\"PRODUCTION STATE MANAGER TEST:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create conversation manager\n",
    "manager = ConversationStateManager(\n",
    "    conversation_id=\"CONV-2026-001\",\n",
    "    token_budget=2000,\n",
    "    strategy=\"hybrid\"\n",
    ")\n",
    "\n",
    "# Simulate realistic multi-turn conversation\n",
    "conversation_turns = [\n",
    "    \"Hi, I need help with a mortgage application.\",\n",
    "    \"I'm looking at a $480,000 home in Austin, Texas.\",\n",
    "    \"I have $96,000 for down payment and my credit score is 710.\",\n",
    "    \"My annual income is $92,000. What rate can I get?\",\n",
    "    \"What documents do I need to provide?\",\n",
    "    \"How long will the approval process take?\",\n",
    "    \"Can I get pre-approved today?\",\n",
    "    \"What's the next step?\"\n",
    "]\n",
    "\n",
    "for i, user_msg in enumerate(conversation_turns):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TURN {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"User: {user_msg}\")\n",
    "    \n",
    "    response = manager.add_turn(user_msg)\n",
    "    \n",
    "    if response['success']:\n",
    "        print(f\"\\nAssistant: {response['content'][:150]}...\")\n",
    "        print(f\"\\nMetrics:\")\n",
    "        print(f\"  Context tokens: {response['context_tokens']}/{manager.token_budget}\")\n",
    "        print(f\"  Total tokens: {response['total_tokens']}\")\n",
    "        print(f\"  Turn: {response['turn_number']}\")\n",
    "    else:\n",
    "        print(f\"\\nError: {response['error']}\")\n",
    "        break\n",
    "\n",
    "# Final metrics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL CONVERSATION METRICS:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "metrics = manager.get_metrics()\n",
    "for key, value in metrics.items():\n",
    "    if not isinstance(value, (dict, list)):\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Export state\n",
    "state = manager.export_state()\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EXPORTABLE STATE:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(json.dumps(state, indent=2, default=str)[:500] + \"...\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eace419",
   "metadata": {},
   "source": [
    "### Key Features\n",
    "\n",
    "✓ **Turn-based management** - Clean API for conversation flow  \n",
    "✓ **Token budget enforcement** - Prevents context overflow  \n",
    "✓ **Strategy selection** - Choose memory approach  \n",
    "✓ **Metrics tracking** - Monitor conversation health  \n",
    "✓ **State export** - Persistence ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a3d35f",
   "metadata": {},
   "source": [
    "## LAB SUMMARY\n",
    "\n",
    "\n",
    "### Token Savings Analysis\n",
    "\n",
    "```\n",
    "30-turn conversation (60 messages)\n",
    "\n",
    "Naive approach: ~12,000 tokens\n",
    "Buffer (last 6 messages): ~600 tokens (95% savings)\n",
    "Summary: ~800 tokens (93% savings)  \n",
    "Entity: ~400 tokens (97% savings)\n",
    "Hybrid: ~1,200 tokens (90% savings, highest accuracy)\n",
    "```\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "Before deploying stateful conversations:\n",
    "\n",
    "- [ ] Choose appropriate memory strategy\n",
    "- [ ] Set token budget (typically 2,000-4,000)\n",
    "- [ ] Implement conversation ID tracking\n",
    "- [ ] Add state persistence (database)\n",
    "- [ ] Monitor token usage per conversation\n",
    "- [ ] Set conversation timeout/max turns\n",
    "- [ ] Implement conversation reset capability\n",
    "- [ ] Test with realistic conversation lengths\n",
    "- [ ] Handle summarization failures gracefully\n",
    "- [ ] Log conversation metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
