{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79fe646a",
   "metadata": {},
   "source": [
    "# LAB 3: LANGCHAIN ORCHESTRATION - COMPLETE GUIDE\n",
    "\n",
    "**Course:** Advanced Prompt Engineering Training  \n",
    "**Session:** Session 3 - RAG & Advanced Retrieval (Day 2)  \n",
    "**Duration:** 120 minutes (2 hours)  \n",
    "**Type:** LangChain-Based Orchestration Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7170e3b9",
   "metadata": {},
   "source": [
    "## LAB OVERVIEW\n",
    "\n",
    "This comprehensive lab teaches you to build **production-grade orchestrated AI workflows using LangChain**. You'll progress through four interconnected modules:\n",
    "\n",
    "1. **Sequential Chains (LCEL)** - Linear multi-step workflows with LangChain Expression Language\n",
    "2. **Parallel Processing (RunnableParallel)** - Concurrent execution for speed\n",
    "3. **Conditional Workflows (RunnableBranch)** - Dynamic routing based on data\n",
    "4. **Production Orchestration** - Error handling, retries, monitoring with LangChain\n",
    "\n",
    "**Scenario:** You're building an AI-powered loan processing system for a bank. Starting with simple sequential workflows, you'll evolve to a production-ready system handling thousands of applications daily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48df07a0",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683acb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 3: LangChain Orchestration\n",
    "# Advanced Prompt Engineering Training - Session 3\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional, Any, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough,\n",
    "    RunnableParallel,\n",
    "    RunnableLambda,\n",
    "    RunnableBranch,\n",
    "    Runnable\n",
    ")\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"✓ LangChain version ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777f845",
   "metadata": {},
   "source": [
    "### Step 2: Configure LangChain Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LangChain ChatOpenAI models\n",
    "GPT4_MODEL = os.environ.get(\"MODEL_NAME\", \"gpt-4o\")\n",
    "GPT35_MODEL = os.environ.get(\"FAST_MODEL_NAME\", \"gpt-3.5-turbo\")\n",
    "\n",
    "# Create model instances\n",
    "llm_gpt4 = ChatOpenAI(\n",
    "    model=GPT4_MODEL,\n",
    "    temperature=0,\n",
    "    max_tokens=2000,\n",
    "    model_kwargs={\"top_p\": 1.0}\n",
    ")\n",
    "\n",
    "llm_gpt35 = ChatOpenAI(\n",
    "    model=GPT35_MODEL,\n",
    "    temperature=0,\n",
    "    max_tokens=2000,\n",
    "    model_kwargs={\"top_p\": 1.0}\n",
    ")\n",
    "\n",
    "# Model with retry configuration\n",
    "llm_gpt4_with_retry = ChatOpenAI(\n",
    "    model=GPT4_MODEL,\n",
    "    temperature=0,\n",
    "    max_tokens=2000,\n",
    "    max_retries=3,\n",
    "    request_timeout=30\n",
    ")\n",
    "\n",
    "print(f\"✓ LangChain ChatOpenAI configured\")\n",
    "print(f\"✓ Primary model: {GPT4_MODEL}\")\n",
    "print(f\"✓ Fast model: {GPT35_MODEL}\")\n",
    "print(f\"✓ Retry-enabled model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c72fbf4",
   "metadata": {},
   "source": [
    "### Step 3: Create Helper Functions and Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f797dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChainMetrics:\n",
    "    \"\"\"Track metrics for chain execution\"\"\"\n",
    "    chain_name: str\n",
    "    start_time: float\n",
    "    end_time: float = 0.0\n",
    "    output: Any = None\n",
    "    error: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> float:\n",
    "        return self.end_time - self.start_time if self.end_time > 0 else 0.0\n",
    "    \n",
    "    def complete(self, output: Any = None, error: Optional[str] = None):\n",
    "        self.end_time = time.time()\n",
    "        self.output = output\n",
    "        self.error = error\n",
    "\n",
    "\n",
    "def print_chain_output(output: Any, title: str = \"Chain Output\", metrics: Optional[ChainMetrics] = None):\n",
    "    \"\"\"Pretty print chain output with optional metrics\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{title}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if metrics:\n",
    "        print(f\"Duration: {metrics.duration:.2f}s\")\n",
    "        print(f\"\")\n",
    "    \n",
    "    if isinstance(output, dict):\n",
    "        print(json.dumps(output, indent=2))\n",
    "    else:\n",
    "        print(output)\n",
    "    \n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "# Test the setup\n",
    "test_chain = llm_gpt35 | StrOutputParser()\n",
    "test_output = test_chain.invoke(\"Say 'LangChain setup complete!' if you receive this.\")\n",
    "print_chain_output(test_output, \"Setup Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff783c69",
   "metadata": {},
   "source": [
    "### Step 4: Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e8c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample loan application data for the lab\n",
    "SAMPLE_LOAN_APPLICATION = \"\"\"\n",
    "LOAN APPLICATION #LA-2024-5821\n",
    "\n",
    "Applicant Information:\n",
    "- Name: Sarah Johnson\n",
    "- Age: 34\n",
    "- Employment: Senior Software Engineer at TechCorp\n",
    "- Annual Income: $145,000\n",
    "- Employment Duration: 6 years\n",
    "- Credit Score: 750\n",
    "\n",
    "Loan Details:\n",
    "- Loan Type: Home Mortgage\n",
    "- Loan Amount: $485,000\n",
    "- Property Value: $620,000\n",
    "- Down Payment: $135,000 (22%)\n",
    "- Loan Term: 30 years\n",
    "\n",
    "Financial Information:\n",
    "- Monthly Gross Income: $12,083\n",
    "- Existing Debts: \n",
    "  * Car Loan: $425/month (12 months remaining)\n",
    "  * Student Loan: $280/month (24 months remaining)\n",
    "  * Credit Card: $150/month average\n",
    "- Total Monthly Debt: $855\n",
    "\n",
    "Property Information:\n",
    "- Property Address: 245 Oak Street, San Francisco, CA\n",
    "- Property Type: Single-family home\n",
    "- Year Built: 2018\n",
    "- Square Footage: 2,100 sq ft\n",
    "- Condition: Excellent\n",
    "\n",
    "Supporting Documents:\n",
    "- 2 years tax returns: Provided\n",
    "- 3 months pay stubs: Provided\n",
    "- Bank statements: Provided\n",
    "- Employment verification: Verified\n",
    "- Property appraisal: $620,000\n",
    "\"\"\"\n",
    "\n",
    "# Additional test applications\n",
    "SMALL_LOAN_APP = \"\"\"\n",
    "LOAN APPLICATION #LA-2024-5822\n",
    "Applicant: Mike Chen\n",
    "Loan Type: Personal Loan\n",
    "Amount: $15,000\n",
    "Income: $65,000\n",
    "Credit Score: 720\n",
    "Employment: 3 years\n",
    "\"\"\"\n",
    "\n",
    "LARGE_LOAN_APP = \"\"\"\n",
    "LOAN APPLICATION #LA-2024-5823\n",
    "Applicant: Jennifer Martinez\n",
    "Loan Type: Commercial Real Estate\n",
    "Amount: $2,500,000\n",
    "Business Revenue: $8,000,000/year\n",
    "Credit Score: 780\n",
    "Business Age: 12 years\n",
    "Collateral: Commercial property valued at $3,200,000\n",
    "\"\"\"\n",
    "\n",
    "print(\"✓ Sample data loaded\")\n",
    "print(f\"✓ Main application: {SAMPLE_LOAN_APPLICATION.split()[3]}\")\n",
    "print(f\"✓ Test applications: 2 additional scenarios ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30556156",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 1: SEQUENTIAL CHAINS WITH LCEL (Lab 3.1)\n",
    "\n",
    "**Duration:** 30 minutes  \n",
    "**Objective:** Build linear multi-step workflows using LangChain Expression Language\n",
    "\n",
    "### Theory: LCEL Sequential Pattern\n",
    "\n",
    "**Concept:** Chain components together using the pipe operator `|`\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | output_parser\n",
    "result = chain.invoke(input)\n",
    "```\n",
    "\n",
    "**Advantages over manual implementation:**\n",
    "- Composable and reusable\n",
    "- Built-in streaming support\n",
    "- Automatic retry handling\n",
    "- Better debugging and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e344c13",
   "metadata": {},
   "source": [
    "### Challenge 1.1: Basic LCEL Sequential Chain (10 minutes)\n",
    "\n",
    "**Scenario:** Process a loan application through 3 steps using LCEL:\n",
    "1. Extract key information\n",
    "2. Calculate financial ratios\n",
    "3. Generate summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929a02fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create extraction chain with JSON output\n",
    "extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a data extraction specialist. Always return valid JSON.\"),\n",
    "    (\"user\", \"\"\"Extract the following information from this loan application and return as JSON:\n",
    "    \n",
    "Required fields:\n",
    "- applicant_name\n",
    "- loan_amount (number only)\n",
    "- annual_income (number only)\n",
    "- credit_score (number only)\n",
    "- monthly_debt (total monthly debt payments, number only)\n",
    "- loan_type\n",
    "- property_value (if applicable, number only)\n",
    "- down_payment (if applicable, number only)\n",
    "\n",
    "Loan Application:\n",
    "{application}\n",
    "\n",
    "Return ONLY valid JSON, no other text.\"\"\")\n",
    "])\n",
    "\n",
    "# Create the extraction chain\n",
    "extraction_chain = extraction_prompt | llm_gpt35 | JsonOutputParser()\n",
    "\n",
    "# Test extraction\n",
    "print(\"Testing extraction chain...\")\n",
    "extraction_result = extraction_chain.invoke({\"application\": SAMPLE_LOAN_APPLICATION})\n",
    "print_chain_output(extraction_result, \"Step 1: Extraction Result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956960d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create ratio calculation chain\n",
    "ratio_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a financial analyst. Always return valid JSON with accurate calculations.\"),\n",
    "    (\"user\", \"\"\"Given this extracted loan application data, calculate these financial metrics:\n",
    "    \n",
    "Extracted Data:\n",
    "{extracted_data}\n",
    "\n",
    "Calculate:\n",
    "1. DTI (Debt-to-Income Ratio): (monthly_debt / monthly_income) × 100\n",
    "   - monthly_income = annual_income / 12\n",
    "   - Express as percentage\n",
    "\n",
    "2. LTV (Loan-to-Value Ratio): (loan_amount / property_value) × 100\n",
    "   - If property_value exists\n",
    "   - Express as percentage\n",
    "\n",
    "3. Estimated Monthly Payment: Rough estimate for 30-year mortgage at 7% interest\n",
    "   - Use formula: P = L[c(1 + c)^n]/[(1 + c)^n - 1]\n",
    "   - Where L = loan amount, c = monthly interest rate, n = number of payments\n",
    "\n",
    "Return as JSON with:\n",
    "{{\n",
    "    \"dti_ratio\": <percentage>,\n",
    "    \"dti_assessment\": \"<Good/Fair/Poor based on: <28% = Good, 28-36% = Fair, >36% = Poor>\",\n",
    "    \"ltv_ratio\": <percentage or null>,\n",
    "    \"ltv_assessment\": \"<Good/Fair/Poor based on: <80% = Good, 80-90% = Fair, >90% = Poor>\",\n",
    "    \"estimated_monthly_payment\": <amount>,\n",
    "    \"affordability_check\": \"<Can afford based on 28% housing ratio rule>\"\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON.\"\"\")\n",
    "])\n",
    "\n",
    "ratio_chain = ratio_prompt | llm_gpt35 | JsonOutputParser()\n",
    "\n",
    "# Test ratio calculation\n",
    "print(\"Testing ratio calculation chain...\")\n",
    "ratio_result = ratio_chain.invoke({\"extracted_data\": json.dumps(extraction_result)})\n",
    "print_chain_output(ratio_result, \"Step 2: Ratio Calculation Result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create summary generation chain\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a senior loan officer writing executive summaries for the credit committee.\"),\n",
    "    (\"user\", \"\"\"Create an executive summary for this loan application.\n",
    "    \n",
    "Applicant Data:\n",
    "{extracted_data}\n",
    "\n",
    "Financial Analysis:\n",
    "{ratios}\n",
    "\n",
    "Write a concise executive summary (3-4 paragraphs) covering:\n",
    "\n",
    "1. Applicant Overview\n",
    "   - Name, income, credit score\n",
    "   - Employment stability\n",
    "\n",
    "2. Financial Strength\n",
    "   - DTI and LTV ratios\n",
    "   - Overall financial health\n",
    "\n",
    "3. Risk Assessment\n",
    "   - Key strengths\n",
    "   - Potential concerns\n",
    "\n",
    "4. Recommendation\n",
    "   - APPROVE: Strong application, low risk\n",
    "   - REVIEW: Acceptable but needs senior review\n",
    "   - DENY: High risk, does not meet criteria\n",
    "\n",
    "Be professional and concise. Focus on facts and ratios.\"\"\")\n",
    "])\n",
    "\n",
    "summary_chain = summary_prompt | llm_gpt4 | StrOutputParser()\n",
    "\n",
    "# Test summary generation\n",
    "print(\"Testing summary generation chain...\")\n",
    "summary_result = summary_chain.invoke({\n",
    "    \"extracted_data\": json.dumps(extraction_result),\n",
    "    \"ratios\": json.dumps(ratio_result)\n",
    "})\n",
    "print_chain_output(summary_result, \"Step 3: Executive Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e45cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compose them into a complete sequential chain using LCEL\n",
    "def format_for_ratios(data):\n",
    "    \"\"\"Helper function to format extraction output for ratio chain\"\"\"\n",
    "    return {\"extracted_data\": json.dumps(data)}\n",
    "\n",
    "def format_for_summary(data):\n",
    "    \"\"\"Helper function to format both outputs for summary chain\"\"\"\n",
    "    return {\n",
    "        \"extracted_data\": json.dumps(data.get(\"extraction\")),\n",
    "        \"ratios\": json.dumps(data.get(\"ratios\"))\n",
    "    }\n",
    "\n",
    "# Complete sequential chain with LCEL\n",
    "complete_sequential_chain = (\n",
    "    {\"application\": RunnablePassthrough()}\n",
    "    | extraction_prompt \n",
    "    | llm_gpt35 \n",
    "    | JsonOutputParser()\n",
    "    | RunnableLambda(lambda x: {\"extraction\": x})\n",
    "    | RunnablePassthrough.assign(\n",
    "        ratios=lambda x: ratio_chain.invoke({\"extracted_data\": json.dumps(x[\"extraction\"])})\n",
    "    )\n",
    "    | RunnablePassthrough.assign(\n",
    "        summary=lambda x: summary_chain.invoke({\n",
    "            \"extracted_data\": json.dumps(x[\"extraction\"]),\n",
    "            \"ratios\": json.dumps(x[\"ratios\"])\n",
    "        })\n",
    "    )\n",
    ")\n",
    "\n",
    "# Execute the complete chain\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXECUTING COMPLETE SEQUENTIAL CHAIN WITH LCEL\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "final_result = complete_sequential_chain.invoke(SAMPLE_LOAN_APPLICATION)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEQUENTIAL CHAIN COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total Time: {total_time:.2f}s\")\n",
    "print(f\"Steps Executed: 3 (extraction → ratios → summary)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(final_result[\"summary\"])\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbde6d0",
   "metadata": {},
   "source": [
    "### Challenge 1.2: Enhanced Sequential Chain with State Tracking (10 minutes)\n",
    "\n",
    "**Objective:** Add comprehensive logging using LangChain's callback system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8569a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.outputs import LLMResult\n",
    "\n",
    "class LoanProcessingCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"Custom callback handler to track chain execution\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "        self.total_tokens = 0\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs) -> None:\n",
    "        self.step_count += 1\n",
    "        timestamp = datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "        log_msg = f\"[{timestamp}] Step {self.step_count}: LLM call started\"\n",
    "        self.logs.append(log_msg)\n",
    "        print(log_msg)\n",
    "    \n",
    "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
    "        timestamp = datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "        if response.llm_output and \"token_usage\" in response.llm_output:\n",
    "            tokens = response.llm_output[\"token_usage\"].get(\"total_tokens\", 0)\n",
    "            self.total_tokens += tokens\n",
    "            log_msg = f\"[{timestamp}] ✓ Step {self.step_count} completed ({tokens} tokens)\"\n",
    "        else:\n",
    "            log_msg = f\"[{timestamp}] ✓ Step {self.step_count} completed\"\n",
    "        self.logs.append(log_msg)\n",
    "        print(log_msg)\n",
    "    \n",
    "    def on_llm_error(self, error: Exception, **kwargs) -> None:\n",
    "        timestamp = datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "        log_msg = f\"[{timestamp}] ✗ Step {self.step_count} failed: {str(error)}\"\n",
    "        self.logs.append(log_msg)\n",
    "        print(log_msg)\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"total_steps\": self.step_count,\n",
    "            \"total_tokens\": self.total_tokens,\n",
    "            \"logs\": self.logs\n",
    "        }\n",
    "\n",
    "# Execute chain with callback handler\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATEFUL SEQUENTIAL WORKFLOW WITH CALLBACKS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "callback_handler = LoanProcessingCallbackHandler()\n",
    "\n",
    "start_time = time.time()\n",
    "result_with_tracking = complete_sequential_chain.invoke(\n",
    "    SAMPLE_LOAN_APPLICATION,\n",
    "    config={\"callbacks\": [callback_handler]}\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Display summary\n",
    "summary = callback_handler.get_summary()\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCESSING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total Steps: {summary['total_steps']}\")\n",
    "print(f\"Total Tokens: {summary['total_tokens']}\")\n",
    "print(f\"Total Time: {total_time:.2f}s\")\n",
    "print(\"\\nProcessing Log:\")\n",
    "for log in summary['logs']:\n",
    "    print(f\"  {log}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e589de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 2: PARALLEL PROCESSING WITH RUNNABLEPARALLEL (Lab 3.2)\n",
    "\n",
    "**Duration:** 30 minutes  \n",
    "**Objective:** Execute independent tasks concurrently using RunnableParallel\n",
    "\n",
    "### Theory: RunnableParallel Pattern\n",
    "\n",
    "**Concept:** Execute multiple chains concurrently and collect results\n",
    "\n",
    "```python\n",
    "parallel_chain = RunnableParallel(\n",
    "    task_a=chain_a,\n",
    "    task_b=chain_b,\n",
    "    task_c=chain_c\n",
    ")\n",
    "results = parallel_chain.invoke(input)  # All run concurrently!\n",
    "```\n",
    "\n",
    "**Expected speedup:** ~3x for 3 parallel tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b7339",
   "metadata": {},
   "source": [
    "### Challenge 2.1: Parallel Credit Checks with RunnableParallel (15 minutes)\n",
    "\n",
    "**Scenario:** Verify loan application across 3 independent sources:\n",
    "1. Credit bureau check\n",
    "2. Employment verification\n",
    "3. Identity verification\n",
    "\n",
    "All run in parallel with RunnableParallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9059c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define verification prompts\n",
    "credit_bureau_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a credit bureau API returning verification results.\"),\n",
    "    (\"user\", \"\"\"Perform credit bureau verification for this applicant:\n",
    "    \n",
    "Applicant Data:\n",
    "{applicant_data}\n",
    "\n",
    "Verify and return JSON with:\n",
    "{{\n",
    "    \"credit_score\": <number>,\n",
    "    \"credit_score_valid\": <true/false>,\n",
    "    \"payment_history\": \"<Excellent/Good/Fair/Poor>\",\n",
    "    \"outstanding_debts\": <total amount>,\n",
    "    \"bankruptcy_history\": <true/false>,\n",
    "    \"verification_status\": \"VERIFIED\" or \"FAILED\",\n",
    "    \"notes\": \"<any concerns or all clear>\"\n",
    "}}\n",
    "\n",
    "Simulate realistic credit bureau response. Use the credit score from applicant_data.\"\"\")\n",
    "])\n",
    "\n",
    "employment_verification_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an employment verification service API.\"),\n",
    "    (\"user\", \"\"\"Verify employment for this applicant:\n",
    "    \n",
    "Applicant Data:\n",
    "{applicant_data}\n",
    "\n",
    "Return JSON with:\n",
    "{{\n",
    "    \"employer\": \"<company name>\",\n",
    "    \"position\": \"<job title>\",\n",
    "    \"employment_verified\": <true/false>,\n",
    "    \"tenure_years\": <number>,\n",
    "    \"annual_income_verified\": <number>,\n",
    "    \"income_matches_stated\": <true/false>,\n",
    "    \"employment_stable\": <true/false>,\n",
    "    \"verification_status\": \"VERIFIED\" or \"FAILED\",\n",
    "    \"notes\": \"<any discrepancies or all clear>\"\n",
    "}}\n",
    "\n",
    "Use data from applicant_data. Simulate realistic employment verification.\"\"\")\n",
    "])\n",
    "\n",
    "identity_verification_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an identity verification service API.\"),\n",
    "    (\"user\", \"\"\"Verify identity for this applicant:\n",
    "    \n",
    "Applicant Data:\n",
    "{applicant_data}\n",
    "\n",
    "Return JSON with:\n",
    "{{\n",
    "    \"identity_verified\": <true/false>,\n",
    "    \"name_matches\": <true/false>,\n",
    "    \"address_verified\": <true/false>,\n",
    "    \"fraud_flags\": <number of flags, 0 = none>,\n",
    "    \"risk_level\": \"<LOW/MEDIUM/HIGH>\",\n",
    "    \"verification_status\": \"VERIFIED\" or \"FAILED\",\n",
    "    \"notes\": \"<any red flags or all clear>\"\n",
    "}}\n",
    "\n",
    "Simulate realistic identity verification check.\"\"\")\n",
    "])\n",
    "\n",
    "# Create individual verification chains\n",
    "credit_check_chain = credit_bureau_prompt | llm_gpt35 | JsonOutputParser()\n",
    "employment_check_chain = employment_verification_prompt | llm_gpt35 | JsonOutputParser()\n",
    "identity_check_chain = identity_verification_prompt | llm_gpt35 | JsonOutputParser()\n",
    "\n",
    "print(\"✓ Verification chains created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3702dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parallel verification chain using RunnableParallel\n",
    "parallel_verification_chain = RunnableParallel(\n",
    "    credit_bureau=credit_check_chain,\n",
    "    employment=employment_check_chain,\n",
    "    identity=identity_check_chain\n",
    ")\n",
    "\n",
    "# Test data\n",
    "test_applicant = {\n",
    "    \"name\": \"Sarah Johnson\",\n",
    "    \"annual_income\": 145000,\n",
    "    \"credit_score\": 750,\n",
    "    \"employer\": \"TechCorp\",\n",
    "    \"position\": \"Senior Software Engineer\",\n",
    "    \"employment_years\": 6,\n",
    "    \"address\": \"245 Oak Street, San Francisco, CA\"\n",
    "}\n",
    "\n",
    "# Execute parallel verifications\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARALLEL VERIFICATION CHECKS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Starting parallel execution...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "verification_results = parallel_verification_chain.invoke({\n",
    "    \"applicant_data\": json.dumps(test_applicant, indent=2)\n",
    "})\n",
    "wall_clock_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARALLEL VERIFICATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Wall Clock Time: {wall_clock_time:.2f}s\")\n",
    "print(f\"\\nNote: If run sequentially, this would take ~3x longer!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for check_name, result in verification_results.items():\n",
    "    print(f\"\\n{check_name.replace('_', ' ').title()}:\")\n",
    "    for key, value in result.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa0492",
   "metadata": {},
   "source": [
    "### Challenge 2.2: Hybrid Sequential + Parallel Chain (15 minutes)\n",
    "\n",
    "**Objective:** Combine sequential and parallel patterns in one chain\n",
    "\n",
    "```\n",
    "Step 1: Extract data (sequential)\n",
    "    ↓\n",
    "Step 2: Fork into parallel checks → Join\n",
    "    ↓\n",
    "Step 3: Synthesize results (sequential)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the synthesis chain\n",
    "synthesis_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a senior credit officer making final loan decisions.\"),\n",
    "    (\"user\", \"\"\"Based on the following loan application data and verification results, \n",
    "provide a final credit decision.\n",
    "\n",
    "Applicant Data:\n",
    "{applicant_data}\n",
    "\n",
    "Verification Results:\n",
    "- Credit Bureau: {credit_bureau}\n",
    "- Employment: {employment}\n",
    "- Identity: {identity}\n",
    "\n",
    "Provide final decision in this format:\n",
    "\n",
    "DECISION: [APPROVE/DENY/MANUAL_REVIEW]\n",
    "\n",
    "CONFIDENCE: [HIGH/MEDIUM/LOW]\n",
    "\n",
    "KEY FACTORS:\n",
    "- [List 3-5 key factors influencing this decision]\n",
    "\n",
    "REASONING:\n",
    "[2-3 sentences explaining the decision]\n",
    "\n",
    "CONDITIONS (if applicable):\n",
    "[Any conditions for approval, or N/A]\"\"\")\n",
    "])\n",
    "\n",
    "synthesis_chain = synthesis_prompt | llm_gpt4 | StrOutputParser()\n",
    "\n",
    "# Build hybrid chain: Sequential → Parallel → Sequential\n",
    "hybrid_loan_processing_chain = (\n",
    "    # Step 1: Extract applicant data (sequential)\n",
    "    {\"application\": RunnablePassthrough()}\n",
    "    | extraction_prompt\n",
    "    | llm_gpt35\n",
    "    | JsonOutputParser()\n",
    "    | RunnableLambda(lambda x: {\"extracted_data\": x})\n",
    "    # Step 2: Parallel verifications\n",
    "    | RunnablePassthrough.assign(\n",
    "        verifications=RunnableLambda(\n",
    "            lambda x: parallel_verification_chain.invoke({\n",
    "                \"applicant_data\": json.dumps(x[\"extracted_data\"])\n",
    "            })\n",
    "        )\n",
    "    )\n",
    "    # Step 3: Synthesize final decision (sequential)\n",
    "    | RunnablePassthrough.assign(\n",
    "        final_decision=lambda x: synthesis_chain.invoke({\n",
    "            \"applicant_data\": json.dumps(x[\"extracted_data\"], indent=2),\n",
    "            \"credit_bureau\": json.dumps(x[\"verifications\"][\"credit_bureau\"], indent=2),\n",
    "            \"employment\": json.dumps(x[\"verifications\"][\"employment\"], indent=2),\n",
    "            \"identity\": json.dumps(x[\"verifications\"][\"identity\"], indent=2)\n",
    "        })\n",
    "    )\n",
    ")\n",
    "\n",
    "# Execute hybrid workflow\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYBRID WORKFLOW: Sequential + Parallel + Sequential\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "step_times = []\n",
    "\n",
    "print(\"\\n[Step 1/3] Extracting applicant data (sequential)...\")\n",
    "step1_start = time.time()\n",
    "\n",
    "overall_start = time.time()\n",
    "hybrid_result = hybrid_loan_processing_chain.invoke(SAMPLE_LOAN_APPLICATION)\n",
    "total_time = time.time() - overall_start\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYBRID WORKFLOW COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total Wall Clock Time: {total_time:.2f}s\")\n",
    "print(f\"\\nWorkflow Pattern: Sequential → Parallel (3 tasks) → Sequential\")\n",
    "print(f\"Advantage: Parallel processing where possible, maintaining logical flow\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display final decision\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL CREDIT DECISION\")\n",
    "print(\"=\"*80)\n",
    "print(hybrid_result[\"final_decision\"])\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0469a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 3: CONDITIONAL WORKFLOWS WITH RUNNABLEBRANCH (Lab 3.3)\n",
    "\n",
    "**Duration:** 30 minutes  \n",
    "**Objective:** Build dynamic workflows that route based on data using RunnableBranch\n",
    "\n",
    "### Theory: RunnableBranch Pattern\n",
    "\n",
    "**Concept:** Route inputs to different chains based on conditions\n",
    "\n",
    "```python\n",
    "branch = RunnableBranch(\n",
    "    (condition_1, chain_1),\n",
    "    (condition_2, chain_2),\n",
    "    default_chain  # fallback\n",
    ")\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- Different inputs need different handling\n",
    "- Want to optimize processing\n",
    "- Clear decision criteria exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefba29d",
   "metadata": {},
   "source": [
    "### Challenge 3.1: Risk-Based Routing with RunnableBranch (20 minutes)\n",
    "\n",
    "**Scenario:** Route loans to different approval workflows based on amount:\n",
    "- **Small loans** (<$100K): Fast-track (2 steps)\n",
    "- **Medium loans** ($100K-$500K): Standard (4 steps)\n",
    "- **Large loans** (>$500K): Enhanced due diligence (6 steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, extract loan amount for routing decision\n",
    "loan_amount_extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You extract loan amounts.\"),\n",
    "    (\"user\", \"\"\"Extract only the loan amount from this application.\n",
    "Return as JSON: {{\"loan_amount\": <number>}}\n",
    "\n",
    "Application:\n",
    "{application}\"\"\")\n",
    "])\n",
    "\n",
    "loan_amount_chain = loan_amount_extraction_prompt | llm_gpt35 | JsonOutputParser()\n",
    "\n",
    "# Define threshold constants\n",
    "SMALL_LOAN_THRESHOLD = 100000\n",
    "LARGE_LOAN_THRESHOLD = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small loan fast-track chain\n",
    "small_loan_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a fast-track credit assessor for small loans.\"),\n",
    "    (\"user\", \"\"\"Perform quick credit assessment for small loan:\n",
    "\n",
    "Application:\n",
    "{application}\n",
    "\n",
    "Provide quick decision based on:\n",
    "- Credit score (must be >650)\n",
    "- Income stability\n",
    "- Basic debt check\n",
    "\n",
    "Return as text with:\n",
    "DECISION: [APPROVE/DENY/NEEDS_REVIEW]\n",
    "REASON: [brief explanation]\n",
    "\n",
    "Keep it concise - this is fast-track processing.\"\"\")\n",
    "])\n",
    "\n",
    "small_loan_chain = small_loan_prompt | llm_gpt35 | StrOutputParser()\n",
    "\n",
    "# Medium loan standard chain (reuse our previous chains)\n",
    "medium_loan_chain = (\n",
    "    {\"application\": RunnablePassthrough()}\n",
    "    | extraction_prompt\n",
    "    | llm_gpt35\n",
    "    | JsonOutputParser()\n",
    "    | RunnableLambda(lambda x: {\"extracted_data\": x})\n",
    "    | RunnablePassthrough.assign(\n",
    "        ratios=lambda x: ratio_chain.invoke({\"extracted_data\": json.dumps(x[\"extracted_data\"])})\n",
    "    )\n",
    "    | RunnablePassthrough.assign(\n",
    "        summary=lambda x: summary_chain.invoke({\n",
    "            \"extracted_data\": json.dumps(x[\"extracted_data\"]),\n",
    "            \"ratios\": json.dumps(x[\"ratios\"])\n",
    "        })\n",
    "    )\n",
    "    | RunnableLambda(lambda x: x[\"summary\"])\n",
    ")\n",
    "\n",
    "# Large loan enhanced due diligence chain\n",
    "large_loan_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a senior financial analyst for large commercial loans.\"),\n",
    "    (\"user\", \"\"\"Perform comprehensive enhanced due diligence for large loan (>$500,000):\n",
    "\n",
    "Application:\n",
    "{application}\n",
    "\n",
    "Provide comprehensive analysis including:\n",
    "1. Complete financial analysis (DTI, LTV, DSCR, cash flow)\n",
    "2. Collateral assessment\n",
    "3. Market risk factors\n",
    "4. Stress testing scenarios (best/base/stress cases)\n",
    "5. Default probability estimate\n",
    "6. Senior review requirements\n",
    "7. Credit committee decision framework\n",
    "\n",
    "This requires SENIOR REVIEW + COMMITTEE APPROVAL.\n",
    "\n",
    "Be thorough - this is a large loan requiring enhanced due diligence.\"\"\")\n",
    "])\n",
    "\n",
    "large_loan_chain = large_loan_prompt | llm_gpt4 | StrOutputParser()\n",
    "\n",
    "print(\"✓ All workflow-specific chains created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7913b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create routing conditions\n",
    "def is_small_loan(data: Dict) -> bool:\n",
    "    \"\"\"Check if loan amount is less than $100K\"\"\"\n",
    "    loan_amount = data.get(\"loan_info\", {}).get(\"loan_amount\", 0)\n",
    "    return loan_amount < SMALL_LOAN_THRESHOLD\n",
    "\n",
    "def is_large_loan(data: Dict) -> bool:\n",
    "    \"\"\"Check if loan amount is greater than $500K\"\"\"\n",
    "    loan_amount = data.get(\"loan_info\", {}).get(\"loan_amount\", 0)\n",
    "    return loan_amount >= LARGE_LOAN_THRESHOLD\n",
    "\n",
    "# Extract application for routing\n",
    "def extract_application(data: Dict) -> Dict:\n",
    "    \"\"\"Extract the application text for downstream chains\"\"\"\n",
    "    return {\"application\": data.get(\"application\", \"\")}\n",
    "\n",
    "# Create the routing branch\n",
    "loan_routing_branch = RunnableBranch(\n",
    "    (\n",
    "        is_small_loan,\n",
    "        RunnableLambda(extract_application) | small_loan_chain\n",
    "    ),\n",
    "    (\n",
    "        is_large_loan,\n",
    "        RunnableLambda(extract_application) | large_loan_chain\n",
    "    ),\n",
    "    # Default: medium loan\n",
    "    RunnableLambda(extract_application) | medium_loan_chain\n",
    ")\n",
    "\n",
    "# Complete conditional routing chain\n",
    "conditional_routing_chain = (\n",
    "    {\"application\": RunnablePassthrough()}\n",
    "    | RunnablePassthrough.assign(\n",
    "        loan_info=lambda x: loan_amount_chain.invoke({\"application\": x[\"application\"]})\n",
    "    )\n",
    "    | RunnablePassthrough.assign(\n",
    "        result=loan_routing_branch\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"✓ Conditional routing chain with RunnableBranch created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7795d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the conditional router with different loan sizes\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING CONDITIONAL ROUTING WITH RUNNABLEBRANCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_cases = [\n",
    "    (\"Small Loan\", SMALL_LOAN_APP),\n",
    "    (\"Medium Loan\", SAMPLE_LOAN_APPLICATION),\n",
    "    (\"Large Loan\", LARGE_LOAN_APP)\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for test_name, application in test_cases:\n",
    "    print(f\"\\n### TEST: {test_name.upper()} ###\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    result = conditional_routing_chain.invoke(application)\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    loan_amount = result[\"loan_info\"][\"loan_amount\"]\n",
    "    \n",
    "    # Determine category\n",
    "    if loan_amount < SMALL_LOAN_THRESHOLD:\n",
    "        category = \"SMALL (Fast-Track)\"\n",
    "    elif loan_amount < LARGE_LOAN_THRESHOLD:\n",
    "        category = \"MEDIUM (Standard)\"\n",
    "    else:\n",
    "        category = \"LARGE (Enhanced DD)\"\n",
    "    \n",
    "    print(f\"Loan Amount: ${loan_amount:,}\")\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Processing Time: {processing_time:.2f}s\")\n",
    "    print(f\"\\nResult Preview:\")\n",
    "    print(str(result[\"result\"])[:300] + \"...\")\n",
    "    \n",
    "    results.append({\n",
    "        \"name\": test_name,\n",
    "        \"amount\": loan_amount,\n",
    "        \"category\": category,\n",
    "        \"time\": processing_time\n",
    "    })\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ROUTING EFFICIENCY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "for r in results:\n",
    "    print(f\"{r['name']:12} | ${r['amount']:>12,} | {r['category']:20} | {r['time']:.2f}s\")\n",
    "print(\"\\nConditional routing optimizes processing based on loan size and risk!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b28e5c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 4: PRODUCTION ORCHESTRATION WITH LANGCHAIN (Lab 3.4)\n",
    "\n",
    "**Duration:** 30 minutes  \n",
    "**Objective:** Add production-grade features using LangChain's built-in capabilities\n",
    "\n",
    "### Production Features in LangChain:\n",
    "\n",
    "- ✓ **Automatic Retries** - Built into ChatOpenAI with `max_retries`\n",
    "- ✓ **Error Handling** - Try/catch at chain level\n",
    "- ✓ **Callbacks** - Comprehensive logging and monitoring\n",
    "- ✓ **Streaming** - Real-time output\n",
    "- ✓ **Fallbacks** - Use `with_fallbacks()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdfa1b",
   "metadata": {},
   "source": [
    "### Challenge 4.1: Production-Ready Chain with Error Handling (15 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing import Iterator\n",
    "\n",
    "class ProductionCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"Production-grade callback handler with comprehensive logging\"\"\"\n",
    "    \n",
    "    def __init__(self, workflow_name: str):\n",
    "        self.workflow_name = workflow_name\n",
    "        self.logs = []\n",
    "        self.errors = []\n",
    "        self.total_tokens = 0\n",
    "        self.step_count = 0\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def log(self, message: str, level: str = \"INFO\"):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "        log_entry = f\"[{timestamp}] [{level}] {message}\"\n",
    "        self.logs.append(log_entry)\n",
    "        print(log_entry)\n",
    "    \n",
    "    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs) -> None:\n",
    "        self.log(f\"Chain started: {self.workflow_name}\", \"INFO\")\n",
    "    \n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:\n",
    "        duration = time.time() - self.start_time\n",
    "        self.log(f\"Chain completed in {duration:.2f}s\", \"SUCCESS\")\n",
    "    \n",
    "    def on_chain_error(self, error: Exception, **kwargs) -> None:\n",
    "        self.errors.append(str(error))\n",
    "        self.log(f\"Chain error: {str(error)}\", \"ERROR\")\n",
    "    \n",
    "    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs) -> None:\n",
    "        self.step_count += 1\n",
    "        self.log(f\"LLM call {self.step_count} started\", \"INFO\")\n",
    "    \n",
    "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
    "        if response.llm_output and \"token_usage\" in response.llm_output:\n",
    "            tokens = response.llm_output[\"token_usage\"].get(\"total_tokens\", 0)\n",
    "            self.total_tokens += tokens\n",
    "            self.log(f\"LLM call {self.step_count} completed ({tokens} tokens)\", \"SUCCESS\")\n",
    "    \n",
    "    def on_llm_error(self, error: Exception, **kwargs) -> None:\n",
    "        self.errors.append(f\"LLM error at step {self.step_count}: {str(error)}\")\n",
    "        self.log(f\"LLM error: {str(error)}\", \"ERROR\")\n",
    "    \n",
    "    def on_retry(self, retry_state, **kwargs) -> None:\n",
    "        self.log(f\"Retrying after error...\", \"WARNING\")\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"workflow_name\": self.workflow_name,\n",
    "            \"total_duration\": time.time() - self.start_time,\n",
    "            \"total_steps\": self.step_count,\n",
    "            \"total_tokens\": self.total_tokens,\n",
    "            \"error_count\": len(self.errors),\n",
    "            \"logs\": self.logs,\n",
    "            \"errors\": self.errors\n",
    "        }\n",
    "\n",
    "print(\"✓ Production callback handler created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a production chain with retry and fallback\n",
    "# Primary model with retries\n",
    "primary_llm = ChatOpenAI(\n",
    "    model=GPT4_MODEL,\n",
    "    temperature=0,\n",
    "    max_retries=3,\n",
    "    request_timeout=30\n",
    ")\n",
    "\n",
    "# Fallback to faster model if primary fails\n",
    "fallback_llm = ChatOpenAI(\n",
    "    model=GPT35_MODEL,\n",
    "    temperature=0,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "# Create chain with fallback\n",
    "production_llm = primary_llm.with_fallbacks([fallback_llm])\n",
    "\n",
    "# Production extraction chain\n",
    "production_extraction_chain = extraction_prompt | production_llm | JsonOutputParser()\n",
    "\n",
    "# Production complete workflow\n",
    "production_workflow_chain = (\n",
    "    {\"application\": RunnablePassthrough()}\n",
    "    | extraction_prompt\n",
    "    | production_llm\n",
    "    | JsonOutputParser()\n",
    "    | RunnableLambda(lambda x: {\"extraction\": x})\n",
    "    | RunnablePassthrough.assign(\n",
    "        ratios=lambda x: (ratio_prompt | production_llm | JsonOutputParser()).invoke({\n",
    "            \"extracted_data\": json.dumps(x[\"extraction\"])\n",
    "        })\n",
    "    )\n",
    "    | RunnablePassthrough.assign(\n",
    "        summary=lambda x: (summary_prompt | production_llm | StrOutputParser()).invoke({\n",
    "            \"extracted_data\": json.dumps(x[\"extraction\"]),\n",
    "            \"ratios\": json.dumps(x[\"ratios\"])\n",
    "        })\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"✓ Production workflow with retry and fallback created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b46b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute production workflow with monitoring\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRODUCTION WORKFLOW EXECUTION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "callback_handler = ProductionCallbackHandler(\"Loan Processing Pipeline\")\n",
    "\n",
    "try:\n",
    "    result = production_workflow_chain.invoke(\n",
    "        SAMPLE_LOAN_APPLICATION,\n",
    "        config={\"callbacks\": [callback_handler]}\n",
    "    )\n",
    "    \n",
    "    # Display metrics\n",
    "    metrics = callback_handler.get_metrics()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PRODUCTION METRICS DASHBOARD\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Workflow: {metrics['workflow_name']}\")\n",
    "    print(f\"Status: {'✓ SUCCESS' if metrics['error_count'] == 0 else '✗ FAILED'}\")\n",
    "    print(f\"Duration: {metrics['total_duration']:.2f}s\")\n",
    "    print(f\"Total Steps: {metrics['total_steps']}\")\n",
    "    print(f\"Total Tokens: {metrics['total_tokens']}\")\n",
    "    print(f\"Errors: {metrics['error_count']}\")\n",
    "    print(\"\\nExecution Log:\")\n",
    "    for log in metrics['logs'][-10:]:  # Show last 10 log entries\n",
    "        print(f\"  {log}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if metrics['error_count'] == 0:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"FINAL SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(result[\"summary\"])\n",
    "        print(\"=\"*80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Workflow failed: {str(e)}\")\n",
    "    metrics = callback_handler.get_metrics()\n",
    "    print(f\"\\nErrors encountered:\")\n",
    "    for error in metrics['errors']:\n",
    "        print(f\"  - {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc6d76",
   "metadata": {},
   "source": [
    "### Challenge 4.2: Streaming Support (15 minutes)\n",
    "\n",
    "**Objective:** Add real-time streaming to see results as they're generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693aca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming chain for the final summary\n",
    "streaming_summary_chain = summary_prompt | llm_gpt4 | StrOutputParser()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STREAMING EXECUTION DEMO\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerating executive summary with real-time streaming...\\n\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# First, get the required data\n",
    "extraction_data = extraction_chain.invoke({\"application\": SAMPLE_LOAN_APPLICATION})\n",
    "ratio_data = ratio_chain.invoke({\"extracted_data\": json.dumps(extraction_data)})\n",
    "\n",
    "# Stream the summary generation\n",
    "for chunk in streaming_summary_chain.stream({\n",
    "    \"extracted_data\": json.dumps(extraction_data),\n",
    "    \"ratios\": json.dumps(ratio_data)\n",
    "}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\n✓ Streaming complete!\")\n",
    "print(\"\\nNote: In a production web app, each chunk would be sent to the client immediately.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404575ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## CAPSTONE: ENTERPRISE LOAN PROCESSOR WITH LANGCHAIN\n",
    "\n",
    "**Objective:** Complete production system combining all LangChain patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4121bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnterpriseLoanProcessor:\n",
    "    \"\"\"\n",
    "    Complete production loan processing system using LangChain:\n",
    "    - Conditional routing (RunnableBranch)\n",
    "    - Parallel processing (RunnableParallel)\n",
    "    - Sequential workflows (LCEL)\n",
    "    - Error handling (retry + fallback)\n",
    "    - Monitoring (callbacks)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize the conditional routing chain we built earlier\n",
    "        self.routing_chain = conditional_routing_chain\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'total_processed': 0,\n",
    "            'successful': 0,\n",
    "            'failed': 0,\n",
    "            'total_time': 0.0,\n",
    "            'total_tokens': 0\n",
    "        }\n",
    "    \n",
    "    def process_application(self, application: str, app_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process single loan application through complete LangChain system\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ENTERPRISE PROCESSOR - Application {app_id}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        callback = ProductionCallbackHandler(f\"Application {app_id}\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            result = self.routing_chain.invoke(\n",
    "                application,\n",
    "                config={\"callbacks\": [callback]}\n",
    "            )\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            metrics = callback.get_metrics()\n",
    "            \n",
    "            # Update statistics\n",
    "            self.stats['total_processed'] += 1\n",
    "            self.stats['successful'] += 1\n",
    "            self.stats['total_time'] += processing_time\n",
    "            self.stats['total_tokens'] += metrics['total_tokens']\n",
    "            \n",
    "            loan_amount = result['loan_info']['loan_amount']\n",
    "            if loan_amount < SMALL_LOAN_THRESHOLD:\n",
    "                workflow = \"Fast-Track\"\n",
    "            elif loan_amount < LARGE_LOAN_THRESHOLD:\n",
    "                workflow = \"Standard\"\n",
    "            else:\n",
    "                workflow = \"Enhanced DD\"\n",
    "            \n",
    "            print(f\"\\n✓ Processing complete\")\n",
    "            print(f\"  Loan Amount: ${loan_amount:,}\")\n",
    "            print(f\"  Workflow: {workflow}\")\n",
    "            print(f\"  Time: {processing_time:.2f}s\")\n",
    "            print(f\"  Tokens: {metrics['total_tokens']}\")\n",
    "            \n",
    "            return {\n",
    "                'app_id': app_id,\n",
    "                'success': True,\n",
    "                'loan_amount': loan_amount,\n",
    "                'workflow': workflow,\n",
    "                'processing_time': processing_time,\n",
    "                'result': result['result'],\n",
    "                'metrics': metrics\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.stats['total_processed'] += 1\n",
    "            self.stats['failed'] += 1\n",
    "            \n",
    "            print(f\"\\n✗ Processing failed: {str(e)}\")\n",
    "            \n",
    "            return {\n",
    "                'app_id': app_id,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return processing statistics\"\"\"\n",
    "        avg_time = (\n",
    "            self.stats['total_time'] / self.stats['total_processed']\n",
    "            if self.stats['total_processed'] > 0 else 0\n",
    "        )\n",
    "        \n",
    "        success_rate = (\n",
    "            self.stats['successful'] / self.stats['total_processed'] * 100\n",
    "            if self.stats['total_processed'] > 0 else 0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'total_processed': self.stats['total_processed'],\n",
    "            'successful': self.stats['successful'],\n",
    "            'failed': self.stats['failed'],\n",
    "            'success_rate': success_rate,\n",
    "            'average_processing_time': avg_time,\n",
    "            'total_tokens': self.stats['total_tokens']\n",
    "        }\n",
    "\n",
    "print(\"✓ Enterprise Loan Processor class created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfd9589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and test enterprise processor\n",
    "processor = EnterpriseLoanProcessor()\n",
    "\n",
    "# Process multiple applications\n",
    "applications = [\n",
    "    (SMALL_LOAN_APP, \"LA-2024-001\"),\n",
    "    (SAMPLE_LOAN_APPLICATION, \"LA-2024-002\"),\n",
    "    (LARGE_LOAN_APP, \"LA-2024-003\")\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENTERPRISE BATCH PROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for app, app_id in applications:\n",
    "    result = processor.process_application(app, app_id)\n",
    "    results.append(result)\n",
    "    time.sleep(0.5)  # Brief pause between applications\n",
    "\n",
    "# Display final statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENTERPRISE PROCESSING STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "stats = processor.get_statistics()\n",
    "print(f\"Total Processed: {stats['total_processed']}\")\n",
    "print(f\"Successful: {stats['successful']}\")\n",
    "print(f\"Failed: {stats['failed']}\")\n",
    "print(f\"Success Rate: {stats['success_rate']:.1f}%\")\n",
    "print(f\"Average Processing Time: {stats['average_processing_time']:.2f}s\")\n",
    "print(f\"Total Tokens Used: {stats['total_tokens']}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
