{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f5a7e01",
   "metadata": {},
   "source": [
    "# LAB 4: RAG SYSTEMS - COMPLETE IMPLEMENTATION GUIDE\n",
    "\n",
    "**Course:** Advanced Prompt Engineering Training  \n",
    "**Session:** Session 3 - RAG & Advanced Retrieval (Day 2)  \n",
    "**Duration:** 150 minutes (2.5 hours)  \n",
    "**Type:** Comprehensive RAG Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c672f",
   "metadata": {},
   "source": [
    "## LAB OVERVIEW\n",
    "\n",
    "This comprehensive lab teaches you to build **production-grade RAG (Retrieval-Augmented Generation) systems**. You'll progress through five interconnected modules:\n",
    "\n",
    "1. **Document Chunking** - Optimal text segmentation strategies\n",
    "2. **Embeddings & Search** - Vector representations and similarity\n",
    "3. **Vector Database** - Indexing and retrieval at scale\n",
    "4. **Complete RAG Pipeline** - End-to-end question answering\n",
    "5. **Advanced Patterns** - Hybrid search, re-ranking, query expansion\n",
    "\n",
    "**Scenario:** You're building an AI-powered knowledge base for a bank's internal documentation - policies, procedures, compliance guidelines, and product information. Starting with raw documents, you'll build a complete RAG system that employees can query in natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc50be",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 4: RAG Systems Implementation\n",
    "# Advanced Prompt Engineering Training - Session 3\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Vector database\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db412bbd",
   "metadata": {},
   "source": [
    "### Step 2: Configure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10e0e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Model configurations\n",
    "GPT4 = os.environ.get(\"MODEL_NAME\", \"gpt-4o\")\n",
    "GPT35 = os.environ.get(\"FAST_MODEL_NAME\", \"gpt-3.5-turbo\")\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "EMBEDDING_DIMENSIONS = 1536\n",
    "\n",
    "print(f\"✓ OpenAI client configured\")\n",
    "print(f\"✓ LLM models: {GPT4}, {GPT35}\")\n",
    "print(f\"✓ Embedding model: {EMBEDDING_MODEL} ({EMBEDDING_DIMENSIONS}d)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51cc34d",
   "metadata": {},
   "source": [
    "### Step 3: Core Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Document with metadata\"\"\"\n",
    "    content: str\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    doc_id: Optional[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.doc_id is None:\n",
    "            self.doc_id = f\"doc_{hash(self.content) % 10**8}\"\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Text chunk with metadata and embedding\"\"\"\n",
    "    content: str\n",
    "    chunk_id: str\n",
    "    doc_id: str\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    embedding: Optional[List[float]] = None\n",
    "    start_index: int = 0\n",
    "    end_index: int = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.content)\n",
    "\n",
    "def count_tokens(text: str, model: str = \"gpt-4\") -> int:\n",
    "    \"\"\"Count tokens in text\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def get_embedding(text: str, model: str = EMBEDDING_MODEL) -> List[float]:\n",
    "    \"\"\"Get embedding vector for text\"\"\"\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return [0.0] * EMBEDDING_DIMENSIONS\n",
    "\n",
    "def call_llm(\n",
    "    prompt: str,\n",
    "    system_prompt: str = \"You are a helpful AI assistant.\",\n",
    "    model: str = GPT4,\n",
    "    temperature: float = 0\n",
    ") -> str:\n",
    "    \"\"\"Call LLM and return response\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"✓ Helper functions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d011990",
   "metadata": {},
   "source": [
    "### Step 4: Load Sample Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b753f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample bank knowledge base documents\n",
    "BANK_KNOWLEDGE_BASE = {\n",
    "    \"hr_policies\": \"\"\"\n",
    "HUMAN RESOURCES POLICIES - CONSOLIDATED HANDBOOK\n",
    "\n",
    "Section 1: Paid Time Off (PTO) Policy\n",
    "\n",
    "Employees are entitled to paid time off for rest and personal needs. Our PTO policy is designed to promote work-life balance while ensuring business continuity.\n",
    "\n",
    "PTO Accrual:\n",
    "- Full-time employees accrue 20 days (160 hours) of PTO annually\n",
    "- PTO accrual begins after successful completion of 90-day probationary period\n",
    "- PTO accrues at a rate of 1.67 days per month\n",
    "- Part-time employees accrue PTO on a pro-rated basis\n",
    "- Maximum PTO bank: 30 days (240 hours); excess is forfeited\n",
    "\n",
    "Usage Guidelines:\n",
    "- PTO requests must be submitted at least 2 weeks in advance for periods longer than 3 days\n",
    "- Manager approval required for all PTO requests\n",
    "- Holiday periods (Thanksgiving, Christmas, New Year) require 30-day advance notice\n",
    "- Unused PTO does not roll over to the following year\n",
    "- PTO cannot be cashed out except upon termination\n",
    "\n",
    "Blackout Periods:\n",
    "- First week of each quarter (financial close)\n",
    "- Annual audit period (March 1-31)\n",
    "- Department-specific blackout periods may apply\n",
    "\n",
    "Section 2: Health Insurance Coverage\n",
    "\n",
    "We offer comprehensive health insurance plans to eligible employees and their dependents.\n",
    "\n",
    "Eligibility:\n",
    "- Full-time employees (30+ hours/week) are eligible\n",
    "- Coverage begins on the first day of the month following 30 days of employment\n",
    "- Eligible dependents include spouse and children under 26\n",
    "\n",
    "Plan Options:\n",
    "- PPO Plan: $250 deductible, 80/20 coinsurance, $3,000 out-of-pocket max\n",
    "- HMO Plan: $100 deductible, 90/10 coinsurance, $2,000 out-of-pocket max\n",
    "- HSA-eligible HDHP: $1,500 deductible, 100% after deductible, $4,000 max\n",
    "\n",
    "Employee Contributions (per month):\n",
    "- Employee only: $150 (PPO), $100 (HMO), $75 (HDHP)\n",
    "- Employee + Spouse: $350 (PPO), $280 (HMO), $220 (HDHP)\n",
    "- Employee + Family: $500 (PPO), $420 (HMO), $340 (HDHP)\n",
    "\n",
    "Coverage Includes:\n",
    "- Preventive care at 100% (no cost sharing)\n",
    "- Emergency room visits (subject to deductible)\n",
    "- Prescription drugs (3-tier copay structure)\n",
    "- Mental health services (same as medical)\n",
    "- Dental and vision available as separate add-ons\n",
    "\n",
    "Section 3: Remote Work Policy\n",
    "\n",
    "Our hybrid work model allows flexibility while maintaining collaboration and culture.\n",
    "\n",
    "Eligibility:\n",
    "- Employees must complete 6 months of employment before becoming eligible\n",
    "- Manager approval required\n",
    "- Position must be suitable for remote work\n",
    "- Performance must meet or exceed expectations\n",
    "\n",
    "Requirements:\n",
    "- Employees must work from primary residence within the United States\n",
    "- Dedicated workspace with reliable internet (minimum 25 Mbps)\n",
    "- Available during core hours (10 AM - 3 PM local time)\n",
    "- Attend all required in-person meetings and events\n",
    "\n",
    "Schedule Options:\n",
    "- Hybrid: 2-3 days in office per week (Monday and Thursday required)\n",
    "- Fully Remote: Available for specific roles only\n",
    "- Flexible: Varies by department and business need\n",
    "\n",
    "Equipment:\n",
    "- Company provides laptop, monitor, keyboard, mouse\n",
    "- $500 home office stipend for furniture/accessories\n",
    "- IT support available during business hours\n",
    "- Security requirements must be met (VPN, encrypted drives)\n",
    "\n",
    "Performance Expectations:\n",
    "- Same productivity and quality standards as in-office work\n",
    "- Regular check-ins with manager (minimum weekly)\n",
    "- Response time: within 2 hours during core hours\n",
    "- Remote work privilege may be revoked for performance issues\n",
    "\"\"\",\n",
    "    \n",
    "    \"loan_products\": \"\"\"\n",
    "LOAN PRODUCTS GUIDE - CONSUMER LENDING\n",
    "\n",
    "Product 1: Personal Loans\n",
    "\n",
    "Personal loans provide flexible funding for debt consolidation, home improvements, major purchases, or other personal needs.\n",
    "\n",
    "Loan Amounts: $5,000 to $50,000\n",
    "Terms: 12, 24, 36, 48, or 60 months\n",
    "Interest Rates: 7.99% - 18.99% APR (based on creditworthiness)\n",
    "\n",
    "Eligibility Requirements:\n",
    "- Minimum credit score: 650\n",
    "- Minimum annual income: $35,000\n",
    "- Debt-to-income ratio: Maximum 43%\n",
    "- Employment: Minimum 2 years current job or 3 years in same field\n",
    "- U.S. citizen or permanent resident\n",
    "\n",
    "Application Process:\n",
    "- Online application (15 minutes)\n",
    "- Instant pre-qualification decision\n",
    "- Document submission: 2 pay stubs, 2 bank statements, tax returns\n",
    "- Final approval: 1-3 business days\n",
    "- Funding: 1-2 business days after approval\n",
    "\n",
    "Fees:\n",
    "- Origination fee: 1-5% of loan amount\n",
    "- Late payment fee: $35 or 5% of payment (whichever is greater)\n",
    "- Returned payment fee: $35\n",
    "- No prepayment penalty\n",
    "\n",
    "Product 2: Home Mortgage Loans\n",
    "\n",
    "We offer competitive mortgage products for home purchase and refinancing.\n",
    "\n",
    "Conventional Mortgages:\n",
    "- Loan amounts up to $726,200 (conforming limits)\n",
    "- Down payment: Minimum 5% (20% to avoid PMI)\n",
    "- Terms: 15, 20, or 30 years\n",
    "- Interest rates: 6.50% - 8.25% (based on credit, LTV, term)\n",
    "\n",
    "FHA Loans:\n",
    "- Down payment: Minimum 3.5%\n",
    "- Credit score: Minimum 580\n",
    "- Mortgage insurance required\n",
    "- Loan limits vary by county\n",
    "\n",
    "VA Loans (for eligible veterans):\n",
    "- No down payment required\n",
    "- No mortgage insurance\n",
    "- Competitive interest rates\n",
    "- Funding fee: 2.3% (can be financed)\n",
    "\n",
    "Jumbo Mortgages (over $726,200):\n",
    "- Down payment: Minimum 20%\n",
    "- Credit score: Minimum 700\n",
    "- Cash reserves: 6-12 months PITI\n",
    "- Rates typically 0.25-0.50% higher than conforming\n",
    "\n",
    "Qualification Guidelines:\n",
    "- Credit score: Minimum 620 (conventional), 580 (FHA)\n",
    "- DTI ratio: Maximum 43% (some exceptions to 50%)\n",
    "- Employment verification: 2 years history\n",
    "- Down payment must be from acceptable sources\n",
    "- Property must appraise at or above purchase price\n",
    "\n",
    "Closing Costs:\n",
    "- Typically 2-5% of loan amount\n",
    "- Includes: appraisal ($500-800), title insurance, origination fee (0.5-1%), inspections, recording fees\n",
    "- Seller may contribute up to 3-6% toward closing costs\n",
    "\n",
    "Product 3: Auto Loans\n",
    "\n",
    "Competitive financing for new and used vehicle purchases.\n",
    "\n",
    "New Vehicle Loans:\n",
    "- Loan amounts: $5,000 to $100,000\n",
    "- Terms: 24, 36, 48, 60, or 72 months\n",
    "- Rates: 4.99% - 9.99% APR\n",
    "- Maximum LTV: 125%\n",
    "\n",
    "Used Vehicle Loans:\n",
    "- Vehicle age: Up to 8 years old\n",
    "- Mileage: Maximum 100,000 miles\n",
    "- Terms: 24, 36, 48, or 60 months\n",
    "- Rates: 5.99% - 11.99% APR\n",
    "- Maximum LTV: 120%\n",
    "\n",
    "Requirements:\n",
    "- Credit score: Minimum 600\n",
    "- Down payment: Minimum 10% for used, 5% for new\n",
    "- Income verification required\n",
    "- Full coverage insurance required\n",
    "- Vehicle must meet bank's age and mileage criteria\n",
    "\n",
    "Special Programs:\n",
    "- Recent graduate program: 0.25% rate discount\n",
    "- Multi-car household: 0.15% discount\n",
    "- Auto-pay discount: 0.25% rate reduction\n",
    "- Trade-in assistance available\n",
    "\n",
    "Application Process:\n",
    "- Pre-approval available (does not affect credit score)\n",
    "- Online application (10 minutes)\n",
    "- Decision: Same day for most applicants\n",
    "- Funding: 24-48 hours after approval\n",
    "- Dealership direct financing available\n",
    "\"\"\",\n",
    "\n",
    "    \"compliance_guidelines\": \"\"\"\n",
    "COMPLIANCE AND REGULATORY GUIDELINES\n",
    "\n",
    "Anti-Money Laundering (AML) Requirements\n",
    "\n",
    "Our AML program ensures compliance with the Bank Secrecy Act and USA PATRIOT Act.\n",
    "\n",
    "Customer Due Diligence (CDD):\n",
    "- All new customers must be verified using government-issued ID\n",
    "- Collect: Full legal name, date of birth, physical address, SSN/TIN\n",
    "- Document type and ID number must be recorded\n",
    "- Verification must occur before account opening\n",
    "\n",
    "Enhanced Due Diligence (EDD) Required For:\n",
    "- Cash-intensive businesses\n",
    "- Non-resident aliens\n",
    "- Politically exposed persons (PEPs)\n",
    "- High-risk jurisdictions\n",
    "- Customers with prior suspicious activity\n",
    "\n",
    "Transaction Monitoring:\n",
    "- All transactions over $10,000 must be reported (CTR)\n",
    "- Structured transactions must be identified and reported (SAR)\n",
    "- Wire transfers require additional screening\n",
    "- Daily monitoring of all accounts for unusual patterns\n",
    "\n",
    "Suspicious Activity Reporting (SAR):\n",
    "- File within 30 days of detecting suspicious activity\n",
    "- Threshold: $5,000 for most violations\n",
    "- Threshold: $25,000 for securities violations\n",
    "- Customer must not be notified of SAR filing\n",
    "- Maintain SAR confidentiality\n",
    "\n",
    "Red Flags to Report:\n",
    "- Customer reluctant to provide information\n",
    "- Multiple accounts used to avoid reporting thresholds\n",
    "- Large cash deposits inconsistent with business\n",
    "- Sudden increase in transaction volume\n",
    "- Transactions with high-risk countries\n",
    "- Frequent wire transfers with no clear purpose\n",
    "\n",
    "Know Your Customer (KYC) Program\n",
    "\n",
    "Ongoing customer monitoring to assess risk and detect changes.\n",
    "\n",
    "Risk Categories:\n",
    "- Low Risk: Employed individuals, small balances, low transaction volume\n",
    "- Medium Risk: Self-employed, moderate balances, regular transactions\n",
    "- High Risk: Cash businesses, high balances, frequent international activity\n",
    "\n",
    "Review Frequency:\n",
    "- Low risk: Annual review\n",
    "- Medium risk: Semi-annual review\n",
    "- High risk: Quarterly review or continuous monitoring\n",
    "\n",
    "Information to Update:\n",
    "- Current address and contact information\n",
    "- Employment status and income sources\n",
    "- Business activities and ownership\n",
    "- Expected account activity and transaction patterns\n",
    "- Source of funds for large deposits\n",
    "\n",
    "OFAC Screening Requirements\n",
    "\n",
    "Screen against Office of Foreign Assets Control lists.\n",
    "\n",
    "When to Screen:\n",
    "- All new customer accounts\n",
    "- All wire transfers (domestic and international)\n",
    "- Large cash transactions\n",
    "- When customer information changes\n",
    "- Daily batch screening of all customers\n",
    "\n",
    "Blocked Person Lists:\n",
    "- Specially Designated Nationals (SDN) list\n",
    "- Sectoral Sanctions Identifications List\n",
    "- Foreign Sanctions Evaders List\n",
    "- Country-based sanctions programs\n",
    "\n",
    "Screening Procedures:\n",
    "- Use automated screening software\n",
    "- Check names, addresses, ID numbers\n",
    "- Screen beneficial owners and related parties\n",
    "- Document all screening results\n",
    "- Escalate potential matches immediately\n",
    "\n",
    "Response to Matches:\n",
    "- Block transaction immediately\n",
    "- Do not notify customer\n",
    "- Contact Compliance Department within 1 hour\n",
    "- File report with OFAC within 10 days\n",
    "- Freeze account if required\n",
    "\n",
    "Privacy and Data Protection\n",
    "\n",
    "Safeguarding customer information is paramount.\n",
    "\n",
    "Gramm-Leach-Bliley Act (GLBA) Compliance:\n",
    "- Provide privacy notice at account opening\n",
    "- Annual privacy notice required\n",
    "- Opt-out must be offered for information sharing\n",
    "- Notice of adverse action when required\n",
    "\n",
    "Information Security:\n",
    "- Encrypt all sensitive data at rest and in transit\n",
    "- Access controls: minimum necessary principle\n",
    "- Password requirements: 12+ characters, MFA enabled\n",
    "- Log and monitor all access to customer data\n",
    "- Report security incidents within 24 hours\n",
    "\n",
    "Permissible Data Sharing:\n",
    "- With customer consent\n",
    "- To service providers (with contract protections)\n",
    "- To comply with legal requirements\n",
    "- To prevent fraud or unauthorized transactions\n",
    "- To report to credit bureaus\n",
    "\n",
    "Prohibited Activities:\n",
    "- Sharing account numbers for marketing\n",
    "- Selling customer information to third parties\n",
    "- Accessing accounts without business need\n",
    "- Discussing customer information in public areas\n",
    "- Removing customer data from secure systems\n",
    "\n",
    "Fair Lending Requirements\n",
    "\n",
    "We are committed to fair and equal treatment of all customers.\n",
    "\n",
    "Equal Credit Opportunity Act (ECOA):\n",
    "- Cannot discriminate based on protected classes\n",
    "- Protected classes: race, color, religion, national origin, sex, marital status, age (if of legal age), income from public assistance\n",
    "- Adverse action notice required within 30 days\n",
    "- Specific reasons for denial must be provided\n",
    "\n",
    "Fair Housing Act:\n",
    "- Applies to residential real estate loans\n",
    "- Cannot redline or steer applicants\n",
    "- Marketing must reach diverse communities\n",
    "- Underwriting must be consistent and objective\n",
    "\n",
    "Documentation Requirements:\n",
    "- All credit decisions must be documented\n",
    "- Exceptions to policy must be justified in writing\n",
    "- Maintain evidence of fair lending compliance\n",
    "- Monitor for disparate impact\n",
    "- Regular fair lending training required\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Create Document objects\n",
    "documents = []\n",
    "for doc_type, content in BANK_KNOWLEDGE_BASE.items():\n",
    "    doc = Document(\n",
    "        content=content,\n",
    "        metadata={\n",
    "            \"type\": doc_type,\n",
    "            \"source\": \"bank_knowledge_base\",\n",
    "            \"created_at\": datetime.now().isoformat()\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "print(f\"✓ Loaded {len(documents)} knowledge base documents\")\n",
    "print(f\"✓ Total characters: {sum(len(doc.content) for doc in documents):,}\")\n",
    "print(f\"✓ Total tokens (approx): {sum(count_tokens(doc.content) for doc in documents):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1320fc4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 1: DOCUMENT CHUNKING STRATEGIES (Lab 4.1)\n",
    "\n",
    "**Duration:** 30 minutes  \n",
    "**Objective:** Implement and compare different chunking strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f3e52",
   "metadata": {},
   "source": [
    "### Challenge 1.1: Fixed-Size Chunking (10 minutes)\n",
    "\n",
    "**Strategy:** Split text into chunks of fixed token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ba033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedSizeChunker:\n",
    "    \"\"\"Split text into fixed-size chunks with optional overlap\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "        self.encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    def chunk_document(self, document: Document) -> List[Chunk]:\n",
    "        \"\"\"Split document into fixed-size chunks\"\"\"\n",
    "        \n",
    "        # Tokenize entire document\n",
    "        tokens = self.encoding.encode(document.content)\n",
    "        total_tokens = len(tokens)\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        chunk_num = 0\n",
    "        \n",
    "        while start < total_tokens:\n",
    "            # Calculate end position\n",
    "            end = min(start + self.chunk_size, total_tokens)\n",
    "            \n",
    "            # Extract chunk tokens\n",
    "            chunk_tokens = tokens[start:end]\n",
    "            \n",
    "            # Decode back to text\n",
    "            chunk_text = self.encoding.decode(chunk_tokens)\n",
    "            \n",
    "            # Create Chunk object\n",
    "            chunk = Chunk(\n",
    "                content=chunk_text,\n",
    "                chunk_id=f\"{document.doc_id}_chunk_{chunk_num}\",\n",
    "                doc_id=document.doc_id,\n",
    "                metadata={\n",
    "                    **document.metadata,\n",
    "                    \"chunk_num\": chunk_num,\n",
    "                    \"total_chunks\": None,  # Will update later\n",
    "                    \"chunking_strategy\": \"fixed_size\",\n",
    "                    \"chunk_size\": self.chunk_size\n",
    "                },\n",
    "                start_index=start,\n",
    "                end_index=end\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            # If we've reached the end, break out of the loop\n",
    "            if end >= total_tokens:\n",
    "                break\n",
    "            \n",
    "            # Move to next chunk (with overlap)\n",
    "            start = end - self.overlap\n",
    "            chunk_num += 1\n",
    "        \n",
    "        # Update total_chunks metadata\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata[\"total_chunks\"] = len(chunks)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "\n",
    "# Test fixed-size chunking\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING FIXED-SIZE CHUNKING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "chunker = FixedSizeChunker(chunk_size=500, overlap=50)\n",
    "\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = chunker.chunk_document(doc)\n",
    "    all_chunks.extend(chunks)\n",
    "    print(f\"Document '{doc.metadata['type']}':\")\n",
    "    print(f\"  Original tokens: {count_tokens(doc.content)}\")\n",
    "    print(f\"  Created {len(chunks)} chunks\")\n",
    "    print(f\"  Avg chunk size: {np.mean([count_tokens(c.content) for c in chunks]):.0f} tokens\")\n",
    "    print()\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(\"\\nSample chunk:\")\n",
    "print(\"-\" * 80)\n",
    "print(all_chunks[0].content[:500] + \"...\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb171f3",
   "metadata": {},
   "source": [
    "### Challenge 1.2: Semantic Chunking (15 minutes)\n",
    "\n",
    "**Strategy:** Split by semantic boundaries (paragraphs, sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2aaae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticChunker:\n",
    "    \"\"\"Split text by semantic boundaries (sections, paragraphs)\"\"\"\n",
    "    \n",
    "    def __init__(self, max_chunk_size: int = 800):\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    def split_by_sections(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text by section headers\"\"\"\n",
    "        # Look for section patterns like \"Section 1:\", \"Product 1:\", etc.\n",
    "        section_pattern = r'\\n(?:Section|Product|Chapter|Part)\\s+\\d+[:\\.]'\n",
    "        sections = re.split(section_pattern, text)\n",
    "        \n",
    "        # Filter empty sections\n",
    "        sections = [s.strip() for s in sections if s.strip()]\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def split_by_paragraphs(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text by paragraph breaks\"\"\"\n",
    "        # Split on double newlines\n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "        paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "        \n",
    "        return paragraphs\n",
    "    \n",
    "    def merge_small_chunks(self, chunks: List[str]) -> List[str]:\n",
    "        \"\"\"Merge chunks that are too small\"\"\"\n",
    "        merged = []\n",
    "        current = \"\"\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunk_tokens = count_tokens(chunk)\n",
    "            current_tokens = count_tokens(current)\n",
    "            \n",
    "            if current_tokens + chunk_tokens <= self.max_chunk_size:\n",
    "                current = (current + \"\\n\\n\" + chunk).strip()\n",
    "            else:\n",
    "                if current:\n",
    "                    merged.append(current)\n",
    "                current = chunk\n",
    "        \n",
    "        if current:\n",
    "            merged.append(current)\n",
    "        \n",
    "        return merged\n",
    "    \n",
    "    def chunk_document(self, document: Document) -> List[Chunk]:\n",
    "        \"\"\"Split document by semantic boundaries\"\"\"\n",
    "        \n",
    "        # Try section-based splitting first\n",
    "        sections = self.split_by_sections(document.content)\n",
    "        \n",
    "        if len(sections) <= 1:\n",
    "            # Fall back to paragraph splitting\n",
    "            sections = self.split_by_paragraphs(document.content)\n",
    "        \n",
    "        # Merge small chunks\n",
    "        merged_chunks = self.merge_small_chunks(sections)\n",
    "        \n",
    "        # Split any chunks that are too large\n",
    "        final_text_chunks = []\n",
    "        for chunk_text in merged_chunks:\n",
    "            if count_tokens(chunk_text) > self.max_chunk_size:\n",
    "                # Use fixed-size chunking for oversized chunks\n",
    "                temp_doc = Document(content=chunk_text, metadata=document.metadata)\n",
    "                fixed_chunker = FixedSizeChunker(chunk_size=self.max_chunk_size, overlap=0)\n",
    "                sub_chunks = fixed_chunker.chunk_document(temp_doc)\n",
    "                final_text_chunks.extend([c.content for c in sub_chunks])\n",
    "            else:\n",
    "                final_text_chunks.append(chunk_text)\n",
    "        \n",
    "        # Create Chunk objects\n",
    "        chunks = []\n",
    "        for i, chunk_text in enumerate(final_text_chunks):\n",
    "            chunk = Chunk(\n",
    "                content=chunk_text,\n",
    "                chunk_id=f\"{document.doc_id}_semantic_{i}\",\n",
    "                doc_id=document.doc_id,\n",
    "                metadata={\n",
    "                    **document.metadata,\n",
    "                    \"chunk_num\": i,\n",
    "                    \"total_chunks\": len(final_text_chunks),\n",
    "                    \"chunking_strategy\": \"semantic\",\n",
    "                    \"max_chunk_size\": self.max_chunk_size\n",
    "                }\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "\n",
    "# Test semantic chunking\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING SEMANTIC CHUNKING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "semantic_chunker = SemanticChunker(max_chunk_size=800)\n",
    "\n",
    "semantic_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = semantic_chunker.chunk_document(doc)\n",
    "    semantic_chunks.extend(chunks)\n",
    "    print(f\"Document '{doc.metadata['type']}':\")\n",
    "    print(f\"  Original tokens: {count_tokens(doc.content)}\")\n",
    "    print(f\"  Created {len(chunks)} semantic chunks\")\n",
    "    print(f\"  Avg chunk size: {np.mean([count_tokens(c.content) for c in chunks]):.0f} tokens\")\n",
    "    print(f\"  Chunk sizes: {[count_tokens(c.content) for c in chunks]}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Total semantic chunks: {len(semantic_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678e7e99",
   "metadata": {},
   "source": [
    "### Challenge 1.3: Chunking Comparison (5 minutes)\n",
    "\n",
    "**Objective:** Compare different strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0b49c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_chunking_strategies(document: Document) -> pd.DataFrame:\n",
    "    \"\"\"Compare different chunking strategies on same document\"\"\"\n",
    "    \n",
    "    strategies = {\n",
    "        \"Fixed 300\": FixedSizeChunker(chunk_size=300, overlap=30),\n",
    "        \"Fixed 500\": FixedSizeChunker(chunk_size=500, overlap=50),\n",
    "        \"Fixed 800\": FixedSizeChunker(chunk_size=800, overlap=80),\n",
    "        \"Semantic 600\": SemanticChunker(max_chunk_size=600),\n",
    "        \"Semantic 800\": SemanticChunker(max_chunk_size=800),\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, chunker in strategies.items():\n",
    "        chunks = chunker.chunk_document(document)\n",
    "        \n",
    "        chunk_sizes = [count_tokens(c.content) for c in chunks]\n",
    "        \n",
    "        results.append({\n",
    "            \"Strategy\": name,\n",
    "            \"Num Chunks\": len(chunks),\n",
    "            \"Min Size\": int(np.min(chunk_sizes)),\n",
    "            \"Avg Size\": int(np.mean(chunk_sizes)),\n",
    "            \"Max Size\": int(np.max(chunk_sizes)),\n",
    "            \"Std Dev\": int(np.std(chunk_sizes))\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Compare strategies\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHUNKING STRATEGY COMPARISON\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for doc in documents[:2]:  # Compare on first 2 documents\n",
    "    print(f\"\\nDocument: {doc.metadata['type']}\")\n",
    "    print(f\"Original size: {count_tokens(doc.content)} tokens\\n\")\n",
    "    \n",
    "    comparison = compare_chunking_strategies(doc)\n",
    "    print(comparison.to_string(index=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd30e4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 2: EMBEDDINGS & SEMANTIC SEARCH (Lab 4.2)\n",
    "\n",
    "**Duration:** 30 minutes  \n",
    "**Objective:** Generate embeddings and implement semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d6a5e2",
   "metadata": {},
   "source": [
    "### Challenge 2.1: Generate Embeddings (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    \"\"\"Generate and manage embeddings for chunks\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = EMBEDDING_MODEL):\n",
    "        self.model = model\n",
    "        self.cache = {}  # Simple cache to avoid re-embedding\n",
    "    \n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for single text\"\"\"\n",
    "        # Check cache\n",
    "        cache_key = hash(text)\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = get_embedding(text, self.model)\n",
    "        \n",
    "        # Cache it\n",
    "        self.cache[cache_key] = embedding\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def embed_chunks(self, chunks: List[Chunk], show_progress: bool = True) -> List[Chunk]:\n",
    "        \"\"\"Generate embeddings for all chunks\"\"\"\n",
    "        total = len(chunks)\n",
    "        \n",
    "        if show_progress:\n",
    "            print(f\"Generating embeddings for {total} chunks...\")\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if chunk.embedding is None:\n",
    "                chunk.embedding = self.embed_text(chunk.content)\n",
    "            \n",
    "            if show_progress and (i + 1) % 5 == 0:\n",
    "                print(f\"  Progress: {i + 1}/{total} chunks embedded\")\n",
    "        \n",
    "        if show_progress:\n",
    "            print(f\"✓ All {total} chunks embedded\\n\")\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Return cache statistics\"\"\"\n",
    "        return {\n",
    "            \"cached_embeddings\": len(self.cache),\n",
    "            \"total_dimensions\": EMBEDDING_DIMENSIONS\n",
    "        }\n",
    "\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATING EMBEDDINGS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Use fixed-size chunks from earlier\n",
    "embedder = EmbeddingGenerator()\n",
    "embedded_chunks = embedder.embed_chunks(all_chunks, show_progress=True)\n",
    "\n",
    "# Verify embeddings\n",
    "sample_chunk = embedded_chunks[0]\n",
    "print(f\"Sample chunk: {sample_chunk.chunk_id}\")\n",
    "print(f\"Embedding dimensions: {len(sample_chunk.embedding)}\")\n",
    "print(f\"First 10 values: {sample_chunk.embedding[:10]}\")\n",
    "print(f\"\\nCache stats: {embedder.get_cache_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d594d0",
   "metadata": {},
   "source": [
    "### Challenge 2.2: Semantic Similarity Search (15 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99306388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearchEngine:\n",
    "    \"\"\"Perform semantic search across embedded chunks\"\"\"\n",
    "    \n",
    "    def __init__(self, chunks: List[Chunk]):\n",
    "        self.chunks = chunks\n",
    "        self.embeddings = np.array([c.embedding for c in chunks])\n",
    "        self.embedder = EmbeddingGenerator()\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        min_similarity: float = 0.0\n",
    "    ) -> List[Tuple[Chunk, float]]:\n",
    "        \"\"\"\n",
    "        Search for chunks most similar to query\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            top_k: Number of results to return\n",
    "            min_similarity: Minimum similarity threshold\n",
    "        \n",
    "        Returns:\n",
    "            List of (chunk, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = np.array(self.embedder.embed_text(query)).reshape(1, -1)\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        # Filter by minimum similarity\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            similarity = similarities[idx]\n",
    "            if similarity >= min_similarity:\n",
    "                results.append((self.chunks[idx], float(similarity)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def search_with_metadata_filter(\n",
    "        self,\n",
    "        query: str,\n",
    "        metadata_filter: Dict[str, Any],\n",
    "        top_k: int = 5\n",
    "    ) -> List[Tuple[Chunk, float]]:\n",
    "        \"\"\"Search with metadata filtering\"\"\"\n",
    "        # Filter chunks by metadata\n",
    "        filtered_chunks = [\n",
    "            c for c in self.chunks\n",
    "            if all(c.metadata.get(k) == v for k, v in metadata_filter.items())\n",
    "        ]\n",
    "        \n",
    "        if not filtered_chunks:\n",
    "            return []\n",
    "        \n",
    "        # Create temporary search engine with filtered chunks\n",
    "        temp_engine = SemanticSearchEngine(filtered_chunks)\n",
    "        return temp_engine.search(query, top_k)\n",
    "\n",
    "\n",
    "# Test semantic search\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEMANTIC SEARCH TESTING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "search_engine = SemanticSearchEngine(embedded_chunks)\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"How many vacation days do employees get?\",\n",
    "    \"What is the minimum credit score for a personal loan?\",\n",
    "    \"What are the AML requirements for new customers?\",\n",
    "    \"Can I work from home?\",\n",
    "    \"How much is the mortgage origination fee?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    results = search_engine.search(query, top_k=3)\n",
    "    \n",
    "    for i, (chunk, similarity) in enumerate(results, 1):\n",
    "        print(f\"\\nResult {i} (similarity: {similarity:.3f}):\")\n",
    "        print(f\"Source: {chunk.metadata['type']}\")\n",
    "        print(f\"Content preview: {chunk.content[:200]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d17843",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 3: VECTOR DATABASE & RETRIEVAL (Lab 4.3)\n",
    "\n",
    "**Duration:** 30 minutes  \n",
    "**Objective:** Build scalable vector database for production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56eb0e1",
   "metadata": {},
   "source": [
    "### Challenge 3.1: ChromaDB Integration (20 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc25a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDatabase:\n",
    "    \"\"\"Production-grade vector database using ChromaDB\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"bank_knowledge\"):\n",
    "        # Initialize ChromaDB client\n",
    "        self.client = chromadb.Client(Settings(\n",
    "            anonymized_telemetry=False,\n",
    "            allow_reset=True\n",
    "        ))\n",
    "        \n",
    "        # Reset if exists (for development)\n",
    "        try:\n",
    "            self.client.delete_collection(collection_name)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Create collection\n",
    "        self.collection = self.client.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"description\": \"Bank knowledge base\"}\n",
    "        )\n",
    "        \n",
    "        self.embedder = EmbeddingGenerator()\n",
    "    \n",
    "    def add_chunks(self, chunks: List[Chunk], show_progress: bool = True):\n",
    "        \"\"\"Add chunks to vector database\"\"\"\n",
    "        if show_progress:\n",
    "            print(f\"Adding {len(chunks)} chunks to vector database...\")\n",
    "        \n",
    "        # Prepare data for batch insertion\n",
    "        ids = []\n",
    "        embeddings = []\n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            ids.append(chunk.chunk_id)\n",
    "            \n",
    "            # Ensure chunk has embedding\n",
    "            if chunk.embedding is None:\n",
    "                chunk.embedding = self.embedder.embed_text(chunk.content)\n",
    "            \n",
    "            embeddings.append(chunk.embedding)\n",
    "            documents.append(chunk.content)\n",
    "            metadatas.append(chunk.metadata)\n",
    "        \n",
    "        # Batch insert\n",
    "        self.collection.add(\n",
    "            ids=ids,\n",
    "            embeddings=embeddings,\n",
    "            documents=documents,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "        \n",
    "        if show_progress:\n",
    "            print(f\"✓ Added {len(chunks)} chunks to database\\n\")\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        n_results: int = 5,\n",
    "        where: Optional[Dict] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Search vector database\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            n_results: Number of results to return\n",
    "            where: Metadata filter (e.g., {\"type\": \"hr_policies\"})\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with ids, documents, distances, metadatas\n",
    "        \"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedder.embed_text(query)\n",
    "        \n",
    "        # Search\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            where=where\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def hybrid_search(\n",
    "        self,\n",
    "        query: str,\n",
    "        keyword: Optional[str] = None,\n",
    "        n_results: int = 5\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Hybrid search combining vector similarity and keyword filtering\n",
    "        \"\"\"\n",
    "        # Semantic search\n",
    "        semantic_results = self.search(query, n_results=n_results*2)\n",
    "        \n",
    "        if keyword is None:\n",
    "            return semantic_results\n",
    "        \n",
    "        # Filter by keyword\n",
    "        keyword_lower = keyword.lower()\n",
    "        filtered_ids = []\n",
    "        filtered_docs = []\n",
    "        filtered_distances = []\n",
    "        filtered_metas = []\n",
    "        \n",
    "        for i, doc in enumerate(semantic_results['documents'][0]):\n",
    "            if keyword_lower in doc.lower():\n",
    "                filtered_ids.append(semantic_results['ids'][0][i])\n",
    "                filtered_docs.append(doc)\n",
    "                filtered_distances.append(semantic_results['distances'][0][i])\n",
    "                filtered_metas.append(semantic_results['metadatas'][0][i])\n",
    "                \n",
    "                if len(filtered_ids) >= n_results:\n",
    "                    break\n",
    "        \n",
    "        return {\n",
    "            'ids': [filtered_ids],\n",
    "            'documents': [filtered_docs],\n",
    "            'distances': [filtered_distances],\n",
    "            'metadatas': [filtered_metas]\n",
    "        }\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get database statistics\"\"\"\n",
    "        return {\n",
    "            \"total_chunks\": self.collection.count(),\n",
    "            \"collection_name\": self.collection.name\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize vector database\n",
    "print(\"=\"*80)\n",
    "print(\"VECTOR DATABASE SETUP\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "vector_db = VectorDatabase(collection_name=\"bank_knowledge_base\")\n",
    "vector_db.add_chunks(embedded_chunks, show_progress=True)\n",
    "\n",
    "print(f\"Database stats: {vector_db.get_stats()}\\n\")\n",
    "\n",
    "# Test vector database search\n",
    "print(\"=\"*80)\n",
    "print(\"VECTOR DATABASE SEARCH TESTING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "test_queries = [\n",
    "    (\"How many PTO days do I get?\", None),\n",
    "    (\"What credit score do I need for a mortgage?\", None),\n",
    "    (\"Tell me about AML requirements\", \"suspicious\"),  # Hybrid search\n",
    "]\n",
    "\n",
    "for query, keyword in test_queries:\n",
    "    print(f\"Query: '{query}'\")\n",
    "    if keyword:\n",
    "        print(f\"Keyword filter: '{keyword}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if keyword:\n",
    "        results = vector_db.hybrid_search(query, keyword=keyword, n_results=3)\n",
    "    else:\n",
    "        results = vector_db.search(query, n_results=3)\n",
    "    \n",
    "    for i, (doc, distance, meta) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['distances'][0],\n",
    "        results['metadatas'][0]\n",
    "    ), 1):\n",
    "        similarity = 1 - distance  # Convert distance to similarity\n",
    "        print(f\"\\nResult {i} (similarity: {similarity:.3f}):\")\n",
    "        print(f\"Source: {meta['type']}\")\n",
    "        print(f\"Preview: {doc[:200]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26fb9d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 4: COMPLETE RAG PIPELINE (Lab 4.4)\n",
    "\n",
    "**Duration:** 40 minutes  \n",
    "**Objective:** Build end-to-end RAG system from query to answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79d58fc",
   "metadata": {},
   "source": [
    "### Challenge 4.1: Basic RAG Implementation (20 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61793c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"Complete Retrieval-Augmented Generation system\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_db: VectorDatabase, model: str = GPT4):\n",
    "        self.vector_db = vector_db\n",
    "        self.model = model\n",
    "        self.query_log = []\n",
    "    \n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        n_results: int = 5,\n",
    "        min_similarity: float = -1.0\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks for query\n",
    "        \n",
    "        Returns list of chunks with content and metadata\n",
    "        \n",
    "        Note: ChromaDB uses L2 distance by default, so we convert to similarity score.\n",
    "        Lower distance = higher similarity. Default min_similarity of -1.0 accepts all results.\n",
    "        \"\"\"\n",
    "        # Search vector database\n",
    "        results = self.vector_db.search(query, n_results=n_results)\n",
    "        \n",
    "        # Convert to list of dicts and filter by similarity\n",
    "        retrieved = []\n",
    "        for i, (doc, distance, meta) in enumerate(zip(\n",
    "            results['documents'][0],\n",
    "            results['distances'][0],\n",
    "            results['metadatas'][0]\n",
    "        )):\n",
    "            # Convert distance to similarity-like score (lower distance = better match)\n",
    "            # Note: With L2 distance, this can be negative for distant matches\n",
    "            similarity = 1 - distance\n",
    "            if similarity >= min_similarity:\n",
    "                retrieved.append({\n",
    "                    'content': doc,\n",
    "                    'metadata': meta,\n",
    "                    'similarity': similarity,\n",
    "                    'distance': distance  # Keep original distance for reference\n",
    "                })\n",
    "        \n",
    "        return retrieved\n",
    "    \n",
    "    def generate_answer(\n",
    "        self,\n",
    "        query: str,\n",
    "        context_chunks: List[Dict[str, Any]],\n",
    "        include_sources: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer using retrieved context\n",
    "        \"\"\"\n",
    "        # Build context from chunks\n",
    "        context_parts = []\n",
    "        for i, chunk in enumerate(context_chunks, 1):\n",
    "            context_parts.append(f\"[Source {i}] {chunk['content']}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"\n",
    "Answer the following question using ONLY the provided context. Be specific and cite sources.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Instructions:\n",
    "- Answer based ONLY on the context provided\n",
    "- If the context doesn't contain enough information, say so\n",
    "- Cite which source(s) you used (e.g., \"According to Source 1...\")\n",
    "- Be concise but complete\n",
    "- If multiple sources provide relevant info, synthesize them\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = call_llm(\n",
    "            prompt,\n",
    "            system_prompt=\"You are a helpful assistant answering questions based on provided context. Always cite your sources.\",\n",
    "            model=self.model\n",
    "        )\n",
    "        \n",
    "        # Optionally append source citations\n",
    "        if include_sources:\n",
    "            sources_text = \"\\n\\nSources:\\n\"\n",
    "            for i, chunk in enumerate(context_chunks, 1):\n",
    "                sources_text += f\"[{i}] {chunk['metadata']['type']} (similarity: {chunk['similarity']:.2f})\\n\"\n",
    "            answer += sources_text\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        n_results: int = 5,\n",
    "        min_similarity: float = -1.0,\n",
    "        include_sources: bool = True\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Complete RAG query: retrieve + generate\n",
    "        \n",
    "        Returns dictionary with question, answer, retrieved chunks, metrics\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Retrieve relevant chunks\n",
    "        retrieved_chunks = self.retrieve(question, n_results, min_similarity)\n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        if not retrieved_chunks:\n",
    "            return {\n",
    "                'question': question,\n",
    "                'answer': \"I don't have enough information to answer that question.\",\n",
    "                'retrieved_chunks': [],\n",
    "                'metrics': {\n",
    "                    'retrieval_time': retrieval_time,\n",
    "                    'generation_time': 0,\n",
    "                    'total_time': retrieval_time,\n",
    "                    'chunks_retrieved': 0,\n",
    "                    'avg_similarity': 0\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Generate answer\n",
    "        gen_start = time.time()\n",
    "        answer = self.generate_answer(question, retrieved_chunks, include_sources)\n",
    "        generation_time = time.time() - gen_start\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Log query\n",
    "        self.query_log.append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'question': question,\n",
    "            'chunks_retrieved': len(retrieved_chunks),\n",
    "            'total_time': total_time\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'retrieved_chunks': retrieved_chunks,\n",
    "            'metrics': {\n",
    "                'retrieval_time': retrieval_time,\n",
    "                'generation_time': generation_time,\n",
    "                'total_time': total_time,\n",
    "                'chunks_retrieved': len(retrieved_chunks),\n",
    "                'avg_similarity': np.mean([c['similarity'] for c in retrieved_chunks])\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_query_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about queries\"\"\"\n",
    "        if not self.query_log:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'total_queries': len(self.query_log),\n",
    "            'avg_time': np.mean([q['total_time'] for q in self.query_log]),\n",
    "            'avg_chunks_retrieved': np.mean([q['chunks_retrieved'] for q in self.query_log])\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize RAG system\n",
    "print(\"=\"*80)\n",
    "print(\"RAG SYSTEM INITIALIZATION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "rag_system = RAGSystem(vector_db, model=GPT4)\n",
    "\n",
    "print(\"✓ RAG system ready\\n\")\n",
    "\n",
    "# Test RAG system\n",
    "print(\"=\"*80)\n",
    "print(\"RAG SYSTEM TESTING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "test_questions = [\n",
    "    \"How many vacation days do full-time employees get per year?\",\n",
    "    \"What is the minimum credit score needed for a personal loan?\",\n",
    "    \"What are the key AML requirements when opening a new account?\",\n",
    "    \"Can employees work remotely and what are the requirements?\",\n",
    "    \"What fees are associated with personal loans?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    result = rag_system.query(question, n_results=3, include_sources=True)\n",
    "    \n",
    "    print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "    \n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  Retrieval time: {result['metrics']['retrieval_time']:.3f}s\")\n",
    "    print(f\"  Generation time: {result['metrics']['generation_time']:.3f}s\")\n",
    "    print(f\"  Total time: {result['metrics']['total_time']:.3f}s\")\n",
    "    print(f\"  Chunks retrieved: {result['metrics']['chunks_retrieved']}\")\n",
    "    print(f\"  Avg similarity: {result['metrics']['avg_similarity']:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fff3a1",
   "metadata": {},
   "source": [
    "### Challenge 4.2: RAG with Query Enhancement (15 minutes)\n",
    "\n",
    "**Objective:** Improve retrieval with query expansion and reformulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582db5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedRAGSystem(RAGSystem):\n",
    "    \"\"\"RAG system with query enhancement\"\"\"\n",
    "    \n",
    "    def expand_query(self, query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate query variations to improve retrieval\n",
    "        \"\"\"\n",
    "        expansion_prompt = f\"\"\"\n",
    "Generate 2 alternative phrasings of this question that preserve the meaning but use different words:\n",
    "\n",
    "Original: {query}\n",
    "\n",
    "Return as JSON array: [\"variation 1\", \"variation 2\"]\n",
    "\"\"\"\n",
    "        \n",
    "        response = call_llm(expansion_prompt, model=GPT35)\n",
    "        \n",
    "        try:\n",
    "            variations = json.loads(response)\n",
    "            return [query] + variations\n",
    "        except:\n",
    "            return [query]\n",
    "    \n",
    "    def query_with_expansion(\n",
    "        self,\n",
    "        question: str,\n",
    "        n_results: int = 5\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Query with automatic query expansion\n",
    "        \"\"\"\n",
    "        # Expand query\n",
    "        query_variations = self.expand_query(question)\n",
    "        \n",
    "        # Retrieve for each variation\n",
    "        all_chunks = {}  # Use dict to deduplicate by chunk_id\n",
    "        \n",
    "        for variant in query_variations:\n",
    "            chunks = self.retrieve(variant, n_results=n_results)\n",
    "            for chunk in chunks:\n",
    "                chunk_id = chunk['metadata'].get('chunk_num', id(chunk))\n",
    "                if chunk_id not in all_chunks or chunk['similarity'] > all_chunks[chunk_id]['similarity']:\n",
    "                    all_chunks[chunk_id] = chunk\n",
    "        \n",
    "        # Get top chunks by similarity\n",
    "        top_chunks = sorted(\n",
    "            all_chunks.values(),\n",
    "            key=lambda x: x['similarity'],\n",
    "            reverse=True\n",
    "        )[:n_results]\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.generate_answer(question, top_chunks, include_sources=True)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'query_variations': query_variations,\n",
    "            'answer': answer,\n",
    "            'retrieved_chunks': top_chunks\n",
    "        }\n",
    "\n",
    "\n",
    "# Test enhanced RAG\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENHANCED RAG WITH QUERY EXPANSION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "enhanced_rag = EnhancedRAGSystem(vector_db, model=GPT4)\n",
    "\n",
    "test_question = \"What do I need to qualify for a home loan?\"\n",
    "\n",
    "print(f\"Question: {test_question}\\n\")\n",
    "result = enhanced_rag.query_with_expansion(test_question, n_results=3)\n",
    "\n",
    "print(f\"Query variations generated:\")\n",
    "for i, var in enumerate(result['query_variations'], 1):\n",
    "    print(f\"  {i}. {var}\")\n",
    "\n",
    "print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190d251",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 5: ADVANCED RAG PATTERNS (Lab 4.5)\n",
    "\n",
    "**Duration:** 20 minutes  \n",
    "**Objective:** Implement production-grade RAG enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dcf81e",
   "metadata": {},
   "source": [
    "### Challenge 5.1: Re-ranking for Precision (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ac7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReRankingRAG(RAGSystem):\n",
    "    \"\"\"RAG with two-stage retrieval: broad recall + precise reranking\"\"\"\n",
    "    \n",
    "    def rerank_with_llm(\n",
    "        self,\n",
    "        query: str,\n",
    "        chunks: List[Dict[str, Any]],\n",
    "        top_k: int = 5\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Re-rank chunks using LLM for better precision\n",
    "        \"\"\"\n",
    "        # Create prompt for reranking\n",
    "        chunks_text = \"\\n\\n\".join([\n",
    "            f\"[{i}] {chunk['content'][:300]}...\"\n",
    "            for i, chunk in enumerate(chunks, 1)\n",
    "        ])\n",
    "        \n",
    "        rerank_prompt = f\"\"\"\n",
    "Rate how relevant each text chunk is to answering this question.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Text Chunks:\n",
    "{chunks_text}\n",
    "\n",
    "For each chunk, provide a relevance score from 0-10 where:\n",
    "- 10 = Perfectly answers the question\n",
    "- 7-9 = Highly relevant, contains key information\n",
    "- 4-6 = Somewhat relevant, provides context\n",
    "- 1-3 = Tangentially related\n",
    "- 0 = Not relevant\n",
    "\n",
    "Return JSON array: [score1, score2, score3, ...]\n",
    "\"\"\"\n",
    "        \n",
    "        response = call_llm(rerank_prompt, model=GPT35)\n",
    "        \n",
    "        try:\n",
    "            scores = json.loads(response)\n",
    "            \n",
    "            # Combine scores with chunks\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if i < len(scores):\n",
    "                    chunk['rerank_score'] = scores[i]\n",
    "                else:\n",
    "                    chunk['rerank_score'] = 0\n",
    "            \n",
    "            # Sort by rerank score\n",
    "            reranked = sorted(chunks, key=lambda x: x['rerank_score'], reverse=True)\n",
    "            \n",
    "            return reranked[:top_k]\n",
    "            \n",
    "        except:\n",
    "            # Fallback to original ranking\n",
    "            return chunks[:top_k]\n",
    "    \n",
    "    def query_with_reranking(\n",
    "        self,\n",
    "        question: str,\n",
    "        initial_results: int = 20,\n",
    "        final_results: int = 5\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Two-stage retrieval: broad recall + precise reranking\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Stage 1: Broad retrieval (use permissive threshold to get more candidates)\n",
    "        initial_chunks = self.retrieve(question, n_results=initial_results, min_similarity=-1.0)\n",
    "        \n",
    "        if not initial_chunks:\n",
    "            # Fallback to basic query with consistent metrics structure\n",
    "            result = self.query(question, n_results=final_results)\n",
    "            result['metrics']['initial_retrieval'] = 0\n",
    "            result['metrics']['final_results'] = len(result['retrieved_chunks'])\n",
    "            return result\n",
    "        \n",
    "        # Stage 2: Rerank\n",
    "        reranked_chunks = self.rerank_with_llm(question, initial_chunks, top_k=final_results)\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.generate_answer(question, reranked_chunks, include_sources=True)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'retrieved_chunks': reranked_chunks,\n",
    "            'metrics': {\n",
    "                'initial_retrieval': initial_results,\n",
    "                'final_results': len(reranked_chunks),\n",
    "                'total_time': time.time() - start_time\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Test re-ranking\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RE-RANKING RAG TESTING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "reranking_rag = ReRankingRAG(vector_db, model=GPT4)\n",
    "\n",
    "test_question = \"What documentation is required for a mortgage application?\"\n",
    "\n",
    "print(f\"Question: {test_question}\\n\")\n",
    "\n",
    "# Compare basic vs reranking\n",
    "print(\"Basic RAG:\")\n",
    "basic_result = rag_system.query(test_question, n_results=3)\n",
    "print(f\"Chunks: {len(basic_result['retrieved_chunks'])}\")\n",
    "print(f'Similarities: {[f\"{c[\"similarity\"]:.2f}\" for c in basic_result[\"retrieved_chunks\"]]}\\n')\n",
    "\n",
    "print(\"Re-ranking RAG:\")\n",
    "rerank_result = reranking_rag.query_with_reranking(test_question, initial_results=10, final_results=3)\n",
    "print(f\"Initial retrieval: {rerank_result['metrics']['initial_retrieval']}\")\n",
    "print(f\"Final results: {rerank_result['metrics']['final_results']}\")\n",
    "print(f'Re-rank scores: {[f\"{c.get(\"rerank_score\", 0)}/10\" for c in rerank_result[\"retrieved_chunks\"]]}\\n')\n",
    "\n",
    "print(f\"Answer:\\n{rerank_result['answer']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6231d524",
   "metadata": {},
   "source": [
    "### Challenge 5.2: Evaluation Metrics (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"Evaluate RAG system quality\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.test_cases = []\n",
    "    \n",
    "    def add_test_case(\n",
    "        self,\n",
    "        question: str,\n",
    "        expected_answer_contains: List[str],\n",
    "        relevant_doc_types: List[str]\n",
    "    ):\n",
    "        \"\"\"Add test case for evaluation\"\"\"\n",
    "        self.test_cases.append({\n",
    "            'question': question,\n",
    "            'expected_answer_contains': expected_answer_contains,\n",
    "            'relevant_doc_types': relevant_doc_types\n",
    "        })\n",
    "    \n",
    "    def evaluate_retrieval(\n",
    "        self,\n",
    "        rag_system: RAGSystem,\n",
    "        test_case: Dict\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate retrieval quality\"\"\"\n",
    "        result = rag_system.query(test_case['question'], n_results=5)\n",
    "        \n",
    "        retrieved_types = [\n",
    "            chunk['metadata']['type']\n",
    "            for chunk in result['retrieved_chunks']\n",
    "        ]\n",
    "        \n",
    "        # Precision: How many retrieved are relevant?\n",
    "        relevant_retrieved = sum(\n",
    "            1 for t in retrieved_types\n",
    "            if t in test_case['relevant_doc_types']\n",
    "        )\n",
    "        precision = relevant_retrieved / len(retrieved_types) if retrieved_types else 0\n",
    "        \n",
    "        # Recall: Did we get all relevant doc types?\n",
    "        recall = len(set(retrieved_types) & set(test_case['relevant_doc_types'])) / len(test_case['relevant_doc_types'])\n",
    "        \n",
    "        # F1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'avg_similarity': result['metrics']['avg_similarity']\n",
    "        }\n",
    "    \n",
    "    def evaluate_answer_quality(\n",
    "        self,\n",
    "        rag_system: RAGSystem,\n",
    "        test_case: Dict\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate generated answer quality\"\"\"\n",
    "        result = rag_system.query(test_case['question'])\n",
    "        answer = result['answer'].lower()\n",
    "        \n",
    "        # Check if expected content is in answer\n",
    "        contains_count = sum(\n",
    "            1 for expected in test_case['expected_answer_contains']\n",
    "            if expected.lower() in answer\n",
    "        )\n",
    "        \n",
    "        completeness = contains_count / len(test_case['expected_answer_contains'])\n",
    "        \n",
    "        return {\n",
    "            'completeness': completeness,\n",
    "            'answer_length': len(result['answer']),\n",
    "            'has_sources': 'Source' in result['answer']\n",
    "        }\n",
    "    \n",
    "    def run_evaluation(self, rag_system: RAGSystem) -> pd.DataFrame:\n",
    "        \"\"\"Run full evaluation suite\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for test_case in self.test_cases:\n",
    "            retrieval_metrics = self.evaluate_retrieval(rag_system, test_case)\n",
    "            answer_metrics = self.evaluate_answer_quality(rag_system, test_case)\n",
    "            \n",
    "            results.append({\n",
    "                'question': test_case['question'][:50] + '...',\n",
    "                **retrieval_metrics,\n",
    "                **answer_metrics\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Create evaluation suite\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RAG EVALUATION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "evaluator = RAGEvaluator()\n",
    "\n",
    "# Add test cases\n",
    "evaluator.add_test_case(\n",
    "    question=\"How much PTO do employees get?\",\n",
    "    expected_answer_contains=[\"20 days\", \"160 hours\", \"annual\"],\n",
    "    relevant_doc_types=[\"hr_policies\"]\n",
    ")\n",
    "\n",
    "evaluator.add_test_case(\n",
    "    question=\"What credit score is needed for a personal loan?\",\n",
    "    expected_answer_contains=[\"650\", \"credit score\", \"minimum\"],\n",
    "    relevant_doc_types=[\"loan_products\"]\n",
    ")\n",
    "\n",
    "evaluator.add_test_case(\n",
    "    question=\"What are AML customer verification requirements?\",\n",
    "    expected_answer_contains=[\"government-issued ID\", \"SSN\", \"address\"],\n",
    "    relevant_doc_types=[\"compliance_guidelines\"]\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluator.run_evaluation(rag_system)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(eval_results.to_string(index=False))\n",
    "\n",
    "print(f\"\\n\\nOverall Metrics:\")\n",
    "print(f\"  Avg Precision: {eval_results['precision'].mean():.2f}\")\n",
    "print(f\"  Avg Recall: {eval_results['recall'].mean():.2f}\")\n",
    "print(f\"  Avg F1 Score: {eval_results['f1'].mean():.2f}\")\n",
    "print(f\"  Avg Completeness: {eval_results['completeness'].mean():.2f}\")\n",
    "print(f\"  Avg Similarity: {eval_results['avg_similarity'].mean():.3f}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
